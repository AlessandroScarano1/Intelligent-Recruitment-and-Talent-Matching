{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports loaded\n",
      "project root: /home/developer/project\n",
      "kafka broker: kafka-broker:29092\n",
      "target sample size: 150,000\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, concat, monotonically_increasing_id, length, udf, when, lower\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# setup paths - detect project root\n",
    "cwd = os.getcwd()\n",
    "if 'notebooks' in cwd:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# kafka config\n",
    "KAFKA_BROKER = os.environ.get('KAFKA_BROKER', 'kafka-broker:29092')\n",
    "\n",
    "# sampling config\n",
    "TARGET_SAMPLE_SIZE = 150000  # 150K jobs for balanced training with 7K CVs\n",
    "MIN_PER_DOMAIN = 1000        # minimum samples per ISCO domain\n",
    "\n",
    "print('imports loaded')\n",
    "print(f'project root: {PROJECT_ROOT}')\n",
    "print(f'kafka broker: {KAFKA_BROKER}')\n",
    "print(f'target sample size: {TARGET_SAMPLE_SIZE:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/27 01:28:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark started: 4.1.1\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('FinalEmbeddingOutput') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f'spark started: {spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1,345,711 jobs\n",
      "columns: ['id', 'job_title', 'company', 'job_location', 'skills', 'seniority', 'embedding_text']\n",
      "sample:\n",
      "+------------------------------------+-----------------------------------------------------+------------------------------------------------------------+\n",
      "|                                  id|                                            job_title|                                              embedding_text|\n",
      "+------------------------------------+-----------------------------------------------------+------------------------------------------------------------+\n",
      "|71af9d85-4b75-40d6-9835-38d7153df1d5|\"Telephonic\" Nurse Case Manager II at Elevance Health|Role of \"Telephonic\" Nurse Case Manager II at Elevance He...|\n",
      "|e19c544b-810f-42b8-a861-b68bc9be0d71|\"Telephonic\" Nurse Case Manager II at Elevance Health|Role of \"Telephonic\" Nurse Case Manager II at Elevance He...|\n",
      "|4f9d65ee-1255-429e-8fcd-127196cc1080|          $10,000 Sign on Bonus Laboratory Supervisor|Role of $10,000 Sign on Bonus Laboratory Supervisor at Cl...|\n",
      "+------------------------------------+-----------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": "input_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'unified_job_postings', 'unified_jobs.parquet')\njobs_df = spark.read.parquet(input_path)\ntotal_jobs = jobs_df.count()\n\nprint(f'loaded {total_jobs:,} jobs')\nprint(f'columns: {jobs_df.columns}')\nprint('sample:')\njobs_df.select('id', 'job_title', 'embedding_text').show(3, truncate=60)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fast ID assignment function defined (uses monotonically_increasing_id)\n"
     ]
    }
   ],
   "source": [
    "# Fast ID assignment using monotonically_increasing_id()---> ~100x faster than zipWithIndex because it doesn't require shuffle\n",
    "# IDs will have gaps (B0, B1, B8589934592...) should not be a problem for embeddings\n",
    "\n",
    "def assign_sequential_ids_fast(df, prefix='B'):\n",
    "    if 'id' in df.columns:\n",
    "        df = df.withColumnRenamed('id', 'original_uuid')\n",
    "    \n",
    "    # monotonically_increasing_id, distributed and fast\n",
    "    df = df.withColumn(\n",
    "        'job_id',\n",
    "        concat(lit(prefix), monotonically_increasing_id().cast('string'))\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print('fast ID assignment function defined (uses monotonically_increasing_id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assigning B IDs (fast method)...\n",
      "assigned IDs to 1,345,711 jobs\n",
      "sample with IDs:\n",
      "+------+------------------------------------+------------------------------------------------------------+\n",
      "|job_id|                       original_uuid|                                                   job_title|\n",
      "+------+------------------------------------+------------------------------------------------------------+\n",
      "|    B0|71af9d85-4b75-40d6-9835-38d7153df1d5|       \"Telephonic\" Nurse Case Manager II at Elevance Health|\n",
      "|    B1|e19c544b-810f-42b8-a861-b68bc9be0d71|       \"Telephonic\" Nurse Case Manager II at Elevance Health|\n",
      "|    B2|4f9d65ee-1255-429e-8fcd-127196cc1080|                 $10,000 Sign on Bonus Laboratory Supervisor|\n",
      "|    B3|fb06029b-149b-4093-903a-f86a06e182a5|       $10K Bonus - In Patient RN, Dialysis Registered Nurse|\n",
      "|    B4|83ba594c-6dd6-46e5-93f0-0646e2a1cbeb|                       $16/Hr Experienced Department Manager|\n",
      "|    B5|b84d9d28-40f7-4086-a976-e11f2ac4c72c|$17.00 Anytime availability -Shift Manager Starting at $1...|\n",
      "|    B6|27278ec8-6a38-41e2-a9a2-dfbe857d26d5|$17.00 Anytime availability -Shift Manager Starting at $1...|\n",
      "|    B7|e21dc63b-1840-482b-b892-ec1021bbb1c2|                                    $18.00 Assistant Manager|\n",
      "|    B8|c81f0055-53bf-4a98-9340-5fddeb36cd15|                                   $20-$25 Assistant Manager|\n",
      "|    B9|fe6e7e29-822b-43f6-b6a5-72df4f7576e1|                       $25/hr Customer Service Represetative|\n",
      "+------+------------------------------------+------------------------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "print('assigning B IDs (fast method)...')\n",
    "jobs_with_ids = assign_sequential_ids_fast(jobs_df, prefix='B')\n",
    "\n",
    "# count is now fast since we're not using RDD\n",
    "job_count = jobs_with_ids.count()\n",
    "print(f'assigned IDs to {job_count:,} jobs')\n",
    "\n",
    "print('sample with IDs:')\n",
    "jobs_with_ids.select('job_id', 'original_uuid', 'job_title').show(10, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISCO classification function defined, 10 ISCO major groups configured\n"
     ]
    }
   ],
   "source": [
    "# ISCO-08 classification patterns based on International Standard Classification of Occupations\n",
    "#ref: https://www.ilo.org/public/english/bureau/stat/isco/isco08/\n",
    "\n",
    "ISCO_PATTERNS = {\n",
    "    1: ['manager', 'director', 'chief', 'head of', 'president', 'vp ', 'vice president', \n",
    "        'ceo', 'cfo', 'cto', 'coo', 'executive', 'supervisor', 'lead', 'principal'],\n",
    "    2: ['engineer', 'developer', 'scientist', 'doctor', 'nurse', 'teacher', 'professor',\n",
    "        'architect', 'designer', 'analyst', 'consultant', 'specialist', 'attorney', \n",
    "        'lawyer', 'accountant', 'pharmacist', 'therapist', 'physician'],\n",
    "    3: ['technician', 'associate', 'assistant', 'coordinator', 'representative',\n",
    "        'administrator', 'officer', 'agent', 'inspector', 'controller'],\n",
    "    4: ['clerk', 'secretary', 'receptionist', 'cashier', 'teller', 'bookkeeper',\n",
    "        'data entry', 'typist', 'filing'],\n",
    "    5: ['sales', 'retail', 'customer service', 'waiter', 'waitress', 'bartender',\n",
    "        'cook', 'chef', 'server', 'barista', 'store', 'shop'],\n",
    "    6: ['farmer', 'agricultural', 'fisherman', 'forestry', 'gardener', 'rancher'],\n",
    "    7: ['electrician', 'mechanic', 'plumber', 'carpenter', 'welder', 'machinist',\n",
    "        'installer', 'repair', 'maintenance', 'hvac'],\n",
    "    8: ['driver', 'operator', 'machine', 'assembler', 'production', 'manufacturing',\n",
    "        'warehouse', 'forklift', 'truck'],\n",
    "    9: ['cleaner', 'janitor', 'helper', 'laborer', 'packer', 'loader', 'dishwasher',\n",
    "        'housekeeper', 'security guard'],\n",
    "    0: ['military', 'army', 'navy', 'marine', 'air force', 'soldier']\n",
    "}\n",
    "\n",
    "ISCO_NAMES = {\n",
    "    0: 'Military',\n",
    "    1: 'Managers',\n",
    "    2: 'Professionals', \n",
    "    3: 'Technicians',\n",
    "    4: 'Clerical',\n",
    "    5: 'Service/Sales',\n",
    "    6: 'Agriculture',\n",
    "    7: 'Craft/Trade',\n",
    "    8: 'Operators',\n",
    "    9: 'Elementary'\n",
    "}\n",
    "\n",
    "def classify_isco(job_title):\n",
    "    #classify job title into ISCO-08 major group\n",
    "    if not job_title:\n",
    "        return 2  # default to Professionals\n",
    "    \n",
    "    title_lower = job_title.lower()\n",
    "    \n",
    "    # check each ISCO group's patterns\n",
    "    for isco_code, patterns in ISCO_PATTERNS.items():\n",
    "        for pattern in patterns:\n",
    "            if pattern in title_lower:\n",
    "                return isco_code\n",
    "    \n",
    "    # default to Professionals (most common in job datasets)\n",
    "    return 2\n",
    "\n",
    "# register as Spark UDF\n",
    "classify_isco_udf = udf(classify_isco, IntegerType())\n",
    "\n",
    "print('ISCO classification function defined, 10 ISCO major groups configured')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifying jobs by ISCO domain\n",
      "ISCO distribution (before sampling):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=================>                                        (4 + 9) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 - Military       :        639 (  0.0%)\n",
      "  1 - Managers       :    492,215 ( 36.6%)\n",
      "  2 - Professionals  :    691,783 ( 51.4%)\n",
      "  3 - Technicians    :    100,870 (  7.5%)\n",
      "  4 - Clerical       :      9,981 (  0.7%)\n",
      "  5 - Service/Sales  :     25,491 (  1.9%)\n",
      "  6 - Agriculture    :        157 (  0.0%)\n",
      "  7 - Craft/Trade    :     10,200 (  0.8%)\n",
      "  8 - Operators      :     11,525 (  0.9%)\n",
      "  9 - Elementary     :      2,850 (  0.2%)\n",
      "\n",
      "total jobs: 1,345,711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# apply ISCO classification to all jobs\n",
    "print('classifying jobs by ISCO domain')\n",
    "\n",
    "jobs_with_isco = jobs_with_ids.withColumn(\n",
    "    'isco_code',\n",
    "    classify_isco_udf(col('job_title'))\n",
    ")\n",
    "\n",
    "# show distribution\n",
    "print('ISCO distribution (before sampling):')\n",
    "isco_counts = jobs_with_isco.groupBy('isco_code').count().orderBy('isco_code').collect()\n",
    "\n",
    "total = sum(row['count'] for row in isco_counts)\n",
    "for row in isco_counts:\n",
    "    code = row['isco_code']\n",
    "    count = row['count']\n",
    "    pct = count / total * 100\n",
    "    name = ISCO_NAMES.get(code, 'Unknown')\n",
    "    print(f'  {code} - {name:15s}: {count:>10,} ({pct:5.1f}%)')\n",
    "\n",
    "print(f'\\ntotal jobs: {total:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating stratified sampling fractions, target: 150,000 jobs, minimum per domain: 1,000\n",
      "\n",
      "sampling fractions per domain:\n",
      "  0 - Military       : 1.000 (target ~639)\n",
      "  1 - Managers       : 0.121 (target ~54,355)\n",
      "  2 - Professionals  : 0.121 (target ~76,393)\n",
      "  3 - Technicians    : 0.121 (target ~11,139)\n",
      "  4 - Clerical       : 0.121 (target ~1,102)\n",
      "  5 - Service/Sales  : 0.121 (target ~2,814)\n",
      "  6 - Agriculture    : 1.000 (target ~157)\n",
      "  7 - Craft/Trade    : 0.121 (target ~1,126)\n",
      "  8 - Operators      : 0.121 (target ~1,272)\n",
      "  9 - Elementary     : 0.386 (target ~1,000)\n"
     ]
    }
   ],
   "source": [
    "# calculate stratified sampling fractions, proportional sampling with minimum per domain\n",
    "\n",
    "print(f'calculating stratified sampling fractions, target: {TARGET_SAMPLE_SIZE:,} jobs, minimum per domain: {MIN_PER_DOMAIN:,}')\n",
    "\n",
    "# get counts per domain\n",
    "domain_counts = {row['isco_code']: row['count'] for row in isco_counts}\n",
    "\n",
    "# calculate how many to sample from each domain, proportional to size, but ensure minimum\n",
    "sample_per_domain = {}\n",
    "remaining_budget = TARGET_SAMPLE_SIZE\n",
    "\n",
    "# first pass: allocate minimum to small domains\n",
    "for code, count in domain_counts.items():\n",
    "    if count < MIN_PER_DOMAIN:\n",
    "        # take all if domain is smaller than minimum\n",
    "        sample_per_domain[code] = count\n",
    "        remaining_budget -= count\n",
    "    elif count * (TARGET_SAMPLE_SIZE / total) < MIN_PER_DOMAIN:\n",
    "        # domain is small, give it minimum\n",
    "        sample_per_domain[code] = MIN_PER_DOMAIN\n",
    "        remaining_budget -= MIN_PER_DOMAIN\n",
    "\n",
    "# second pass: proportional allocation for remaining\n",
    "remaining_domains = {k: v for k, v in domain_counts.items() if k not in sample_per_domain}\n",
    "remaining_total = sum(remaining_domains.values())\n",
    "\n",
    "for code, count in remaining_domains.items():\n",
    "    proportion = count / remaining_total\n",
    "    sample_per_domain[code] = int(proportion * remaining_budget)\n",
    "\n",
    "# calculate fractions for Spark sampleBy\n",
    "fractions = {}\n",
    "for code, sample_count in sample_per_domain.items():\n",
    "    domain_total = domain_counts[code]\n",
    "    fraction = min(1.0, sample_count / domain_total * 1.1)  # 10% buffer for randomness\n",
    "    fractions[code] = fraction\n",
    "\n",
    "print('\\nsampling fractions per domain:')\n",
    "for code in sorted(fractions.keys()):\n",
    "    name = ISCO_NAMES.get(code, 'Unknown')\n",
    "    frac = fractions[code]\n",
    "    target = sample_per_domain[code]\n",
    "    print(f'  {code} - {name:15s}: {frac:.3f} (target ~{target:,})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing stratified sampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:===========================================>            (10 + 3) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled 165,193 jobs (target: 150,000, sampling ratio: 12.3% of original\n",
      "\n",
      "ISCO distribution (after sampling):\n",
      "  0 - Military       :      639 (  0.4%) [from 639]\n",
      "  1 - Managers       :   59,897 ( 36.3%) [from 492,215]\n",
      "  2 - Professionals  :   84,320 ( 51.0%) [from 691,783]\n",
      "  3 - Technicians    :   12,146 (  7.4%) [from 100,870]\n",
      "  4 - Clerical       :    1,220 (  0.7%) [from 9,981]\n",
      "  5 - Service/Sales  :    3,051 (  1.8%) [from 25,491]\n",
      "  6 - Agriculture    :      157 (  0.1%) [from 157]\n",
      "  7 - Craft/Trade    :    1,255 (  0.8%) [from 10,200]\n",
      "  8 - Operators      :    1,418 (  0.9%) [from 11,525]\n",
      "  9 - Elementary     :    1,090 (  0.7%) [from 2,850]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# perform stratified sampling using Spark's sampleBy\n",
    "print('performing stratified sampling')\n",
    "\n",
    "sampled_jobs = jobs_with_isco.sampleBy('isco_code', fractions, seed=42)\n",
    "\n",
    "# cache and count\n",
    "sampled_jobs.cache()\n",
    "sampled_count = sampled_jobs.count()\n",
    "\n",
    "print(f'sampled {sampled_count:,} jobs (target: {TARGET_SAMPLE_SIZE:,}, sampling ratio: {sampled_count/total*100:.1f}% of original')\n",
    "\n",
    "# show new distribution\n",
    "print('\\nISCO distribution (after sampling):')\n",
    "sampled_isco = sampled_jobs.groupBy('isco_code').count().orderBy('isco_code').collect()\n",
    "\n",
    "for row in sampled_isco:\n",
    "    code = row['isco_code']\n",
    "    count = row['count']\n",
    "    pct = count / sampled_count * 100\n",
    "    name = ISCO_NAMES.get(code, 'Unknown')\n",
    "    orig = domain_counts.get(code, 0)\n",
    "    print(f'  {code} - {name:15s}: {count:>8,} ({pct:5.1f}%) [from {orig:,}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added passage prefix to sampled jobs\n",
      "sample:\n",
      "+------+---------+----------------------------------------------------------------------+\n",
      "|job_id|isco_code|                                                        embedding_text|\n",
      "+------+---------+----------------------------------------------------------------------+\n",
      "|    B7|        1|passage: Role of $18.00 Assistant Manager at McDonald's in Whitevil...|\n",
      "|   B16|        1|passage: Role of $4,500 Sign on Bonus - MDS Coordinator at The Good...|\n",
      "|   B18|        2|passage: Role of $45/hr - School RN at Maxim Healthcare Staffing in...|\n",
      "|   B50|        1|passage: Role of (CAN) Department Manager -FR at Walmart Canada in ...|\n",
      "|   B58|        1|passage: Role of (CAN) Department Manager at Walmart Canada in Kitc...|\n",
      "+------+---------+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# add passage prefix to sampled jobs\n",
    "sampled_with_prefix = sampled_jobs.withColumn(\n",
    "    'embedding_text',\n",
    "    concat(lit('passage: '), col('embedding_text'))\n",
    ")\n",
    "\n",
    "print('added passage prefix to sampled jobs')\n",
    "print('sample:')\n",
    "sampled_with_prefix.select('job_id', 'isco_code', 'embedding_text').show(5, truncate=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final output columns:\n",
      "['job_id', 'embedding_text', 'isco_code']\n",
      "total records: 165,193\n",
      "\n",
      "sample:\n",
      "+------+----------------------------------------------------------------------+---------+\n",
      "|job_id|                                                        embedding_text|isco_code|\n",
      "+------+----------------------------------------------------------------------+---------+\n",
      "|    B7|passage: Role of $18.00 Assistant Manager at McDonald's in Whitevil...|        1|\n",
      "|   B16|passage: Role of $4,500 Sign on Bonus - MDS Coordinator at The Good...|        1|\n",
      "|   B18|passage: Role of $45/hr - School RN at Maxim Healthcare Staffing in...|        2|\n",
      "|   B50|passage: Role of (CAN) Department Manager -FR at Walmart Canada in ...|        1|\n",
      "|   B58|passage: Role of (CAN) Department Manager at Walmart Canada in Kitc...|        1|\n",
      "|   B63|passage: Role of (CAN) Department Manager at Walmart Canada in Peac...|        1|\n",
      "|   B66|passage: Role of (CAN) GT - Senior Software Developer at Walmart Ca...|        2|\n",
      "|   B72|passage: Role of (CAN) Staff Pharmacist at Walmart Canada in Parry ...|        2|\n",
      "|   B79|passage: Role of (EXT) Technology Risk - Assurance - IT Audit Manag...|        1|\n",
      "|   B88|passage: Role of (Global Oil Gas) Business Communication Partner, T...|        2|\n",
      "+------+----------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# select final columns for embedding\n",
    "final_output = sampled_with_prefix.select('job_id', 'embedding_text', 'isco_code')\n",
    "\n",
    "print('final output columns:')\n",
    "print(final_output.columns)\n",
    "print(f'total records: {sampled_count:,}')\n",
    "print('\\nsample:')\n",
    "final_output.show(10, truncate=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 165,193 jobs to: /home/developer/project/output/final/jobs_to_embed.parquet\n"
     ]
    }
   ],
   "source": "# save final output\noutput_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'final', 'jobs_to_embed.parquet')\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\n\n# save with isco_code for later stratified splitting\nfinal_output.coalesce(1).write.mode('overwrite').parquet(output_path)\n\nprint(f'saved {sampled_count:,} jobs to: {output_path}')"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved full UUID mapping (1,345,711 jobs) to: /home/developer/project/output/final/uuid_to_bid_mapping.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": "# save full UUID to B ID mapping (for all 1.3M jobs, not just sampled)\nmapping_df = jobs_with_ids.select('original_uuid', 'job_id')\nmapping_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'final', 'uuid_to_bid_mapping.parquet')\nmapping_df.coalesce(1).write.mode('overwrite').parquet(mapping_path)\n\nprint(f'saved full UUID mapping ({total_jobs:,} jobs) to: {mapping_path}')"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved ISCO distribution to: /home/developer/project/output/final/isco_distribution.json\n"
     ]
    }
   ],
   "source": "# save ISCO distribution as JSON for reference\nisco_dist = {\n    'original_total': total,\n    'sampled_total': sampled_count,\n    'target_size': TARGET_SAMPLE_SIZE,\n    'domains': {}\n}\n\nfor row in sampled_isco:\n    code = row['isco_code']\n    count = row['count']\n    isco_dist['domains'][str(code)] = {\n        'name': ISCO_NAMES.get(code, 'Unknown'),\n        'original_count': domain_counts.get(code, 0),\n        'sampled_count': count,\n        'percentage': round(count / sampled_count * 100, 1)\n    }\n\nisco_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'final', 'isco_distribution.json')\nwith open(isco_path, 'w') as f:\n    json.dump(isco_dist, f, indent=2)\n\nprint(f'saved ISCO distribution to: {isco_path}')"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafka producer initialized\n"
     ]
    }
   ],
   "source": [
    "kafka_config = {\n",
    "    'bootstrap.servers': KAFKA_BROKER,\n",
    "    'client.id': 'final-embedding-producer'\n",
    "}\n",
    "\n",
    "try:\n",
    "    producer = Producer(kafka_config)\n",
    "    print('kafka producer initialized')\n",
    "    kafka_available = True\n",
    "except Exception as e:\n",
    "    print(f'kafka not available: {e}')\n",
    "    print('skipping kafka publishing')\n",
    "    kafka_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publishing to kafka topic: jobs_to_embed\n",
      " published 10,000 jobs...\n",
      " published 20,000 jobs...\n",
      " published 30,000 jobs...\n",
      " published 40,000 jobs...\n",
      " published 50,000 jobs...\n",
      " published 60,000 jobs...\n",
      " published 70,000 jobs...\n",
      " published 80,000 jobs...\n",
      " published 90,000 jobs...\n",
      " published 100,000 jobs...\n",
      " published 110,000 jobs...\n",
      " published 120,000 jobs...\n",
      " published 130,000 jobs...\n",
      " published 140,000 jobs...\n",
      " published 150,000 jobs...\n",
      " published 160,000 jobs...\n",
      "\n",
      "published 165,193 jobs to kafka\n"
     ]
    }
   ],
   "source": [
    "if kafka_available:\n",
    "    print('publishing to kafka topic: jobs_to_embed')\n",
    "    \n",
    "    # collect to pandas for iteration\n",
    "    pdf = final_output.select('job_id', 'embedding_text').toPandas()\n",
    "    \n",
    "    published = 0\n",
    "    for idx, row in pdf.iterrows():\n",
    "        msg = json.dumps({\n",
    "            'job_id': row['job_id'],\n",
    "            'embedding_text': row['embedding_text']\n",
    "        })\n",
    "        \n",
    "        producer.produce('jobs_to_embed', value=msg.encode('utf-8'))\n",
    "        published += 1\n",
    "        \n",
    "        if published % 10000 == 0:\n",
    "            producer.flush()\n",
    "            print(f' published {published:,} jobs...')\n",
    "    \n",
    "    producer.flush()\n",
    "    print(f'\\npublished {published:,} jobs to kafka')\n",
    "else:\n",
    "    published = 0\n",
    "    print('kafka publishing skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original jobs: 1,345,711\n",
      "sampled jobs: 165,193\n",
      "sampling ratio: 12.3%\n",
      "\n",
      "ISCO domains: 10\n",
      "\n",
      "embedding text lengths:\n",
      " average: 339 chars\n",
      " min: 95 chars\n",
      " max: 2282 chars\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, min as spark_min, max as spark_max\n",
    "\n",
    "length_stats = final_output.agg(\n",
    "    avg(length('embedding_text')).alias('avg_len'),\n",
    "    spark_min(length('embedding_text')).alias('min_len'),\n",
    "    spark_max(length('embedding_text')).alias('max_len')\n",
    ").collect()[0]\n",
    "\n",
    "print(f'original jobs: {total_jobs:,}')\n",
    "print(f'sampled jobs: {sampled_count:,}')\n",
    "print(f'sampling ratio: {sampled_count/total_jobs*100:.1f}%')\n",
    "print(f'\\nISCO domains: {len(sampled_isco)}')\n",
    "print(f'\\nembedding text lengths:')\n",
    "print(f' average: {length_stats[\"avg_len\"]:.0f} chars')\n",
    "print(f' min: {length_stats[\"min_len\"]} chars')\n",
    "print(f' max: {length_stats[\"max_len\"]} chars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: /home/developer/project/output/unified_job_postings/unified_jobs.parquet\n",
      "\n",
      "stratified sampling:\n",
      " original: 1,345,711 jobs\n",
      " sampled: 165,193 jobs\n",
      " ratio: 1:8\n",
      "\n",
      "outputs:\n",
      " parquet: /home/developer/project/output/final/jobs_to_embed.parquet\n",
      " mapping: /home/developer/project/output/final/uuid_to_bid_mapping.parquet\n",
      " isco: /home/developer/project/output/final/isco_distribution.json\n",
      " kafka: jobs_to_embed topic (165,193 messages)\n",
      "\n",
      "balance with CVs:\n",
      "  7K CVs : 165,193 jobs = 1:23\n"
     ]
    }
   ],
   "source": [
    "print(f'input: {input_path}')\n",
    "print(f'\\nstratified sampling:')\n",
    "print(f' original: {total_jobs:,} jobs')\n",
    "print(f' sampled: {sampled_count:,} jobs')\n",
    "print(f' ratio: 1:{total_jobs//sampled_count}')\n",
    "print(f'\\noutputs:')\n",
    "print(f' parquet: {output_path}')\n",
    "print(f' mapping: {mapping_path}')\n",
    "print(f' isco: {isco_path}')\n",
    "if kafka_available:\n",
    "    print(f' kafka: jobs_to_embed topic ({published:,} messages)')\n",
    "print(f'\\nbalance with CVs:')\n",
    "print(f'  7K CVs : {sampled_count:,} jobs = 1:{sampled_count//7000}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark stopped\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print('spark stopped')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports loaded\n",
      "spaCy version: 3.8.11\n",
      "project root: /home/developer/project\n",
      "kafka broker: kafka-broker:29092\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, concat_ws, lit\n",
    "from pyspark.sql.types import ArrayType, StringType, StructType, StructField\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "# setup paths - detect project root\n",
    "cwd = os.getcwd()\n",
    "if 'notebooks' in cwd:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# kafka config\n",
    "KAFKA_BROKER = os.environ.get('KAFKA_BROKER', 'kafka-broker:29092')\n",
    "\n",
    "print('imports loaded')\n",
    "print(f'spaCy version: {spacy.__version__}')\n",
    "print(f'project root: {PROJECT_ROOT}')\n",
    "print(f'kafka broker: {KAFKA_BROKER}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/27 01:27:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark started: 4.1.1\n",
      "spark app name: NLPExtractionWithSpaCy\n"
     ]
    }
   ],
   "source": [
    "# start spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('NLPExtractionWithSpaCy') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f'spark started: {spark.version}')\n",
    "print(f'spark app name: {spark.sparkContext.appName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1,348,711 jobs\n",
      "\n",
      "by source:\n",
      "+---------+-------+\n",
      "|   source|  count|\n",
      "+---------+-------+\n",
      "| linkedin|1348454|\n",
      "|   indeed|    100|\n",
      "|glassdoor|    157|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": "# load jobs from processed linkedin output\nlinkedin_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'processed', 'linkedin', 'linkedin_jobs_with_skills')\njobs_df = spark.read.parquet(linkedin_path)\n\ntotal_jobs = jobs_df.count()\nprint(f'loaded {total_jobs:,} jobs')\n\n# show source distribution\nprint('\\nby source:')\njobs_df.groupBy('source').count().show()"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indeed/glassdoor jobs: 257\n"
     ]
    }
   ],
   "source": [
    "# filter to indeed/glassdoor jobs needing extraction\n",
    "indeed_glassdoor_df = jobs_df.filter(col('source').isin(['indeed', 'glassdoor']))\n",
    "\n",
    "ig_count = indeed_glassdoor_df.count()\n",
    "print(f'indeed/glassdoor jobs: {ig_count:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 22,245 skills (min 100 occurrences)\n",
      "\n",
      "collecting skills to driver\n",
      "after filtering stopwords/generic words: 22,104 skills\n",
      "removed 141 noisy entries\n"
     ]
    }
   ],
   "source": "# load skill dictionary\nskill_dict_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'skill_dictionary', 'all_skills')\nskill_df = spark.read.parquet(skill_dict_path)\n\n# !! using min 100 occurrences to filter out noise\n# the raw dictionary has 3.3M entries including stopwords and generic words\nskill_df_filtered = skill_df.filter(col('count') >= 100)\n\nskill_count = skill_df_filtered.count()\nprint(f'loaded {skill_count:,} skills (min 100 occurrences)')\n\n# define stopwords to filter out\nSTOPWORDS = {'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', \n             'with', 'by', 'from', 'as', 'is', 'was', 'are', 'were', 'been', 'be', 'have',\n             'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',\n             'might', 'must', 'shall', 'can', 'need', 'our', 'we', 'you', 'your', 'they',\n             'their', 'its', 'it', 'that', 'this', 'which', 'what', 'who', 'whom', 'any',\n             'all', 'some', 'more', 'most', 'other', 'each', 'few', 'many', 'such', 'no',\n             'not', 'only', 'same', 'so', 'than', 'too', 'very', 'just', 'also', 'now'}\n\n# generic resume words, appear in almost every job description but aren't skills\nGENERIC_RESUME_WORDS = {'experience', 'years', 'strong', 'team', 'work', 'working', 'position',\n                        'ability', 'able', 'excellent', 'good', 'great', 'skills', 'knowledge',\n                        'understanding', 'familiarity', 'proficiency', 'expertise', 'demonstrated',\n                        'proven', 'equivalent', 'gain', 'various', 'different', 'multiple', 'new',\n                        'within', 'across', 'using', 'including', 'related', 'required', 'preferred',\n                        'minimum', 'ideal', 'desirable', 'essential', 'degree', 'bachelor', 'master',\n                        'diploma', 'intern', 'internship', 'job', 'role', 'opportunity', 'employment',\n                        'company', 'organization', 'business', 'environment', 'requirements', \n                        'responsibilities', 'status', 'seeking', 'level', 'current', 'support'}\n\n# ultra-generic single words that are NOT skills when alone\n#keep multi-word versions like 'data analysis' but filter single 'data'\n# also filter job titles when they appear as single words\nULTRA_GENERIC_SINGLE = {'data', 'information', 'process', 'systems', 'solutions',\n                        'pull', 'building', 'tools', 'supporting', 'identification',\n                        'preparation', 'methodology', 'investigations', 'engagement',\n                        'developer', 'engineer', 'analyst', 'scientist', 'manager',\n                        'coordinator', 'director', 'supervisor', 'administrator',\n                        'assistant', 'associate', 'officer', 'executive', 'remote',\n                        'exposure', 'issues', 'structure', 'structures', 'oversight',\n                        'computer', 'lead', 'maintenance'}\n\ndef is_quality_skill(skill):\n    #filter out stopwords, generic resume words, and ultra-generic single words\n    skill_lower = skill.lower().strip()\n    \n    # length check\n    if len(skill_lower) < 3:\n        return False\n    \n    # stopwords\n    if skill_lower in STOPWORDS:\n        return False\n    \n    # generic resume words  \n    if skill_lower in GENERIC_RESUME_WORDS:\n        return False\n    \n    # for single words only, filter ultra-generic terms\n    # keep multi-word phrases like 'data analysis', 'project management', 'remote work'\n    if ' ' not in skill_lower and skill_lower in ULTRA_GENERIC_SINGLE:\n        return False\n        \n    return True\n\n# collect to driver and filter\nprint('\\ncollecting skills to driver')\nraw_skills = [row['skill'] for row in skill_df_filtered.select('skill').collect()]\nskills_list = [s.lower() for s in raw_skills if is_quality_skill(s)]\n\nprint(f'after filtering stopwords/generic words: {len(skills_list):,} skills')\nprint(f'removed {len(raw_skills) - len(skills_list):,} noisy entries')"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy model loaded: en\n",
      "building phrase patterns\n",
      "added 22,104 patterns to matcher\n",
      "phrasematcher ready\n"
     ]
    }
   ],
   "source": [
    "# initialize spacy blank model (just tokenizer, no heavy NLP)\n",
    "nlp = spacy.blank('en')\n",
    "print(f'spacy model loaded: {nlp.lang}')\n",
    "\n",
    "# create PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')  # case-insensitive matching\n",
    "\n",
    "# convert skills to patterns\n",
    "print('building phrase patterns')\n",
    "patterns = [nlp.make_doc(skill) for skill in skills_list]\n",
    "matcher.add('SKILLS', patterns)\n",
    "\n",
    "print(f'added {len(patterns):,} patterns to matcher')\n",
    "print('phrasematcher ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy skill extractor defined\n"
     ]
    }
   ],
   "source": [
    "# skill extraction using PhraseMatcher\n",
    "def extract_skills_spacy(text):\n",
    "    #extract skills using spacy PhraseMatcher\n",
    "    # FAST trie-based lookup, not regex loops\n",
    "    \n",
    "    if not text or not isinstance(text, str):\n",
    "        return []\n",
    "    \n",
    "    doc = nlp(text.lower())\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # extract unique matched skills\n",
    "    found_skills = list(set([doc[start:end].text for _, start, end in matches]))\n",
    "    \n",
    "    return found_skills\n",
    "\n",
    "print('spacy skill extractor defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing skill extraction:\n",
      "------------------------------------------------------------\n",
      "text: Senior Python Developer needed. 5+ years with Django, AWS, PostgreSQL. Remote.\n",
      "skills: ['postgresql', 'aws', 'django', 'python']\n",
      "------------------------------------------------------------\n",
      "text: Data Scientist with Machine Learning, TensorFlow, PyTorch experience.\n",
      "skills: ['pytorch', 'learning', 'machine learning', 'tensorflow']\n",
      "------------------------------------------------------------\n",
      "text: Full Stack Engineer: React, Node.js, MongoDB, Docker, Kubernetes.\n",
      "skills: ['react', 'mongodb', 'node.js', 'docker', 'kubernetes', 'full stack']\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# test extractor with multiple examples\n",
    "test_texts = [\n",
    "    'Senior Python Developer needed. 5+ years with Django, AWS, PostgreSQL. Remote.',\n",
    "    'Data Scientist with Machine Learning, TensorFlow, PyTorch experience.',\n",
    "    'Full Stack Engineer: React, Node.js, MongoDB, Docker, Kubernetes.'\n",
    "]\n",
    "\n",
    "print('testing skill extraction:')\n",
    "print('-' * 60)\n",
    "for text in test_texts:\n",
    "    skills = extract_skills_spacy(text)\n",
    "    print(f'text: {text}')\n",
    "    print(f'skills: {skills}')\n",
    "    print('-' * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcasted 22,104 skills to executors\n"
     ]
    }
   ],
   "source": [
    "# broadcast skills and matcher to executors\n",
    "# this makes them available on all worker nodes\n",
    "broadcast_skills = spark.sparkContext.broadcast(skills_list)\n",
    "\n",
    "print(f'broadcasted {len(skills_list):,} skills to executors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark udf created for skill extraction\n"
     ]
    }
   ],
   "source": [
    "# create UDF for skill extraction\n",
    "# this allows spark to call our python function in distributed way\n",
    "skill_extraction_udf = udf(extract_skills_spacy, ArrayType(StringType()))\n",
    "\n",
    "print('spark udf created for skill extraction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting skills from 257 jobs using spark distributed processing\n",
      "extraction complete, processed 257 jobs in 0.1s\n"
     ]
    }
   ],
   "source": [
    "# apply skill extraction to all indeed/glassdoor jobs\n",
    "print(f'extracting skills from {ig_count:,} jobs using spark distributed processing')\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# add skills column using UDF\n",
    "extracted_df = indeed_glassdoor_df.withColumn(\n",
    "    'extracted_skills',\n",
    "    skill_extraction_udf(col('description'))\n",
    ")\n",
    "\n",
    "# convert array to comma-separated string\n",
    "extracted_df = extracted_df.withColumn(\n",
    "    'skills',\n",
    "    concat_ws(', ', col('extracted_skills'))\n",
    ")\n",
    "\n",
    "# trigger execution by counting\n",
    "result_count = extracted_df.count()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f'extraction complete, processed {result_count:,} jobs in {elapsed:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample extractions:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=================================>                        (4 + 3) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+------------------------+------------------------------------------------------------+\n",
      "|                                      job_title|                 company|                                                      skills|\n",
      "+-----------------------------------------------+------------------------+------------------------------------------------------------+\n",
      "|     IN-16092 Conservation Data Analysis Intern|     World Wildlife Fund|database, management, conservation, data management, scie...|\n",
      "|   Data Science Analyst for The Weather Company|                     IBM|management, acquisition, quality, data management, data m...|\n",
      "|                                 Data Scientist|     Booz Allen Hamilton|                            science, data science, data sets|\n",
      "|                        Healthcare Data Analyst|              Everis USA|                            health, healthcare, data analyst|\n",
      "|                             Sr. Data Scientist|     CHASE Professionals|physics, bachelor\u2019s degree, computer science, science, ma...|\n",
      "|IN-16093 Conservation Data Visualization Intern|     World Wildlife Fund|visualization tools, data visualization tools, developmen...|\n",
      "|                                 Data Scientist|                  Verato|                                                            |\n",
      "|                                 Data Scientist|National Security Agency|mathematics, computer science, science, statistics, data ...|\n",
      "|                                 Data Scientist|                Raytheon|software engineering, science, engineering, software, dat...|\n",
      "|                                 Data Scientist|                     CGI|documentation, design, data preparation, analytics, data ...|\n",
      "+-----------------------------------------------+------------------------+------------------------------------------------------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show sample results\n",
    "print('sample extractions:')\n",
    "extracted_df.select('job_title', 'company', 'skills').show(10, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extraction statistics:\n",
      " total jobs: 257\n",
      " with skills: 256 (99.6%)\n",
      " without skills: 1 (0.4%)\n"
     ]
    }
   ],
   "source": [
    "# extraction stats\n",
    "from pyspark.sql.functions import size, when\n",
    "\n",
    "# count skills extracted\n",
    "stats_df = extracted_df.withColumn(\n",
    "    'skill_count',\n",
    "    size(col('extracted_skills'))\n",
    ").withColumn(\n",
    "    'has_skills',\n",
    "    when(col('skill_count') > 0, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "has_skills = stats_df.filter(col('has_skills') == 1).count()\n",
    "total = stats_df.count()\n",
    "\n",
    "print(f'extraction statistics:')\n",
    "print(f' total jobs: {total:,}')\n",
    "print(f' with skills: {has_skills:,} ({has_skills/total*100:.1f}%)')\n",
    "print(f' without skills: {total - has_skills:,} ({(total-has_skills)/total*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafka producer initialized\n"
     ]
    }
   ],
   "source": [
    "# kafka configuration\n",
    "kafka_config = {\n",
    "    'bootstrap.servers': KAFKA_BROKER,\n",
    "    'client.id': 'nlp-extraction-producer'\n",
    "}\n",
    "\n",
    "producer = Producer(kafka_config)\n",
    "print('kafka producer initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "publishing to kafka topic: extracted_jobs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  published 100/257...\n",
      "  published 200/257...\n",
      "published 257 messages to kafka, kafka publishing complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# publish to kafka\n",
    "print('publishing to kafka topic: extracted_jobs')\n",
    "\n",
    "# collect to driver (small dataset - 771 jobs)\n",
    "results_pd = extracted_df.toPandas()\n",
    "\n",
    "published_count = 0\n",
    "for idx, row in results_pd.iterrows():\n",
    "    # create message\n",
    "    msg = {\n",
    "        'id': row['id'],\n",
    "        'job_title': row['job_title'],\n",
    "        'company': row['company'],\n",
    "        'source': row['source'],\n",
    "        'skills': row['skills'],\n",
    "        'extracted_skill_count': len(row['extracted_skills']),\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    # publish to kafka\n",
    "    producer.produce(\n",
    "        'extracted_jobs',\n",
    "        key=row['id'].encode('utf-8'),\n",
    "        value=json.dumps(msg).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    published_count += 1\n",
    "    \n",
    "    # flush every 100 messages\n",
    "    if published_count % 100 == 0:\n",
    "        producer.flush()\n",
    "        print(f'  published {published_count}/{len(results_pd)}...')\n",
    "\n",
    "# final flush\n",
    "producer.flush()\n",
    "print(f'published {published_count:,} messages to kafka, kafka publishing complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving to /home/developer/project/output/processed/indeed_glassdoor/indeed_glassdoor_extracted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 257 jobs to parquet\n"
     ]
    }
   ],
   "source": "# save extracted jobs to parquet\noutput_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'processed', 'indeed_glassdoor', 'indeed_glassdoor_extracted')\n\nprint(f'saving to {output_path}')\nextracted_df.write.mode('overwrite').parquet(output_path)\n\nprint(f'saved {result_count:,} jobs to parquet')"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP EXTRACTION COMPLETE\n",
      "processed: 257 indeed/glassdoor jobs\n",
      "extraction time: 0.1s\n",
      "technology used:\n",
      " - spark for distributed processing\n",
      " - spacy PhraseMatcher for fast skill matching\n",
      " - kafka for publishing results\n",
      "\n",
      "output:\n",
      " - parquet: /home/developer/project/output/processed/indeed_glassdoor/indeed_glassdoor_extracted\n",
      " - kafka: extracted_jobs topic\n"
     ]
    }
   ],
   "source": [
    "print('NLP EXTRACTION COMPLETE')\n",
    "print(f'processed: {result_count:,} indeed/glassdoor jobs')\n",
    "print(f'extraction time: {elapsed:.1f}s')\n",
    "print(f'technology used:')\n",
    "print(f' - spark for distributed processing')\n",
    "print(f' - spacy PhraseMatcher for fast skill matching')\n",
    "print(f' - kafka for publishing results')\n",
    "print(f'\\noutput:')\n",
    "print(f' - parquet: {output_path}')\n",
    "print(f' - kafka: extracted_jobs topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark stopped\n"
     ]
    }
   ],
   "source": [
    "# cleanup\n",
    "spark.stop()\n",
    "print('spark stopped')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
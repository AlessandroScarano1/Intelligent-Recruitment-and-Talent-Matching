{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from confluent_kafka import Producer, Consumer, KafkaError, KafkaException\n",
    "from confluent_kafka.admin import AdminClient, NewTopic\n",
    "\n",
    "print('Imports loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/developer/project\n",
      "Raw data: /home/developer/project/raw_data/job_postings\n",
      "Output: /home/developer/project/output/raw_job_postings\n",
      "Kafka broker: kafka-broker:29092\n",
      "\n",
      "Cleaning up ALL output folders for fresh pipeline run...\n",
      "Cleared: raw_job_postings\n",
      "Cleared: processed\n",
      "Cleared: skill_dictionary\n",
      "Cleared: unified_job_postings\n",
      "Cleared: final\n",
      "Cleared: embeddings\n",
      "Cleared: splits\n",
      "Output folders cleaned\n"
     ]
    }
   ],
   "source": [
    "# setup paths\n",
    "# this notebook expects to run from notebooks/ directory at project root\n",
    "import shutil\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# detect if we're in notebooks/ or project root\n",
    "if 'notebooks' in cwd:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "\n",
    "RAW_DATA_DIR = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'raw_data')\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'raw_job_postings')\n",
    "OUTPUT_ROOT = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output')\n",
    "\n",
    "# kafka config - using docker network name\n",
    "KAFKA_BROKER = os.environ.get('KAFKA_BROKER', 'kafka-broker:29092')\n",
    "TOPIC = 'raw_job_postings'\n",
    "DLQ_TOPIC = 'dlq_job_postings'\n",
    "\n",
    "print(f'Project root: {PROJECT_ROOT}')\n",
    "print(f'Raw data: {RAW_DATA_DIR}')\n",
    "print(f'Output: {OUTPUT_DIR}')\n",
    "print(f'Kafka broker: {KAFKA_BROKER}')\n",
    "\n",
    "# clean up ALL output folders for fresh pipeline run\n",
    "print('\\nCleaning up ALL output folders for fresh pipeline run...')\n",
    "folders_to_clean = [\n",
    "    'raw_job_postings',\n",
    "    'processed',\n",
    "    'skill_dictionary',\n",
    "    'unified_job_postings',\n",
    "    'final',\n",
    "    'embeddings',\n",
    "    'splits',\n",
    "]\n",
    "\n",
    "for folder in folders_to_clean:\n",
    "    folder_path = os.path.join(OUTPUT_ROOT, folder)\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f'Cleared: {folder}')\n",
    "    else:\n",
    "        print(f'Not found (ok): {folder}')\n",
    "\n",
    "print('Output folders cleaned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Kafka connection\n",
      "[OK] Connected to Kafka\n",
      "Existing topics: ['raw_job_postings', 'dlq_job_postings', '__consumer_offsets', 'extracted_jobs', 'jobs_to_embed']\n"
     ]
    }
   ],
   "source": [
    "# test kafka connection\n",
    "print('Testing Kafka connection')\n",
    "\n",
    "try:\n",
    "    admin = AdminClient({'bootstrap.servers': KAFKA_BROKER})\n",
    "    cluster = admin.list_topics(timeout=10)\n",
    "    print(f'[OK] Connected to Kafka')\n",
    "    print(f'Existing topics: {list(cluster.topics.keys())}')\n",
    "except Exception as e:\n",
    "    print(f'[ERROR] Cannot connect to Kafka: {e}')\n",
    "    print('\\nMake sure Kafka is running:')\n",
    "    print('docker-compose up -d')\n",
    "    raise SystemExit('Kafka not available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting Kafka topics for clean slate\n",
      "Cleaning up old topic data\n",
      "Deleted: raw_job_postings\n",
      "Deleted: dlq_job_postings\n",
      "Waiting for topic deletion\n",
      "Recreating topics\n",
      " Created topic: raw_job_postings\n",
      " Created topic: dlq_job_postings\n",
      "Topics ready\n"
     ]
    }
   ],
   "source": [
    "# function to create kafka topics\n",
    "def create_topics(broker, topics_config):\n",
    "    #create Kafka topics if they don't exist\n",
    "    admin = AdminClient({'bootstrap.servers': broker})\n",
    "    \n",
    "    for cfg in topics_config:\n",
    "        topic = NewTopic(\n",
    "            cfg['name'], \n",
    "            num_partitions=cfg['partitions'], \n",
    "            replication_factor=cfg['replication']\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            fs = admin.create_topics([topic])\n",
    "            for t, f in fs.items():\n",
    "                try:\n",
    "                    f.result()\n",
    "                    print(f' Created topic: {t}')\n",
    "                except Exception as e:\n",
    "                    if 'already exists' in str(e).lower():\n",
    "                        print(f'  Topic exists: {t}')\n",
    "                    else:\n",
    "                        print(f'  Error with {t}: {e}')\n",
    "        except Exception as e:\n",
    "            print(f'  Error: {e}')\n",
    "\n",
    "def reset_topics(broker, topics_config):\n",
    "    # delete and recreate topics for a clean slate.\n",
    "    # This prevents accumulation of old messages from previous runs, without this, consumer with auto.offset.reset='earliest' would\n",
    "    # read ALL messages ever produced to the topic\n",
    "    admin = AdminClient({'bootstrap.servers': broker})\n",
    "    topic_names = [cfg['name'] for cfg in topics_config]\n",
    "    \n",
    "    print('Cleaning up old topic data')\n",
    "    \n",
    "    # delete existing topics\n",
    "    try:\n",
    "        fs = admin.delete_topics(topic_names, operation_timeout=30)\n",
    "        for topic, future in fs.items():\n",
    "            try:\n",
    "                future.result()\n",
    "                print(f'Deleted: {topic}')\n",
    "            except Exception as e:\n",
    "                if 'does not exist' in str(e).lower() or 'unknown topic' in str(e).lower():\n",
    "                    print(f'Not found (ok): {topic}')\n",
    "                else:\n",
    "                    print(f'Error deleting {topic}: {e}')\n",
    "    except Exception as e:\n",
    "        print(f' Delete error: {e}')\n",
    "    \n",
    "    # wait for deletion to complete\n",
    "    print('Waiting for topic deletion')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # recreate topics\n",
    "    print('Recreating topics')\n",
    "    create_topics(broker, topics_config)\n",
    "\n",
    "# topic configuration\n",
    "TOPICS_CONFIG = [\n",
    "    {'name': 'raw_job_postings', 'partitions': 3, 'replication': 1},\n",
    "    {'name': 'dlq_job_postings', 'partitions': 1, 'replication': 1},\n",
    "]\n",
    "\n",
    "# IMPORTANT: reset topics to prevent duplicate data from previous runs\n",
    "print('Resetting Kafka topics for clean slate')\n",
    "reset_topics(KAFKA_BROKER, TOPICS_CONFIG)\n",
    "print('Topics ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data files\n",
      "  linkedin: 1,348,510 rows, 415.3 MB\n",
      "  linkedin_skills: 1,296,381 rows, 672.7 MB\n",
      "  indeed: 4,800 rows, 0.4 MB\n",
      "  glassdoor: 157 rows, 0.6 MB\n"
     ]
    }
   ],
   "source": [
    "# check data files exist\n",
    "print('Checking data files')\n",
    "\n",
    "data_files = {\n",
    "    'linkedin': 'linkedin_job_postings.csv',\n",
    "    'linkedin_skills': 'job_skills.csv',\n",
    "    'indeed': 'indeed_job_listings.csv',\n",
    "    'glassdoor': 'job_descriptions.csv',\n",
    "}\n",
    "\n",
    "for name, filename in data_files.items():\n",
    "    path = os.path.join(RAW_DATA_DIR, filename)\n",
    "    if os.path.exists(path):\n",
    "        size_mb = os.path.getsize(path) / 1e6\n",
    "        # count lines\n",
    "        with open(path, 'rb') as f:\n",
    "            lines = sum(1 for _ in f) - 1\n",
    "        print(f'  {name}: {lines:,} rows, {size_mb:.1f} MB')\n",
    "    else:\n",
    "        print(f' Missing: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source mappings defined\n"
     ]
    }
   ],
   "source": [
    "# column mappings for each data source\n",
    "# each source has different column names, so we map them to a unified schema\n",
    "SOURCE_MAPPINGS = {\n",
    "    'linkedin': {\n",
    "        'columns': {\n",
    "            'job_title': 'job_title',\n",
    "            'company': 'company',\n",
    "            'job_location': 'job_location',\n",
    "            'job_level': 'job_level',\n",
    "            'job_type': 'job_type',\n",
    "            'job_link': 'job_link',\n",
    "        },\n",
    "    },\n",
    "    'indeed': {\n",
    "        'columns': {\n",
    "            'job_title': 'job_title',\n",
    "            'company': 'company',\n",
    "            'job_location': 'location',\n",
    "            'description': 'summary',\n",
    "        },\n",
    "    },\n",
    "    'glassdoor': {\n",
    "        'columns': {\n",
    "            'job_title': 'position',\n",
    "            'company': 'company',\n",
    "            'job_location': 'location',\n",
    "            'description': 'Job Description',\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "def map_row_to_message(row, source_name):\n",
    "    # map a CSV row to unified message format\n",
    "    mapping = SOURCE_MAPPINGS[source_name]['columns']\n",
    "    \n",
    "    msg = {\n",
    "        'id': str(uuid.uuid4()),\n",
    "        'source': source_name,\n",
    "        'job_title': str(row.get(mapping.get('job_title', ''), '') or ''),\n",
    "        'company': str(row.get(mapping.get('company', ''), '') or ''),\n",
    "        'job_location': str(row.get(mapping.get('job_location', ''), '') or ''),\n",
    "    }\n",
    "    \n",
    "    # linkedin specific fields\n",
    "    if source_name == 'linkedin':\n",
    "        msg['job_link'] = str(row.get('job_link', '') or '')\n",
    "        msg['job_level'] = str(row.get('job_level', '') or '')\n",
    "        msg['job_type'] = str(row.get('job_type', '') or '')\n",
    "    \n",
    "    # description for indeed and glassdoor\n",
    "    if 'description' in mapping and mapping['description']:\n",
    "        msg['description'] = str(row.get(mapping['description'], '') or '')\n",
    "    \n",
    "    return msg\n",
    "\n",
    "print('source mappings defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Producer class defined\n"
     ]
    }
   ],
   "source": [
    "# kafka producer class\n",
    "class JobPostingProducer:\n",
    "    #producer to send job postings to Kafka\n",
    "    \n",
    "    def __init__(self, broker):\n",
    "        self.producer = Producer({\n",
    "            'bootstrap.servers': broker,\n",
    "            'client.id': 'batch-producer',\n",
    "            'queue.buffering.max.messages': 500000,\n",
    "            'queue.buffering.max.kbytes': 1048576,\n",
    "            'batch.num.messages': 10000,\n",
    "            'linger.ms': 100,\n",
    "        })\n",
    "        self.produced = 0\n",
    "        self.errors = 0\n",
    "    \n",
    "    def delivery_callback(self, err, msg):\n",
    "        # callback for message delivery\n",
    "        if err:\n",
    "            self.errors += 1\n",
    "    \n",
    "    def produce_source(self, topic, source_name, filepath, max_rows=None):\n",
    "        # produce messages from a CSV file\n",
    "        print(f'\\nProducing {source_name} from {os.path.basename(filepath)}')\n",
    "        \n",
    "        if not os.path.exists(filepath):\n",
    "            print(f'  File not found, skipping')\n",
    "            return 0\n",
    "        \n",
    "        # load data\n",
    "        if max_rows:\n",
    "            df = pd.read_csv(filepath, nrows=max_rows, low_memory=False)\n",
    "        else:\n",
    "            df = pd.read_csv(filepath, low_memory=False)\n",
    "        \n",
    "        print(f'  Loaded {len(df):,} rows')\n",
    "        \n",
    "        count = 0\n",
    "        start = time.time()\n",
    "        \n",
    "        # send each row to kafka\n",
    "        for _, row in df.iterrows():\n",
    "            msg = map_row_to_message(row.to_dict(), source_name)\n",
    "            \n",
    "            self.producer.produce(\n",
    "                topic,\n",
    "                value=json.dumps(msg).encode('utf-8'),\n",
    "                callback=self.delivery_callback\n",
    "            )\n",
    "            \n",
    "            count += 1\n",
    "            self.produced += 1\n",
    "            \n",
    "            # show progress\n",
    "            if count % 50000 == 0:\n",
    "                self.producer.poll(0)\n",
    "                elapsed = time.time() - start\n",
    "                rate = count / elapsed\n",
    "                print(f'  {count:,} sent ({rate:,.0f} msg/sec)')\n",
    "        \n",
    "        # flush remaining messages\n",
    "        self.producer.flush()\n",
    "        elapsed = time.time() - start\n",
    "        rate = count / elapsed if elapsed > 0 else 0\n",
    "        print(f' Done: {count:,} messages in {elapsed:.1f}s ({rate:,.0f} msg/sec)')\n",
    "        \n",
    "        return count\n",
    "\n",
    "print('Producer class defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRODUCING JOB POSTINGS TO KAFKA\n",
      "\n",
      "Producing linkedin from linkedin_job_postings.csv\n",
      "  Loaded 1,348,454 rows\n",
      "  50,000 sent (14,124 msg/sec)\n",
      "  100,000 sent (15,965 msg/sec)\n",
      "  150,000 sent (16,732 msg/sec)\n",
      "  200,000 sent (16,854 msg/sec)\n",
      "  250,000 sent (17,257 msg/sec)\n",
      "  300,000 sent (17,566 msg/sec)\n",
      "  350,000 sent (17,564 msg/sec)\n",
      "  400,000 sent (17,707 msg/sec)\n",
      "  450,000 sent (17,827 msg/sec)\n",
      "  500,000 sent (17,955 msg/sec)\n",
      "  550,000 sent (18,070 msg/sec)\n",
      "  600,000 sent (18,174 msg/sec)\n",
      "  650,000 sent (18,295 msg/sec)\n",
      "  700,000 sent (18,344 msg/sec)\n",
      "  750,000 sent (18,440 msg/sec)\n",
      "  800,000 sent (18,510 msg/sec)\n",
      "  850,000 sent (18,546 msg/sec)\n",
      "  900,000 sent (18,593 msg/sec)\n",
      "  950,000 sent (18,639 msg/sec)\n",
      "  1,000,000 sent (18,701 msg/sec)\n",
      "  1,050,000 sent (18,749 msg/sec)\n",
      "  1,100,000 sent (18,786 msg/sec)\n",
      "  1,150,000 sent (18,811 msg/sec)\n",
      "  1,200,000 sent (18,830 msg/sec)\n",
      "  1,250,000 sent (18,837 msg/sec)\n",
      "  1,300,000 sent (18,865 msg/sec)\n",
      " Done: 1,348,454 messages in 71.5s (18,857 msg/sec)\n",
      "\n",
      "Producing indeed from indeed_job_listings.csv\n",
      "  Loaded 100 rows\n",
      " Done: 100 messages in 0.0s (27,768 msg/sec)\n",
      "\n",
      "Producing glassdoor from job_descriptions.csv\n",
      "  Loaded 157 rows\n",
      " Done: 157 messages in 0.0s (27,154 msg/sec)\n",
      "\n",
      "PRODUCER SUMMARY\n",
      "Total messages: 1,348,711\n",
      "Total time: 75.6s\n",
      "Errors: 0\n"
     ]
    }
   ],
   "source": [
    "# produce all data to kafka\n",
    "print('PRODUCING JOB POSTINGS TO KAFKA')\n",
    "\n",
    "producer = JobPostingProducer(KAFKA_BROKER)\n",
    "\n",
    "# sources to process - NO LIMITS for production\n",
    "sources = [\n",
    "    ('linkedin', 'linkedin_job_postings.csv', None),\n",
    "    ('indeed', 'indeed_job_listings.csv', None),\n",
    "    ('glassdoor', 'job_descriptions.csv', None),\n",
    "]\n",
    "\n",
    "total_start = time.time()\n",
    "total_count = 0\n",
    "\n",
    "for source_name, filename, max_rows in sources:\n",
    "    filepath = os.path.join(RAW_DATA_DIR, filename)\n",
    "    count = producer.produce_source(TOPIC, source_name, filepath, max_rows)\n",
    "    total_count += count\n",
    "\n",
    "total_elapsed = time.time() - total_start\n",
    "\n",
    "print()\n",
    "print('PRODUCER SUMMARY')\n",
    "print(f'Total messages: {total_count:,}')\n",
    "print(f'Total time: {total_elapsed:.1f}s')\n",
    "print(f'Errors: {producer.errors}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DLQ producer defined\n"
     ]
    }
   ],
   "source": [
    "# DLQ producer for failed messages\n",
    "class DLQProducer:\n",
    "    #producer for dead letter queue\n",
    "    \n",
    "    def __init__(self, broker, dlq_topic='dlq_job_postings'):\n",
    "        self.dlq_topic = dlq_topic\n",
    "        self.producer = Producer({\n",
    "            'bootstrap.servers': broker, \n",
    "            'client.id': 'dlq-producer'\n",
    "        })\n",
    "        self.failed_count = 0\n",
    "    \n",
    "    def send_to_dlq(self, original_message, error_type, error_message):\n",
    "        # send failed message to DLQ\n",
    "        dlq_record = {\n",
    "            'original_message': original_message,\n",
    "            'error_type': error_type,\n",
    "            'error_message': str(error_message),\n",
    "            'failed_at': datetime.now().isoformat(),\n",
    "        }\n",
    "        try:\n",
    "            self.producer.produce(\n",
    "                self.dlq_topic, \n",
    "                value=json.dumps(dlq_record).encode('utf-8')\n",
    "            )\n",
    "            self.producer.poll(0)\n",
    "            self.failed_count += 1\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def flush(self):\n",
    "        self.producer.flush()\n",
    "\n",
    "print('DLQ producer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch writer defined\n"
     ]
    }
   ],
   "source": [
    "# define message fields to save\n",
    "MESSAGE_FIELDS = [\n",
    "    'id', 'source', 'job_link', 'job_title', 'company', \n",
    "    'job_location', 'job_level', 'job_type', 'description'\n",
    "]\n",
    "\n",
    "def write_batch(batch, output_dir, file_num, consumer):\n",
    "    # write a batch of messages to parquet\n",
    "    df = pd.DataFrame(batch)\n",
    "    \n",
    "    # add missing columns\n",
    "    for col in MESSAGE_FIELDS:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''\n",
    "    \n",
    "    df = df[MESSAGE_FIELDS]\n",
    "    \n",
    "    # write to parquet\n",
    "    filename = f'part-{file_num:04d}.parquet'\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    df.to_parquet(filepath, index=False)\n",
    "    \n",
    "    # commit offset\n",
    "    consumer.commit()\n",
    "    return filename\n",
    "\n",
    "print('Batch writer defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consumer function defined\n"
     ]
    }
   ],
   "source": [
    "# kafka consumer function\n",
    "def consume_all(topic, broker, output_dir, batch_size=10000):\n",
    "    # consume all messages from Kafka and write to parquet\n",
    "    consumer = Consumer({\n",
    "        'bootstrap.servers': broker,\n",
    "        'group.id': 'full-batch-consumer-v2',\n",
    "        'auto.offset.reset': 'earliest',\n",
    "        'enable.auto.commit': False,\n",
    "        'fetch.message.max.bytes': 52428800,\n",
    "        'max.poll.interval.ms': 600000,\n",
    "    })\n",
    "    consumer.subscribe([topic])\n",
    "    \n",
    "    dlq = DLQProducer(broker)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    processed_ids = set()\n",
    "    messages_consumed = 0\n",
    "    duplicates = 0\n",
    "    files_written = 0\n",
    "    batch = []\n",
    "    empty_polls = 0\n",
    "    max_empty_polls = 15\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    print('CONSUMING MESSAGES FROM KAFKA')\n",
    "    print(f'Topic: {topic}')\n",
    "    print(f'Output: {output_dir}')\n",
    "    print(f'Batch size: {batch_size:,}')\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        while empty_polls < max_empty_polls:\n",
    "            msg = consumer.poll(timeout=2.0)\n",
    "            \n",
    "            if msg is None:\n",
    "                empty_polls += 1\n",
    "                continue\n",
    "            \n",
    "            empty_polls = 0\n",
    "            \n",
    "            if msg.error():\n",
    "                if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "                    continue\n",
    "                else:\n",
    "                    raise KafkaException(msg.error())\n",
    "            \n",
    "            raw_value = None\n",
    "            try:\n",
    "                raw_value = msg.value().decode('utf-8')\n",
    "                data = json.loads(raw_value)\n",
    "                \n",
    "                # check for duplicates\n",
    "                msg_id = data.get('id', '')\n",
    "                if msg_id in processed_ids:\n",
    "                    duplicates += 1\n",
    "                    continue\n",
    "                \n",
    "                record = {field: data.get(field, '') for field in MESSAGE_FIELDS}\n",
    "                \n",
    "                if not record.get('id'):\n",
    "                    raise ValueError('Missing id')\n",
    "                \n",
    "                batch.append(record)\n",
    "                processed_ids.add(msg_id)\n",
    "                messages_consumed += 1\n",
    "                \n",
    "                # show progress\n",
    "                if messages_consumed % 100000 == 0:\n",
    "                    elapsed = time.time() - start_time\n",
    "                    rate = messages_consumed / elapsed\n",
    "                    print(f'  {messages_consumed:,} consumed ({rate:,.0f} msg/sec)')\n",
    "                \n",
    "                # write batch if full\n",
    "                if len(batch) >= batch_size:\n",
    "                    fname = write_batch(batch, output_dir, files_written, consumer)\n",
    "                    files_written += 1\n",
    "                    batch = []\n",
    "            \n",
    "            except json.JSONDecodeError as e:\n",
    "                dlq.send_to_dlq(raw_value, 'JSONDecodeError', str(e))\n",
    "            except Exception as e:\n",
    "                dlq.send_to_dlq(raw_value, type(e).__name__, str(e))\n",
    "        \n",
    "        # write remaining batch\n",
    "        if batch:\n",
    "            fname = write_batch(batch, output_dir, files_written, consumer)\n",
    "            files_written += 1\n",
    "    \n",
    "    finally:\n",
    "        dlq.flush()\n",
    "        consumer.close()\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print()\n",
    "    print('CONSUMER SUMMARY')\n",
    "    print(f'Messages consumed: {messages_consumed:,}')\n",
    "    print(f'Duplicates skipped: {duplicates:,}')\n",
    "    print(f'Failed (DLQ): {dlq.failed_count:,}')\n",
    "    print(f'Files written: {files_written}')\n",
    "    print(f'Time: {elapsed:.1f}s')\n",
    "    if elapsed > 0:\n",
    "        print(f'Rate: {messages_consumed/elapsed:,.0f} msg/sec')\n",
    "    \n",
    "    return messages_consumed, files_written\n",
    "\n",
    "print('Consumer function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONSUMING MESSAGES FROM KAFKA\n",
      "Topic: raw_job_postings\n",
      "Output: /home/developer/project/output/raw_job_postings\n",
      "Batch size: 10,000\n",
      "\n",
      "  100,000 consumed (136,640 msg/sec)\n",
      "  200,000 consumed (128,294 msg/sec)\n",
      "  300,000 consumed (125,229 msg/sec)\n",
      "  400,000 consumed (138,774 msg/sec)\n",
      "  500,000 consumed (132,994 msg/sec)\n",
      "  600,000 consumed (131,824 msg/sec)\n",
      "  700,000 consumed (138,462 msg/sec)\n",
      "  800,000 consumed (136,086 msg/sec)\n",
      "  900,000 consumed (134,549 msg/sec)\n",
      "  1,000,000 consumed (132,277 msg/sec)\n",
      "  1,100,000 consumed (136,834 msg/sec)\n",
      "  1,200,000 consumed (134,733 msg/sec)\n",
      "  1,300,000 consumed (134,145 msg/sec)\n",
      "\n",
      "CONSUMER SUMMARY\n",
      "Messages consumed: 1,348,711\n",
      "Duplicates skipped: 0\n",
      "Failed (DLQ): 0\n",
      "Files written: 135\n",
      "Time: 40.0s\n",
      "Rate: 33,739 msg/sec\n"
     ]
    }
   ],
   "source": [
    "# clear old output\n",
    "import shutil\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    shutil.rmtree(OUTPUT_DIR)\n",
    "    print(f'Cleared old output: {OUTPUT_DIR}')\n",
    "\n",
    "# run consumer\n",
    "consumed, files = consume_all(TOPIC, KAFKA_BROKER, OUTPUT_DIR, batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verifying output\n",
      "Files: 135\n",
      "Total records: 1,348,711\n",
      "Total size: 155.6 MB\n",
      "\n",
      "Records by source:\n",
      "source\n",
      "linkedin     1348454\n",
      "glassdoor        157\n",
      "indeed           100\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# verify output files\n",
    "print('Verifying output')\n",
    "\n",
    "parquet_files = sorted([f for f in os.listdir(OUTPUT_DIR) if f.endswith('.parquet')])\n",
    "total_records = 0\n",
    "total_size = 0\n",
    "\n",
    "for f in parquet_files:\n",
    "    path = os.path.join(OUTPUT_DIR, f)\n",
    "    size_mb = os.path.getsize(path) / 1e6\n",
    "    total_size += size_mb\n",
    "    df = pd.read_parquet(path)\n",
    "    total_records += len(df)\n",
    "\n",
    "print(f'Files: {len(parquet_files)}')\n",
    "print(f'Total records: {total_records:,}')\n",
    "print(f'Total size: {total_size:.1f} MB')\n",
    "\n",
    "# count by source\n",
    "print('\\nRecords by source:')\n",
    "all_dfs = [pd.read_parquet(os.path.join(OUTPUT_DIR, f)) for f in parquet_files]\n",
    "combined = pd.concat(all_dfs, ignore_index=True)\n",
    "print(combined['source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample records:\n",
      "\n",
      "LINKEDIN:\n",
      "  Title: Account Executive - Dispensing (NorCal/Northern Nevada) - Be\n",
      "  Company: BD\n",
      "  Location: San Diego, CA\n",
      "  Level: Mid senior\n",
      "  Link: https://www.linkedin.com/jobs/view/account-executive-dispens...\n",
      "\n",
      "INDEED:\n",
      "  Title: IN-16092 Conservation Data Analysis Intern\n",
      "  Company: World Wildlife Fund\n",
      "  Location: Washington, DC US\n",
      "\n",
      "GLASSDOOR:\n",
      "  Title: Graduate Intern (Summer 2017) - SAP BI / Big Data / Analytic\n",
      "  Company:  Visual BI Solutions Inc\n",
      "  Location:  Plano, TX\n"
     ]
    }
   ],
   "source": [
    "# show sample records\n",
    "print('\\nSample records:')\n",
    "\n",
    "for source in ['linkedin', 'indeed', 'glassdoor']:\n",
    "    sample = combined[combined['source'] == source].head(1)\n",
    "    if len(sample) > 0:\n",
    "        print(f'\\n{source.upper()}:')\n",
    "        row = sample.iloc[0]\n",
    "        print(f'  Title: {row[\"job_title\"][:60]}')\n",
    "        print(f'  Company: {row[\"company\"][:40]}')\n",
    "        print(f'  Location: {row[\"job_location\"][:40]}')\n",
    "        if source == 'linkedin':\n",
    "            print(f'  Level: {row[\"job_level\"]}')\n",
    "            print(f'  Link: {row[\"job_link\"][:60]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAFKA BATCH LOAD COMPLETE\n",
      "Output: /home/developer/project/output/raw_job_postings\n",
      "Records: 1,348,711\n",
      "Files: 135\n"
     ]
    }
   ],
   "source": [
    "print('KAFKA BATCH LOAD COMPLETE')\n",
    "print(f'Output: {OUTPUT_DIR}')\n",
    "print(f'Records: {total_records:,}')\n",
    "print(f'Files: {len(parquet_files)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

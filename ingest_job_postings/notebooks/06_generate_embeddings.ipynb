{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports loaded\n",
      "pytorch: 2.6.0+cu124\n",
      "cuda available: True\n",
      "project root: /home/developer/project\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# setup paths, detect project root\n",
    "cwd = os.getcwd()\n",
    "if 'notebooks' in cwd:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print('imports loaded')\n",
    "print(f'pytorch: {torch.__version__}')\n",
    "print(f'cuda available: {torch.cuda.is_available()}')\n",
    "print(f'project root: {PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick_mode_toggle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUICK_MODE toggle for demo/testing\n",
    "# when True: samples 5000 jobs and saves to output/temp/ for fast demo\n",
    "# when False: processes all jobs (~165K) and saves to training/output/embeddings/\n",
    "\n",
    "QUICK_MODE = True  # set to False for full production run\n",
    "\n",
    "if QUICK_MODE:\n",
    "    SAMPLE_SIZE = 5000\n",
    "    print('QUICK_MODE enabled: sampling 5000 jobs, saving to output/temp/')\n",
    "else:\n",
    "    SAMPLE_SIZE = None  # use all jobs\n",
    "    print('FULL MODE: processing all jobs, saving to training/output/embeddings/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/27 01:31:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark started: 4.1.1\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('GenerateEmbeddings') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f'spark started: {spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 3090\n",
      "total VRAM: 25.3 GB\n",
      "using GPU for encoding\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f'GPU: {gpu_name}')\n",
    "    print(f'total VRAM: {gpu_memory:.1f} GB')\n",
    "    print(f'using GPU for encoding')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('!!! GPU not available, using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading jobs with Spark\n",
      "loaded 165,193 jobs\n",
      "columns: ['job_id', 'embedding_text', 'isco_code']\n",
      "has isco_code column: True\n",
      "\n",
      "sample:\n",
      "+------+--------------------------------------------------------------------------------+---------+\n",
      "|job_id|                                                                  embedding_text|isco_code|\n",
      "+------+--------------------------------------------------------------------------------+---------+\n",
      "|    B7|passage: Role of $18.00 Assistant Manager at McDonald's in Whiteville, NC. Re...|        1|\n",
      "|   B16|passage: Role of $4,500 Sign on Bonus - MDS Coordinator at The Goodman Group,...|        1|\n",
      "|   B18|passage: Role of $45/hr - School RN at Maxim Healthcare Staffing in Roslyn, N...|        2|\n",
      "+------+--------------------------------------------------------------------------------+---------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": "print('loading jobs with Spark')\ninput_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'final', 'jobs_to_embed.parquet')\njobs_df = spark.read.parquet(input_path)\n\n# sample if QUICK_MODE\nif QUICK_MODE and SAMPLE_SIZE:\n    jobs_df = jobs_df.limit(SAMPLE_SIZE)\n    print(f'QUICK_MODE: sampled {SAMPLE_SIZE} jobs')\nprint(f'loaded {jobs_df.count():,} jobs')\nprint(f'columns: {jobs_df.columns}')\n\n# check for isco_code column, needed for stratified splitting in 07\nhas_isco = 'isco_code' in jobs_df.columns\nprint(f'has isco_code column: {has_isco}')\nif not has_isco:\n    print('!!! isco_code not found, stratified splitting will use random splits')\n\nprint('\\nsample:')\njobs_df.show(3, truncate=80)"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobs with passage prefix: 165,193/165,193\n",
      "all jobs have correct prefix\n"
     ]
    }
   ],
   "source": [
    "# verify passage prefix\n",
    "has_prefix = jobs_df.filter(jobs_df['embedding_text'].startswith('passage: ')).count()\n",
    "total = jobs_df.count()\n",
    "\n",
    "print(f'jobs with passage prefix: {has_prefix:,}/{total:,}')\n",
    "if has_prefix == total:\n",
    "    print('all jobs have correct prefix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting to pandas for GPU encoding\n",
      "collected 165,193 jobs to driver\n"
     ]
    }
   ],
   "source": [
    "# collect to pandas for GPU encoding\n",
    "print('collecting to pandas for GPU encoding')\n",
    "jobs_pd = jobs_df.toPandas()\n",
    "print(f'collected {len(jobs_pd):,} jobs to driver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading e5-base-v2 model with fp16 precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded on: cuda\n",
      "precision: float16 (optimized)\n",
      "embedding dimension: 768\n",
      "max sequence length: 512 tokens\n",
      "GPU memory: 0.23 GB\n"
     ]
    }
   ],
   "source": [
    "# load model with fp16 for ~1.5x speedup, based on: https://sbert.net/docs/sentence_transformer/usage/efficiency.html\n",
    "print('loading e5-base-v2 model with fp16 precision')\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    'intfloat/e5-base-v2', \n",
    "    device=device,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}  # fp16 precision\n",
    ")\n",
    "\n",
    "print(f'model loaded on: {device}')\n",
    "print(f'precision: float16 (optimized)')\n",
    "print(f'embedding dimension: {model.get_sentence_embedding_dimension()}')\n",
    "print(f'max sequence length: {model.max_seq_length} tokens')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f'GPU memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding function defined\n"
     ]
    }
   ],
   "source": [
    "# sentenceTransformer's built-in encoding with progress bar\n",
    "# batch_size=256 optimal when GPU near full utilization, ref: https://github.com/UKPLab/sentence-transformers/issues/609\n",
    "\n",
    "def encode_texts(texts, model, batch_size=256):\n",
    "\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "print(f'encoding function defined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding 165,193 jobs with fp16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85626e2f556e4dcbb14ba48bf6841f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/646 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding complete!\n",
      "embeddings shape: (165193, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f'encoding {len(jobs_pd):,} jobs with fp16')\n",
    "\n",
    "job_embeddings = encode_texts(\n",
    "    texts=jobs_pd['embedding_text'].tolist(),\n",
    "    model=model,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "print(f'encoding complete!')\n",
    "print(f'embeddings shape: {job_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding dimension: 768\n",
      "expected: 768\n",
      "dimension check: PASSED\n",
      "\n",
      "L2 norm stats:\n",
      " mean: 1.000000\n",
      " min: 0.999512\n",
      " max: 1.000000\n",
      "normalization check: FAILED\n",
      "\n",
      "quality checks:\n",
      " contains NaN: False\n",
      " contains Inf: False\n",
      "quality check: PASSED\n"
     ]
    }
   ],
   "source": [
    "# check dimension\n",
    "expected_dim = 768\n",
    "actual_dim = job_embeddings.shape[1]\n",
    "\n",
    "print(f'embedding dimension: {actual_dim}')\n",
    "print(f'expected: {expected_dim}')\n",
    "print(f'dimension check: {\"PASSED\" if actual_dim == expected_dim else \"FAILED\"}')\n",
    "\n",
    "# check L2 normalization\n",
    "norms = np.linalg.norm(job_embeddings, axis=1)\n",
    "print(f'\\nL2 norm stats:')\n",
    "print(f' mean: {norms.mean():.6f}')\n",
    "print(f' min: {norms.min():.6f}')\n",
    "print(f' max: {norms.max():.6f}')\n",
    "\n",
    "is_normalized = np.allclose(norms, 1.0, atol=1e-6)\n",
    "print(f'normalization check: {\"PASSED\" if is_normalized else \"FAILED\"}')\n",
    "# check for NaN/inf\n",
    "has_nan = np.isnan(job_embeddings).any()\n",
    "has_inf = np.isinf(job_embeddings).any()\n",
    "print(f'\\nquality checks:')\n",
    "print(f' contains NaN: {has_nan}')\n",
    "print(f' contains Inf: {has_inf}')\n",
    "print(f'quality check: {\"PASSED\" if not (has_nan or has_inf) else \"FAILED\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# save job embeddings in chunks to avoid memory explosion, ref: https://github.com/apache/arrow/issues/20512 (pandas to_parquet quadratic memory)\n\nCHUNK_SIZE = 50000  # 50K records per chunk\nif QUICK_MODE:\n    output_path = os.path.join(PROJECT_ROOT, 'output', 'temp', 'jobs_embedded_sample.parquet')\nelse:\n    output_path = os.path.join(PROJECT_ROOT, 'training', 'output', 'embeddings', 'jobs_embedded.parquet')\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nprint(f'saving {len(jobs_pd):,} jobs in chunks of {CHUNK_SIZE:,}')\n\n# schema depends on whether we have isco_code for stratified splitting\nif has_isco:\n    schema = pa.schema([\n        ('job_id', pa.string()),\n        ('embedding_text', pa.string()),\n        ('embedding', pa.list_(pa.float32(), 768)),\n        ('isco_code', pa.int32())\n    ])\n    print('including isco_code for stratified splitting')\nelse:\n    schema = pa.schema([\n        ('job_id', pa.string()),\n        ('embedding_text', pa.string()),\n        ('embedding', pa.list_(pa.float32(), 768))\n    ])\n    print('no isco_code - using basic schema')\n\nwriter = pq.ParquetWriter(output_path, schema, compression='snappy')\ntotal_written = 0\n\nfor start_idx in range(0, len(jobs_pd), CHUNK_SIZE):\n    end_idx = min(start_idx + CHUNK_SIZE, len(jobs_pd))\n    \n    # build chunk without full .tolist() (memory efficient)\n    chunk_ids = jobs_pd['job_id'].iloc[start_idx:end_idx].values\n    chunk_texts = jobs_pd['embedding_text'].iloc[start_idx:end_idx].values\n    chunk_embs = [job_embeddings[i].tolist() for i in range(start_idx, end_idx)]\n    \n    # build table data\n    table_data = {\n        'job_id': chunk_ids,\n        'embedding_text': chunk_texts,\n        'embedding': chunk_embs\n    }\n    \n    # add isco_code if available\n    if has_isco:\n        chunk_isco = jobs_pd['isco_code'].iloc[start_idx:end_idx].values.astype('int32')\n        table_data['isco_code'] = chunk_isco\n    \n    # write chunk with pyarrow\n    table = pa.table(table_data, schema=schema)\n    writer.write_table(table)\n    total_written += (end_idx - start_idx)\n    \n    # cleanup chunk memory\n    del chunk_ids, chunk_texts, chunk_embs, table, table_data\n    if has_isco:\n        del chunk_isco\n    gc.collect()\n    \n    # progress every 5 chunks\n    chunk_num = start_idx // CHUNK_SIZE + 1\n    if chunk_num % 5 == 0:\n        print(f'  written: {total_written:,}/{len(jobs_pd):,}')\n\nwriter.close()\n\nprint(f'saved to: {output_path}')\nprint(f'records: {total_written:,}')\nfile_size = os.path.getsize(output_path) / 1e6\nprint(f'file size: {file_size:.1f} MB')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print('loading CVs with Spark')\ncv_input_path = os.path.join(PROJECT_ROOT, 'ingest_cv', 'output', 'cv_query_text.parquet')\ncvs_df = spark.read.parquet(cv_input_path)\n\nprint(f'loaded {cvs_df.count():,} CVs')\nprint('\\nsample:')\ncvs_df.show(3, truncate=80)"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting CVs to pandas\n",
      "collected 7,299 CVs\n"
     ]
    }
   ],
   "source": [
    "# collect to pandas\n",
    "print('collecting CVs to pandas')\n",
    "cvs_pd = cvs_df.toPandas()\n",
    "cvs_pd = cvs_pd.rename(columns={'id': 'cv_id', 'text': 'embedding_text'})\n",
    "print(f'collected {len(cvs_pd):,} CVs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding 7,299 CVs with fp16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb67c60798148b09710a06e5de58032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV encoding complete!\n",
      "embeddings shape: (7299, 768)\n"
     ]
    }
   ],
   "source": [
    "print(f'encoding {len(cvs_pd):,} CVs with fp16')\n",
    "\n",
    "cv_embeddings = encode_texts(\n",
    "    texts=cvs_pd['embedding_text'].tolist(),\n",
    "    model=model,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "print(f'CV encoding complete!')\n",
    "print(f'embeddings shape: {cv_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV embedding dimension: 768\n",
      "dimension check: PASSED\n",
      "\n",
      "L2 norm stats:\n",
      " mean: 1.000000\n",
      " min: 0.999512\n",
      " max: 1.000000\n",
      "normalization check: FAILED\n",
      "quality check: PASSED\n"
     ]
    }
   ],
   "source": [
    "cv_dim = cv_embeddings.shape[1]\n",
    "print(f'CV embedding dimension: {cv_dim}')\n",
    "print(f'dimension check: {\"PASSED\" if cv_dim == 768 else \"FAILED\"}')\n",
    "\n",
    "cv_norms = np.linalg.norm(cv_embeddings, axis=1)\n",
    "print(f'\\nL2 norm stats:')\n",
    "print(f' mean: {cv_norms.mean():.6f}')\n",
    "print(f' min: {cv_norms.min():.6f}')\n",
    "print(f' max: {cv_norms.max():.6f}')\n",
    "\n",
    "cv_normalized = np.allclose(cv_norms, 1.0, atol=1e-6)\n",
    "print(f'normalization check: {\"PASSED\" if cv_normalized else \"FAILED\"}')\n",
    "\n",
    "has_nan_cv = np.isnan(cv_embeddings).any()\n",
    "has_inf_cv = np.isinf(cv_embeddings).any()\n",
    "print(f'quality check: {\"PASSED\" if not (has_nan_cv or has_inf_cv) else \"FAILED\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# save CV embeddings with pyarrow (smaller dataset, single write ok)\ncv_output_path = os.path.join(PROJECT_ROOT, 'training', 'output', 'embeddings', 'cvs_embedded.parquet')\n\ncv_schema = pa.schema([\n    ('cv_id', pa.string()),\n    ('embedding_text', pa.string()),\n    ('embedding', pa.list_(pa.float32(), 768))\n])\n\n# build embedding list, small enough for single operation)\ncv_emb_list = [cv_embeddings[i].tolist() for i in range(len(cvs_pd))]\n\ncv_table = pa.table({\n    'cv_id': cvs_pd['cv_id'].values,\n    'embedding_text': cvs_pd['embedding_text'].values,\n    'embedding': cv_emb_list\n}, schema=cv_schema)\n\npq.write_table(cv_table, cv_output_path, compression='snappy')\nprint(f'saved to: {cv_output_path}')\nprint(f'records: {len(cvs_pd):,}')\ncv_file_size = os.path.getsize(cv_output_path) / 1e6\nprint(f'file size: {cv_file_size:.1f} MB')\n\n# cleanup\ndel cv_table, cv_emb_list\ngc.collect()"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved embeddings for demo\n",
      "jobs: 165,193\n",
      "CVs: 7,299\n",
      "job columns: ['job_id', 'embedding_text', 'embedding', 'isco_code']\n",
      "\n",
      "query CV:\n",
      "ID: A1541\n",
      "text: Query: I am a Web Developer with 3 years of experience, (mid-level). My skills include: Azure, English, Django, Kubernetes, Python, Go, fluent, JavaScript, C#, MongoDB, Agile, CI/CD, Redis, Git. I wor...\n",
      "TOP 10 MATCHING JOBS\n",
      "\n",
      "[1] similarity: 0.8359 (ISCO: 2)\n",
      "job ID: B42949761042\n",
      "text: passage: Role of Senior Software Engineer, Backend (Golang) at ClickJobs.io in Worcester, MA. Required skills: Java, Python, SQL, Node, Go, Scala, Open Source RDBMS, NoSQL databases, Container Orchest...\n",
      "\n",
      "[2] similarity: 0.8347 (ISCO: 2)\n",
      "job ID: B89471\n",
      "text: passage: Role of Senior Web Developer, Web Applications Developer, C#, ASP.NET at Beautyk Creative in Oxford, England, United Kingdom. Required skills: C#, ASP.NET MVC, SQL Server, Kentico CMS, JavaSc...\n",
      "\n",
      "[3] similarity: 0.8336 (ISCO: 2)\n",
      "job ID: B25769892288\n",
      "text: passage: Role of Senior Software Engineer, Backend (Golang) at ClickJobs.io in Taunton, MA. Required skills: Java, Python, SQL, Node.js, Go, Scala, RDBMS, NoSQL, Docker, Kubernetes. Experience level: ...\n",
      "\n",
      "[4] similarity: 0.8332 (ISCO: 2)\n",
      "job ID: B77309416084\n",
      "text: passage: Role of Application Performance Engineer III at Kelly Science, Engineering, Technology & Telecom in Tampa, FL. Required skills: Application Performance Engineer, Performance Architecture, Loa...\n",
      "\n",
      "[5] similarity: 0.8331 (ISCO: 2)\n",
      "job ID: B103079258805\n",
      "text: passage: Role of Senior Python Engineer at Jobs via eFinancialCareers in Manchester, England, United Kingdom. Required skills: Python, Data Engineering, Machine Learning, Document Storage Systems, Inf...\n",
      "\n",
      "[6] similarity: 0.8329 (ISCO: 2)\n",
      "job ID: B94489368720\n",
      "text: passage: Role of Senior Software Engineer, Full Stack (Golang) at ClickJobs.io in Madison, WI. Required skills: Golang, JavaScript, Java, HTML/CSS, TypeScript, SQL, Python, Open Source RDBMS, NoSQL, D...\n",
      "\n",
      "[7] similarity: 0.8317 (ISCO: 2)\n",
      "job ID: B94489360304\n",
      "text: passage: Role of SENIOR WEB DEVELOPER (.Net / Angular / Python) at UrBench, LLC in Jersey City, NJ. Required skills: .NET, Angular, Python, HTML5, JavaScript, XML/JSON/YAML, SOAP/Restful Web Services,...\n",
      "\n",
      "[8] similarity: 0.8310 (ISCO: 2)\n",
      "job ID: B25769892224\n",
      "text: passage: Role of Senior Software Engineer at Moov in Denver, CO. Required skills: Go, MySQL, SQLite, HTTP2, JSON, Kafka, Microservices, Webhooks, General ledger, Payment systems. Experience level: Sen...\n",
      "\n",
      "[9] similarity: 0.8310 (ISCO: 2)\n",
      "job ID: B94489326544\n",
      "text: passage: Role of Level III Full Stack Software Engineer at Experfy in Arlington, VA. Required skills: Software Development, Python, JavaScript, Java, React, Vue, Node.js, Spring, Spring Boot, Django. ...\n",
      "\n",
      "[10] similarity: 0.8309 (ISCO: 2)\n",
      "job ID: B25769892157\n",
      "text: passage: Role of Senior Software Engineer - Go at Frontend Masters in Minneapolis, MN. Required skills: Frontend engineering, Node.js, HTML, CSS, JavaScript, React, Computer Science, FullStack, Go pro...\n"
     ]
    }
   ],
   "source": [
    "# demo of cross-tower similarity search, load from saved parquet files to verify they work\n",
    "\n",
    "print('loading saved embeddings for demo')\n",
    "jobs_saved = pd.read_parquet(output_path)\n",
    "cvs_saved = pd.read_parquet(cv_output_path)\n",
    "print(f'jobs: {len(jobs_saved):,}')\n",
    "print(f'CVs: {len(cvs_saved):,}')\n",
    "print(f'job columns: {list(jobs_saved.columns)}')\n",
    "\n",
    "# pick random CV\n",
    "cv_idx = random.randint(0, len(cvs_saved) - 1)\n",
    "query_cv = cvs_saved.iloc[cv_idx]\n",
    "cv_query_emb = np.array(query_cv['embedding']).reshape(1, -1)\n",
    "print(f'\\nquery CV:')\n",
    "print(f'ID: {query_cv[\"cv_id\"]}')\n",
    "print(f'text: {query_cv[\"embedding_text\"][:200]}...')\n",
    "\n",
    "# compute similarities (dot product = cosine for normalized vectors)\n",
    "job_embs = np.array(jobs_saved['embedding'].tolist())\n",
    "similarities = cosine_similarity(cv_query_emb, job_embs)[0]\n",
    "\n",
    "# top 10 matches\n",
    "top_indices = similarities.argsort()[::-1][:10]\n",
    "print('TOP 10 MATCHING JOBS')\n",
    "\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    job = jobs_saved.iloc[idx]\n",
    "    sim = similarities[idx]\n",
    "    isco_str = f' (ISCO: {job[\"isco_code\"]})' if 'isco_code' in job else ''\n",
    "    print(f'\\n[{rank}] similarity: {sim:.4f}{isco_str}')\n",
    "    print(f'job ID: {job[\"job_id\"]}')\n",
    "    print(f'text: {job[\"embedding_text\"][:200]}...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory before cleanup: 0.24 GB\n",
      "GPU memory after cleanup: 0.01 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f'GPU memory before cleanup: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB')\n",
    "    del model\n",
    "    del job_embeddings\n",
    "    del cv_embeddings\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f'GPU memory after cleanup: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark stopped\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print('spark stopped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(f'\\njobs:')\nprint(f' input: {input_path}')\nprint(f' output: {output_path}')\nprint(f' records: {total_written:,}')\nprint(f' file size: {file_size:.1f} MB')\nprint(f' has isco_code: {has_isco}')\nprint(f'\\nCVs:')\nprint(f' input: {cv_input_path}')\nprint(f' output: {cv_output_path}')\nprint(f' records: {len(cvs_pd):,}')\nprint(f' file size: {cv_file_size:.1f} MB')\nprint(f'\\nmodel: e5-base-v2 (768D, L2 normalized)')\nprint(f'\\nNOTE: CV data from ingest_cv/output/ (colleague pipeline)')"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
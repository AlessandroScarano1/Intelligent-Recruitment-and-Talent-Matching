{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded\n",
      "Project root: /home/developer/project\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, when, trim\n",
    "from pyspark.sql.types import StringType\n",
    "import re\n",
    "\n",
    "# setup paths - detect project root\n",
    "cwd = os.getcwd()\n",
    "if 'notebooks' in cwd:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"Imports loaded\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/27 01:27:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started\n",
      "Version: 4.1.1\n"
     ]
    }
   ],
   "source": [
    "# start spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BuildEmbeddingStrings\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session started\")\n",
    "print(f\"Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn jobs: 1,348,711\n",
      "\n",
      "Columns:\n",
      "['id', 'source', 'job_link', 'job_title', 'company', 'job_location', 'job_level', 'job_type', 'description', 'skills', 'seniority', 'embedding_text']\n",
      "\n",
      "Sample:\n",
      "+------------------------------------+--------------------------------------------------+-----------------------------+--------------------------------------------------+\n",
      "|                                  id|                                         job_title|                      company|                                            skills|\n",
      "+------------------------------------+--------------------------------------------------+-----------------------------+--------------------------------------------------+\n",
      "|dfd5107a-29f7-422b-8fbb-cc38e3741060|          Customer Service Operations Team Manager|                  PerkinElmer|Leadership, Supervision, Team building, Communi...|\n",
      "|03a1ee14-f0f3-4d57-bf5f-12930ec014f3|Regional Planner IV - Senior Trails and Greenwa...|Commonwealth of Massachusetts|Data Analysis, Budgeting, Policy Guidance, Best...|\n",
      "|f821adc7-88f3-4a40-a7c4-60ca255c1f88|                   Traffic Control/Foreman Flagger| C-2 Utility Contractors, LLC|Traffic Control, Foreman Flaggers, Google Appli...|\n",
      "+------------------------------------+--------------------------------------------------+-----------------------------+--------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": "# load linkedin jobs, already have skills from JOIN in notebook 02\nlinkedin_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'processed', 'linkedin', 'linkedin_jobs_with_skills')\nlinkedin_df = spark.read.parquet(linkedin_path)\nlinkedin_count = linkedin_df.count()\nprint(f\"LinkedIn jobs: {linkedin_count:,}\")\nprint(\"\\nColumns:\")\nprint(linkedin_df.columns)\nprint(\"\\nSample:\")\nlinkedin_df.select(\"id\", \"job_title\", \"company\", \"skills\").show(3, truncate=50)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indeed/Glassdoor jobs: 257\n",
      "\n",
      "Columns:\n",
      "['id', 'source', 'job_link', 'job_title', 'company', 'job_location', 'job_level', 'job_type', 'description', 'skills', 'seniority', 'embedding_text', 'extracted_skills']\n",
      "\n",
      "Sample:\n",
      "+------------------------------------+--------------------------------------------+-------------------+--------------------------------------------------+\n",
      "|                                  id|                                   job_title|            company|                                            skills|\n",
      "+------------------------------------+--------------------------------------------+-------------------+--------------------------------------------------+\n",
      "|e9054ea7-f716-4e01-9ed7-c86a518482b6|  IN-16092 Conservation Data Analysis Intern|World Wildlife Fund|database, management, conservation, data manage...|\n",
      "|777dbd94-b3d0-4a0b-b805-fa309218d0ec|Data Science Analyst for The Weather Company|                IBM|management, acquisition, quality, data manageme...|\n",
      "|8981e7cf-5dfa-425d-9b79-692c5a516c4f|                              Data Scientist|Booz Allen Hamilton|                  science, data science, data sets|\n",
      "+------------------------------------+--------------------------------------------+-------------------+--------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": "# load indeed/glassdoor jobs, already have NLP extracted fields from notebook 03\nindeed_glassdoor_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'processed', 'indeed_glassdoor', 'indeed_glassdoor_extracted')\nindeed_glassdoor_df = spark.read.parquet(indeed_glassdoor_path)\nig_count = indeed_glassdoor_df.count()\nprint(f\"Indeed/Glassdoor jobs: {ig_count:,}\")\nprint(\"\\nColumns:\")\nprint(indeed_glassdoor_df.columns)\nprint(\"\\nSample:\")\nindeed_glassdoor_df.select(\"id\", \"job_title\", \"company\", \"skills\").show(3, truncate=50)"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seniority extraction function defined\n",
      "\n",
      "Test seniority extraction:\n",
      "  'Senior Software Engineer' -> senior\n",
      "  'Data Science Intern' -> intern\n",
      "  'Lead Machine Learning Engineer' -> lead\n",
      "  'Software Developer' -> mid\n",
      "  'Principal Data Scientist' -> principal\n"
     ]
    }
   ],
   "source": [
    "# extract seniority from job titles\n",
    "# simple regex patterns for common seniority indicators\n",
    "def extract_seniority_from_title(title):\n",
    "    # extract seniority level from job title, returns: intern, junior, mid, senior, lead, principal, or None\n",
    "    \n",
    "    if not title or not str(title).strip():\n",
    "        return None\n",
    "    \n",
    "    title_lower = str(title).lower()\n",
    "    \n",
    "    # patterns in priority order (most specific first)\n",
    "    if any(word in title_lower for word in ['intern', 'internship', 'trainee']):\n",
    "        return 'intern'\n",
    "    elif any(word in title_lower for word in ['principal', 'staff', 'distinguished']):\n",
    "        return 'principal'\n",
    "    elif any(word in title_lower for word in ['lead', 'head of', 'director', 'vp', 'chief']):\n",
    "        return 'lead'\n",
    "    elif any(word in title_lower for word in ['senior', 'sr.', 'sr ']):\n",
    "        return 'senior'\n",
    "    elif any(word in title_lower for word in ['junior', 'jr.', 'jr ', 'entry']):\n",
    "        return 'junior'\n",
    "    else:\n",
    "        # default to mid if no keywords found\n",
    "        return 'mid'\n",
    "\n",
    "# register as UDF\n",
    "extract_seniority_udf = udf(extract_seniority_from_title, StringType())\n",
    "\n",
    "print(\"Seniority extraction function defined\")\n",
    "\n",
    "# test on sample titles\n",
    "test_titles = [\n",
    "    \"Senior Software Engineer\",\n",
    "    \"Data Science Intern\",\n",
    "    \"Lead Machine Learning Engineer\",\n",
    "    \"Software Developer\",\n",
    "    \"Principal Data Scientist\"\n",
    "]\n",
    "print(\"\\nTest seniority extraction:\")\n",
    "for title in test_titles:\n",
    "    seniority = extract_seniority_from_title(title)\n",
    "    print(f\"  '{title}' -> {seniority}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn seniority distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+\n",
      "|seniority|  count|\n",
      "+---------+-------+\n",
      "|      mid|1102664|\n",
      "|   senior| 117480|\n",
      "|     lead|  84413|\n",
      "|principal|  27504|\n",
      "|   intern|  12148|\n",
      "|   junior|   4502|\n",
      "+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# add seniority column to linkedin jobs\n",
    "linkedin_df = linkedin_df.withColumn(\n",
    "    \"seniority\",\n",
    "    extract_seniority_udf(col(\"job_title\"))\n",
    ")\n",
    "\n",
    "# check seniority distribution\n",
    "print(\"LinkedIn seniority distribution:\")\n",
    "linkedin_df.groupBy(\"seniority\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding string builder defined\n",
      "\n",
      "Test embedding string:\n",
      "Role of Senior Data Scientist at Tech Corp in San Francisco, CA. Required skills: Python, Machine Learning, SQL, Spark. Experience level: Senior level, 5+ years experience. Salary range: $120,000 to $180,000. Work type: Hybrid work, partially remote.\n"
     ]
    }
   ],
   "source": [
    "# build embedding string function\n",
    "# creates natural language summary from job fields\n",
    "def build_embedding_string(title, company, location, skills, seniority, \n",
    "                          salary_min, salary_max, remote):\n",
    "    \n",
    "    # natural language embedding string from job fields, template:\n",
    "    # \"Role of {title} at {company} in {location}. Required skills: {skills}. \n",
    "    # Experience level: {seniority}. Salary: {salary}. Work type: {remote}.\"\n",
    "\n",
    "    # handle missing values\n",
    "    title = str(title).strip() if title and str(title).strip() else 'Unknown Position'\n",
    "    company = str(company).strip() if company and str(company).strip() else 'a company'\n",
    "    \n",
    "    # start building string\n",
    "    parts = [f\"Role of {title} at {company}\"]\n",
    "    \n",
    "    # add location if available\n",
    "    if location and str(location).strip():\n",
    "        parts[0] += f\" in {str(location).strip()}\"\n",
    "    parts[0] += \".\"\n",
    "    \n",
    "    # skills - limit to top 10 to keep string manageable\n",
    "    if skills and str(skills).strip():\n",
    "        skill_list = [s.strip() for s in str(skills).split(',')[:10]]\n",
    "        if skill_list:\n",
    "            parts.append(f\"Required skills: {', '.join(skill_list)}.\")\n",
    "    \n",
    "    # seniority with expanded descriptions\n",
    "    if seniority and str(seniority).strip():\n",
    "        seniority_map = {\n",
    "            'intern': 'Intern level, entry position',\n",
    "            'junior': 'Junior level, 0-2 years experience',\n",
    "            'mid': 'Mid-level, 3-5 years experience',\n",
    "            'senior': 'Senior level, 5+ years experience',\n",
    "            'lead': 'Lead level, 7+ years experience with leadership',\n",
    "            'principal': 'Principal level, expert with technical leadership'\n",
    "        }\n",
    "        level = seniority_map.get(str(seniority).lower().strip(), str(seniority))\n",
    "        parts.append(f\"Experience level: {level}.\")\n",
    "    \n",
    "    # salary range\n",
    "    if salary_min and float(salary_min) > 0:\n",
    "        if salary_max and float(salary_max) > 0 and float(salary_max) > float(salary_min):\n",
    "            parts.append(f\"Salary range: ${float(salary_min):,.0f} to ${float(salary_max):,.0f}.\")\n",
    "        else:\n",
    "            parts.append(f\"Minimum salary: ${float(salary_min):,.0f}.\")\n",
    "    \n",
    "    # remote status\n",
    "    if remote and str(remote).strip():\n",
    "        remote_map = {\n",
    "            'remote': 'Remote work available',\n",
    "            'hybrid': 'Hybrid work, partially remote',\n",
    "            'onsite': 'Onsite'\n",
    "        }\n",
    "        work_type = remote_map.get(str(remote).lower().strip(), str(remote))\n",
    "        parts.append(f\"Work type: {work_type}.\")\n",
    "    \n",
    "    return ' '.join(parts)\n",
    "\n",
    "# register as UDF\n",
    "build_embedding_udf = udf(build_embedding_string, StringType())\n",
    "\n",
    "print(\"Embedding string builder defined\")\n",
    "\n",
    "# test on sample data\n",
    "test_str = build_embedding_string(\n",
    "    title=\"Senior Data Scientist\",\n",
    "    company=\"Tech Corp\",\n",
    "    location=\"San Francisco, CA\",\n",
    "    skills=\"Python, Machine Learning, SQL, Spark\",\n",
    "    seniority=\"senior\",\n",
    "    salary_min=120000,\n",
    "    salary_max=180000,\n",
    "    remote=\"hybrid\"\n",
    ")\n",
    "print(\"\\nTest embedding string:\")\n",
    "print(test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn embedding strings generated\n",
      "\n",
      "Sample LinkedIn embedding strings:\n",
      "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                                job_title|                                                                                      embedding_text|\n",
      "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                 Customer Service Operations Team Manager|Role of Customer Service Operations Team Manager at PerkinElmer in Stevenage, England, United Kin...|\n",
      "|Regional Planner IV - Senior Trails and Greenways Planner|Role of Regional Planner IV - Senior Trails and Greenways Planner at Commonwealth of Massachusett...|\n",
      "|                          Traffic Control/Foreman Flagger|Role of Traffic Control/Foreman Flagger at C-2 Utility Contractors, LLC in Savannah, GA. Required...|\n",
      "+---------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# add embedding strings to linkedin jobs\n",
    "# linkedin doesn't have salary_min, salary_max, remote_status, use lit(None)\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "linkedin_df = linkedin_df.withColumn(\n",
    "    \"embedding_text\",\n",
    "    build_embedding_udf(\n",
    "        col(\"job_title\"),\n",
    "        col(\"company\"),\n",
    "        col(\"job_location\"),\n",
    "        col(\"skills\"),\n",
    "        col(\"seniority\"),\n",
    "        lit(None),  # salary_min not available\n",
    "        lit(None),  # salary_max not available\n",
    "        lit(None)   # remote_status not available\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"LinkedIn embedding strings generated\")\n",
    "print(\"\\nSample LinkedIn embedding strings:\")\n",
    "linkedin_df.select(\"job_title\", \"embedding_text\").show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indeed/Glassdoor embedding strings generated\n",
      "\n",
      "Sample Indeed/Glassdoor embedding strings:\n",
      "+--------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                   job_title|                                                                                      embedding_text|\n",
      "+--------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|  IN-16092 Conservation Data Analysis Intern|Role of IN-16092 Conservation Data Analysis Intern at World Wildlife Fund in Washington, DC US. R...|\n",
      "|Data Science Analyst for The Weather Company|Role of Data Science Analyst for The Weather Company at IBM in Boston, MA US. Required skills: ma...|\n",
      "|                              Data Scientist|Role of Data Scientist at Booz Allen Hamilton in Washington, DC US. Required skills: science, dat...|\n",
      "+--------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# add embedding strings to indeed/glassdoor jobs\n",
    "# these also don't have salary/remote columns, so use lit(None)\n",
    "indeed_glassdoor_df = indeed_glassdoor_df.withColumn(\n",
    "    \"embedding_text\",\n",
    "    build_embedding_udf(\n",
    "        col(\"job_title\"),\n",
    "        col(\"company\"),\n",
    "        col(\"job_location\"),\n",
    "        col(\"skills\"),\n",
    "        col(\"seniority\"),\n",
    "        lit(None),  # salary_min not available\n",
    "        lit(None),  # salary_max not available\n",
    "        lit(None)   # remote_status not available\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Indeed/Glassdoor embedding strings generated\")\n",
    "print(\"\\nSample Indeed/Glassdoor embedding strings:\")\n",
    "indeed_glassdoor_df.select(\"job_title\", \"embedding_text\").show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jobs before dedup: 1,348,968\n",
      "  LinkedIn: 1,348,711\n",
      "  Indeed/Glassdoor: 257\n"
     ]
    }
   ],
   "source": [
    "# select common columns for union,keep only columns that exist in both dataframes\n",
    "common_cols = [\n",
    "    \"id\",\n",
    "    \"job_title\",\n",
    "    \"company\",\n",
    "    \"job_location\",\n",
    "    \"skills\",\n",
    "    \"seniority\",\n",
    "    \"embedding_text\"\n",
    "]\n",
    "\n",
    "linkedin_subset = linkedin_df.select(*common_cols)\n",
    "ig_subset = indeed_glassdoor_df.select(*common_cols)\n",
    "\n",
    "# union all sources\n",
    "all_jobs_df = linkedin_subset.union(ig_subset)\n",
    "total_before = all_jobs_df.count()\n",
    "print(f\"Total jobs before dedup: {total_before:,}\")\n",
    "print(f\"  LinkedIn: {linkedin_count:,}\")\n",
    "print(f\"  Indeed/Glassdoor: {ig_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:========================>                                (6 + 8) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total jobs after dedup: 1,345,711\n",
      "Removed 3,257 duplicates (0.24%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# remove duplicate embedding strings\n",
    "# keep first occurrence to preserve UUIDs\n",
    "all_jobs_df = all_jobs_df.dropDuplicates([\"embedding_text\"])\n",
    "total_after = all_jobs_df.count()\n",
    "removed = total_before - total_after\n",
    "\n",
    "print(f\"Total jobs after dedup: {total_after:,}\")\n",
    "print(f\"Removed {removed:,} duplicates ({removed/total_before*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:====================>                                    (5 + 9) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing embedding strings: 0\n",
      "All records have embedding strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check for missing embedding strings\n",
    "missing_count = all_jobs_df.filter(col(\"embedding_text\").isNull() | (col(\"embedding_text\") == \"\")).count()\n",
    "print(f\"Missing embedding strings: {missing_count}\")\n",
    "\n",
    "if missing_count > 0:\n",
    "    print(\"\\n!!!!!!! Found records with missing embedding strings!\")\n",
    "    all_jobs_df.filter(col(\"embedding_text\").isNull() | (col(\"embedding_text\") == \"\")).show(5)\n",
    "else:\n",
    "    print(\"All records have embedding strings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding string length statistics:\n",
      "  Average: 330 chars\n",
      "  Min: 86 chars\n",
      "  Max: 2273 chars\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:================>                                       (4 + 10) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Strings under 50 chars: 0 (0.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# check embedding string lengths\n",
    "from pyspark.sql.functions import length, avg, min as spark_min, max as spark_max\n",
    "\n",
    "lengths_df = all_jobs_df.withColumn(\"text_length\", length(col(\"embedding_text\")))\n",
    "length_stats = lengths_df.agg(\n",
    "    avg(\"text_length\").alias(\"avg_len\"),\n",
    "    spark_min(\"text_length\").alias(\"min_len\"),\n",
    "    spark_max(\"text_length\").alias(\"max_len\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"Embedding string length statistics:\")\n",
    "print(f\"  Average: {length_stats['avg_len']:.0f} chars\")\n",
    "print(f\"  Min: {length_stats['min_len']} chars\")\n",
    "print(f\"  Max: {length_stats['max_len']} chars\")\n",
    "\n",
    "# check for very short strings (might indicate data quality issues)\n",
    "too_short = lengths_df.filter(col(\"text_length\") < 50).count()\n",
    "print(f\"\\nStrings under 50 chars: {too_short} ({too_short/total_after*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample embedding strings by seniority:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INTERN]\n",
      "Role of (Wirtschafts-) Mathematiker:in / Aktuar:in internationale Retrozessionen in Hannover, Deutschland at Energy Jobline in Hanover, MD. Experience level: Intern level, entry position.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[JUNIOR]\n",
      "Role of 2023-24 Nurse - Junior High School at Arlington ISD in Arlington, TX. Required skills: Nursing, Healthcare, Education, Health Screening, CPR Certification, Vision Screening, Hearing Screening, Acanthosis Nigricans Screening, Spinal Screening, CPR. Experience level: Junior level, 0-2 years experience.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[MID]\n",
      "Role of \"Attorney\" (Gov Appt/Non-Merit) Jobs at Commonwealth of Kentucky in Kentucky, United States. Required skills: Law, State and federal laws rules and regulations, Microsoft Office (Word Excel Outlook), Adobe, Information gathering/research, Communication, Research, Legal advice, Preparing recommended orders and opinions, Respond to Open Records Requests subpoenas and agency requests. Experience level: Mid-level, 3-5 years experience.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SENIOR]\n",
      "Role of $55/hr RN - Senior Life (Direct Hire) at ProPivotal Staffing in Boston, MA. Required skills: Nursing, Medical plan and treatment, Scheduling, Electronic medical records, Russian language, Massachusetts RN license, Proactiveness, Collaboration, Interdisciplinary team, Long term chronic care or rehab. Experience level: Senior level, 5+ years experience.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LEAD]\n",
      "Role of #045 Assistant Produce Team Leader at Price Chopper Supermarkets-Market 32 in Albany, NY. Required skills: Customer service, Merchandising, Inventory management, Budgeting, Scheduling, Ordering, Food safety and sanitation, Product quality control, Training and development, Equipment operation. Experience level: Lead level, 7+ years experience with leadership.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:====================================================>   (13 + 1) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[PRINCIPAL]\n",
      "Role of $2,462 Travel RN Emergency Room at Maxim Healthcare Staffing at Health eCareers in Poughkeepsie, NY. Required skills: Nursing process, Patient care, Clinical judgment, Triage, Assessment, Communication, Identification, Action, Execution, Delegation. Experience level: Principal level, expert with technical leadership.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# show sample embedding strings from different seniority levels\n",
    "print(\"Sample embedding strings by seniority:\")\n",
    "for level in ['intern', 'junior', 'mid', 'senior', 'lead', 'principal']:\n",
    "    sample = all_jobs_df.filter(col(\"seniority\") == level).limit(1).collect()\n",
    "    if sample:\n",
    "        print(f\"\\n[{level.upper()}]\")\n",
    "        print(sample[0]['embedding_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                       (0 + 12) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: /home/developer/project/output/unified_job_postings/unified_jobs.parquet\n",
      "Total records: 1,345,711\n",
      "Columns: ['id', 'job_title', 'company', 'job_location', 'skills', 'seniority', 'embedding_text']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": "# save unified jobs with embedding strings\noutput_path = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output', 'unified_job_postings', 'unified_jobs.parquet')\nos.makedirs(os.path.dirname(output_path), exist_ok=True)\nall_jobs_df.write.mode(\"overwrite\").parquet(output_path)\n\nprint(f\"Saved to: {output_path}\")\nprint(f\"Total records: {total_after:,}\")\nprint(f\"Columns: {all_jobs_df.columns}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn jobs: 1,348,711\n",
      "Indeed/Glassdoor jobs: 257\n",
      "Total before dedup: 1,348,968\n",
      "Duplicates removed: 3,257\n",
      "Final job count: 1,345,711\n",
      "\n",
      "Output: /home/developer/project/output/unified_job_postings/unified_jobs.parquet\n",
      "\n",
      "This is the UNIFIED output from all sources, next need to run notebook 05 to assign sequential B IDs\n"
     ]
    }
   ],
   "source": [
    "# final summary\n",
    "print(f\"LinkedIn jobs: {linkedin_count:,}\")\n",
    "print(f\"Indeed/Glassdoor jobs: {ig_count:,}\")\n",
    "print(f\"Total before dedup: {total_before:,}\")\n",
    "print(f\"Duplicates removed: {removed:,}\")\n",
    "print(f\"Final job count: {total_after:,}\")\n",
    "print(f\"\\nOutput: {output_path}\")\n",
    "print(f\"\\nThis is the UNIFIED output from all sources, next need to run notebook 05 to assign sequential B IDs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# cleanup\n",
    "spark.stop()\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
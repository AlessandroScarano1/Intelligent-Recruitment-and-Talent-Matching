{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:00.856968Z",
     "iopub.status.busy": "2026-01-26T15:57:00.856867Z",
     "iopub.status.idle": "2026-01-26T15:57:00.969388Z",
     "shell.execute_reply": "2026-01-26T15:57:00.969069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# pyspark imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, explode, split, trim, lit, udf, broadcast, \n",
    "    when, lower, regexp_extract\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "print('Imports loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:00.985738Z",
     "iopub.status.busy": "2026-01-26T15:57:00.985582Z",
     "iopub.status.idle": "2026-01-26T15:57:00.996991Z",
     "shell.execute_reply": "2026-01-26T15:57:00.996711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/developer/project\n",
      "Raw data: /home/developer/project/raw_data/job_postings\n",
      "Output: /home/developer/project/output\n",
      "\n",
      "Data size: 149M\n"
     ]
    }
   ],
   "source": "# setup paths\ncwd = os.getcwd()\n\n# detect if we're in notebooks/ or project root\nif 'notebooks' in cwd:\n    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\nelse:\n    PROJECT_ROOT = cwd\n\nRAW_DATA_DIR = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'raw_data')\nOUTPUT_DIR = os.path.join(PROJECT_ROOT, 'ingest_job_postings', 'output')\nRAW_JOBS_DIR = os.path.join(OUTPUT_DIR, 'raw_job_postings')\n\nprint(f'Project root: {PROJECT_ROOT}')\nprint(f'Raw data: {RAW_DATA_DIR}')\nprint(f'Output: {OUTPUT_DIR}')\n\n# check data size\nimport subprocess\ntry:\n    result = subprocess.run(\n        ['du', '-sh', RAW_JOBS_DIR], \n        capture_output=True, \n        text=True\n    )\n    data_size = result.stdout.split()[0]\n    print(f'\\nData size: {data_size}')\nexcept:\n    print('\\nCould not determine data size')"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:00.997870Z",
     "iopub.status.busy": "2026-01-26T15:57:00.997787Z",
     "iopub.status.idle": "2026-01-26T15:57:03.608887Z",
     "shell.execute_reply": "2026-01-26T15:57:03.608439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Spark session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/27 01:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.1.1\n",
      "Spark session created\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "print('Creating Spark session')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('JobPostingPipeline') \\\n",
    "    .config('spark.driver.memory', '8g') \\\n",
    "    .config('spark.sql.shuffle.partitions', '16') \\\n",
    "    .config('spark.sql.adaptive.enabled', 'true') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel('WARN')\n",
    "\n",
    "print(f'Spark version: {spark.version}')\n",
    "print('Spark session created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:03.610129Z",
     "iopub.status.busy": "2026-01-26T15:57:03.609939Z",
     "iopub.status.idle": "2026-01-26T15:57:05.648727Z",
     "shell.execute_reply": "2026-01-26T15:57:05.648273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /home/developer/project/output/raw_job_postings\n",
      "\n",
      "Loaded 1,348,711 job postings\n",
      "\n",
      "Records by source:\n",
      "+---------+-------+\n",
      "|   source|  count|\n",
      "+---------+-------+\n",
      "|   indeed|    100|\n",
      "|glassdoor|    157|\n",
      "| linkedin|1348454|\n",
      "+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load job postings from parquet\n",
    "print(f'Loading data from {RAW_JOBS_DIR}')\n",
    "\n",
    "jobs_df = spark.read.parquet(RAW_JOBS_DIR)\n",
    "total_count = jobs_df.count()\n",
    "\n",
    "print(f'\\nLoaded {total_count:,} job postings')\n",
    "print('\\nRecords by source:')\n",
    "jobs_df.groupBy('source').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:05.650002Z",
     "iopub.status.busy": "2026-01-26T15:57:05.649884Z",
     "iopub.status.idle": "2026-01-26T15:57:06.793311Z",
     "shell.execute_reply": "2026-01-26T15:57:06.792993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ground-truth skills from /home/developer/project/raw_data/job_postings/job_skills.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded 1,296,381 skill records\n",
      "\n",
      "Sample:\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|                                                                        job_link|                                                                      job_skills|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "|https://www.linkedin.com/jobs/view/housekeeper-i-pt-at-jacksonville-state-uni...|Building Custodial Services, Cleaning, Janitorial Services, Materials Handlin...|\n",
      "|https://www.linkedin.com/jobs/view/assistant-general-manager-huntington-4131-...|Customer service, Restaurant management, Food safety, Training, Supervision, ...|\n",
      "|https://www.linkedin.com/jobs/view/school-based-behavior-analyst-at-ccres-edu...|Applied Behavior Analysis (ABA), Data analysis, Behavioral assessment, Positi...|\n",
      "+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# load ground-truth skills\n",
    "skills_path = os.path.join(RAW_DATA_DIR, 'job_skills.csv')\n",
    "print(f'Loading ground-truth skills from {skills_path}')\n",
    "\n",
    "skills_df = spark.read.csv(skills_path, header=True, inferSchema=True)\n",
    "skills_count = skills_df.count()\n",
    "\n",
    "print(f'\\nLoaded {skills_count:,} skill records')\n",
    "print('\\nSample:')\n",
    "skills_df.show(3, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:06.794368Z",
     "iopub.status.busy": "2026-01-26T15:57:06.794288Z",
     "iopub.status.idle": "2026-01-26T15:57:07.061276Z",
     "shell.execute_reply": "2026-01-26T15:57:07.060902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn jobs: 1,348,454\n",
      "Other jobs (Indeed/Glassdoor): 257\n"
     ]
    }
   ],
   "source": [
    "# separate linkedin from other sources\n",
    "linkedin_jobs = jobs_df.filter(col('source') == 'linkedin')\n",
    "other_jobs = jobs_df.filter(col('source') != 'linkedin')\n",
    "\n",
    "print(f'LinkedIn jobs: {linkedin_jobs.count():,}')\n",
    "print(f'Other jobs (Indeed/Glassdoor): {other_jobs.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:07.062239Z",
     "iopub.status.busy": "2026-01-26T15:57:07.062138Z",
     "iopub.status.idle": "2026-01-26T15:57:10.395238Z",
     "shell.execute_reply": "2026-01-26T15:57:10.394849Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joining LinkedIn jobs with skills\n",
      "Join completed in 0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinkedIn jobs with skills: 1,294,374 / 1,348,454 (96.0%)\n"
     ]
    }
   ],
   "source": [
    "# join linkedin with skills\n",
    "print('Joining LinkedIn jobs with skills')\n",
    "start_time = time.time()\n",
    "\n",
    "linkedin_with_skills = linkedin_jobs.join(\n",
    "    broadcast(skills_df),\n",
    "    linkedin_jobs['job_link'] == skills_df['job_link'],\n",
    "    'left'\n",
    ").drop(skills_df['job_link']).withColumnRenamed('job_skills', 'skills')\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f'Join completed in {elapsed:.1f}s')\n",
    "\n",
    "with_skills = linkedin_with_skills.filter(col('skills').isNotNull()).count()\n",
    "linkedin_count = linkedin_with_skills.count()\n",
    "pct = 100 * with_skills / linkedin_count if linkedin_count > 0 else 0\n",
    "print(f'LinkedIn jobs with skills: {with_skills:,} / {linkedin_count:,} ({pct:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:10.396108Z",
     "iopub.status.busy": "2026-01-26T15:57:10.396015Z",
     "iopub.status.idle": "2026-01-26T15:57:11.897232Z",
     "shell.execute_reply": "2026-01-26T15:57:11.896825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total jobs: 1,348,711\n"
     ]
    }
   ],
   "source": [
    "# add empty skills to other jobs\n",
    "other_jobs_with_skills = other_jobs.withColumn('skills', lit(None).cast('string'))\n",
    "\n",
    "# combine all\n",
    "all_jobs = linkedin_with_skills.union(other_jobs_with_skills)\n",
    "print(f'\\nTotal jobs: {all_jobs.count():,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:11.898484Z",
     "iopub.status.busy": "2026-01-26T15:57:11.898379Z",
     "iopub.status.idle": "2026-01-26T15:57:14.218964Z",
     "shell.execute_reply": "2026-01-26T15:57:14.218592Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting seniority from job titles\n",
      "\n",
      "Seniority distribution (extracted from job titles):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:====>                                                   (2 + 13) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+\n",
      "|     seniority|  count|\n",
      "+--------------+-------+\n",
      "|           mid|1081113|\n",
      "|        senior| 150007|\n",
      "|lead/principal|  84749|\n",
      "|        junior|  31258|\n",
      "|        intern|   1584|\n",
      "+--------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# extract seniority from job titles using regex\n",
    "# this gives us 5 levels instead of just 2 from job_level column\n",
    "# order matters: check intern/junior/senior BEFORE manager (Sr. Manager = senior)\n",
    "\n",
    "print('Extracting seniority from job titles')\n",
    "\n",
    "all_jobs = all_jobs.withColumn(\n",
    "    'seniority',\n",
    "    when(col('job_title').rlike(r'(?i)\\b(intern|internship)\\b'), 'intern')\n",
    "    .when(col('job_title').rlike(r'(?i)\\b(jr\\.?|junior|entry|graduate|trainee)\\b'), 'junior')\n",
    "    .when(col('job_title').rlike(r'(?i)\\b(sr\\.?|senior|ii|iii|iv)\\b'), 'senior')\n",
    "    .when(col('job_title').rlike(r'(?i)\\b(principal|staff|head of|director|vp|chief|lead)\\b'), 'lead/principal')\n",
    "    .when(col('job_title').rlike(r'(?i)\\bassociate\\b'), 'junior')\n",
    "    .otherwise('mid')\n",
    ")\n",
    "\n",
    "# register as temp view for SQL queries\n",
    "all_jobs.createOrReplaceTempView('jobs')\n",
    "\n",
    "print('\\nSeniority distribution (extracted from job titles):')\n",
    "all_jobs.groupBy('seniority').count().orderBy(col('count').desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:14.219869Z",
     "iopub.status.busy": "2026-01-26T15:57:14.219731Z",
     "iopub.status.idle": "2026-01-26T15:57:17.527800Z",
     "shell.execute_reply": "2026-01-26T15:57:17.527392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample LinkedIn jobs WITH skills:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+-----------------------+----------------------------------------------------------------------+\n",
      "|                                job_title|                company|                                                                skills|\n",
      "+-----------------------------------------+-----------------------+----------------------------------------------------------------------+\n",
      "|            Community Nurse (RMN -Band 6)|                   Hays|Community Mental Health Nurse, Trusted Assessment Model, Clinical d...|\n",
      "|Software Lead Engineer in Cudahy, WI, USA|         Energy Jobline|EV/HEV, Micro Hybrid systems, MATLAB, Simulink, Auto code generatio...|\n",
      "|                 Nursing Clinical Adjunct|State of South Carolina|Nursing, Clinical Management, Student Evaluations, Student Attendan...|\n",
      "|                  Virtualization Engineer|                   Epic|VMware Engineer, VMware vSphere, vSphere PowerCLI, GoLang, Python, ...|\n",
      "|                  LEAD SALES ASSOCIATE-FT|         Dollar General|Customer service, Cashier operations, Inventory management, Merchan...|\n",
      "+-----------------------------------------+-----------------------+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# sample with skills\n",
    "print('\\nSample LinkedIn jobs WITH skills:')\n",
    "all_jobs.filter(\n",
    "    (col('source') == 'linkedin') & (col('skills').isNotNull())\n",
    ").select(\n",
    "    'job_title', 'company', 'skills'\n",
    ").show(5, truncate=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:17.528739Z",
     "iopub.status.busy": "2026-01-26T15:57:17.528653Z",
     "iopub.status.idle": "2026-01-26T15:57:24.361308Z",
     "shell.execute_reply": "2026-01-26T15:57:24.360926Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building skill dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 3,298,453 unique skills in 6.6s\n",
      "\n",
      "Top 25 skills:\n",
      "+----------------------+------+\n",
      "|skill                 |count |\n",
      "+----------------------+------+\n",
      "|Communication         |368202|\n",
      "|Teamwork              |226205|\n",
      "|Leadership            |184292|\n",
      "|Customer service      |166158|\n",
      "|Communication skills  |116169|\n",
      "|Customer Service      |110400|\n",
      "|Problem Solving       |102020|\n",
      "|Sales                 |92718 |\n",
      "|Problemsolving        |92489 |\n",
      "|Nursing               |87419 |\n",
      "|Collaboration         |86774 |\n",
      "|Training              |83178 |\n",
      "|Project Management    |81080 |\n",
      "|Communication Skills  |78700 |\n",
      "|Attention to detail   |75448 |\n",
      "|Microsoft Office Suite|73351 |\n",
      "|Time management       |72460 |\n",
      "|Time Management       |69752 |\n",
      "|Scheduling            |64081 |\n",
      "|Microsoft Office      |60260 |\n",
      "|Multitasking          |59216 |\n",
      "|Adaptability          |58687 |\n",
      "|Patient Care          |58369 |\n",
      "|Attention to Detail   |57658 |\n",
      "|Flexibility           |56530 |\n",
      "+----------------------+------+\n",
      "only showing top 25 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# build skill dictionary from all skills\n",
    "print('Building skill dictionary')\n",
    "start_time = time.time()\n",
    "\n",
    "skill_counts = all_jobs \\\n",
    "    .filter(col('skills').isNotNull()) \\\n",
    "    .select(explode(split(col('skills'), ',')).alias('skill')) \\\n",
    "    .select(trim(col('skill')).alias('skill')) \\\n",
    "    .filter(col('skill') != '') \\\n",
    "    .groupBy('skill') \\\n",
    "    .agg(count('*').alias('count')) \\\n",
    "    .orderBy(col('count').desc())\n",
    "\n",
    "skill_counts.cache()\n",
    "unique_skills = skill_counts.count()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f'\\nFound {unique_skills:,} unique skills in {elapsed:.1f}s')\n",
    "print('\\nTop 25 skills:')\n",
    "skill_counts.show(25, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:24.362401Z",
     "iopub.status.busy": "2026-01-26T15:57:24.362291Z",
     "iopub.status.idle": "2026-01-26T15:57:24.366160Z",
     "shell.execute_reply": "2026-01-26T15:57:24.365768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding UDF defined (with seniority mapping)\n"
     ]
    }
   ],
   "source": [
    "# inline embedding string builder function\n",
    "# this creates natural language descriptions for embedding models\n",
    "\n",
    "def build_embedding_string(title, company, location, skills, seniority, job_type):\n",
    "    # build a natural language string from job posting fields\n",
    "    # template:\n",
    "    # \"Role of {title} at {company} in {location}. Required skills: {skills}.\n",
    "    # Experience level: {seniority}. Work type: {job_type}.\"\n",
    "    parts = []\n",
    "    \n",
    "    # title and company\n",
    "    if title and company and location:\n",
    "        parts.append(f'Role of {title} at {company} in {location}.')\n",
    "    elif title and company:\n",
    "        parts.append(f'Role of {title} at {company}.')\n",
    "    elif title:\n",
    "        parts.append(f'Role of {title}.')\n",
    "    \n",
    "    # skills - take first 15 to keep string manageable\n",
    "    if skills:\n",
    "        skill_list = [s.strip() for s in skills.split(',')][:15]\n",
    "        if skill_list:\n",
    "            parts.append(f'Required skills: {\", \".join(skill_list)}.')\n",
    "    \n",
    "    # seniority with mapped experience levels\n",
    "    seniority_map = {\n",
    "        'intern': 'Internship level',\n",
    "        'junior': 'Junior level, 1-2 years experience',\n",
    "        'mid': 'Mid-level, 3-5 years experience',\n",
    "        'senior': 'Senior level, 5+ years experience',\n",
    "        'lead/principal': 'Principal level, 8+ years experience'\n",
    "    }\n",
    "    if seniority and seniority in seniority_map:\n",
    "        parts.append(f'Experience level: {seniority_map[seniority]}.')\n",
    "    \n",
    "    # job type\n",
    "    if job_type and job_type not in ['nan', 'None', '']:\n",
    "        parts.append(f'Work type: {job_type}.')\n",
    "    \n",
    "    return ' '.join(parts) if parts else ''\n",
    "\n",
    "# register as spark UDF\n",
    "build_embedding_udf = udf(build_embedding_string, StringType())\n",
    "print('Embedding UDF defined (with seniority mapping)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:24.367068Z",
     "iopub.status.busy": "2026-01-26T15:57:24.366972Z",
     "iopub.status.idle": "2026-01-26T15:57:25.686030Z",
     "shell.execute_reply": "2026-01-26T15:57:25.685629Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building embedding strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated embeddings for 1,348,711 jobs in 1.5s\n"
     ]
    }
   ],
   "source": [
    "# generate embedding strings for all jobs\n",
    "print('Building embedding strings')\n",
    "start_time = time.time()\n",
    "\n",
    "jobs_with_embeddings = all_jobs.withColumn(\n",
    "    'embedding_text',\n",
    "    build_embedding_udf(\n",
    "        col('job_title'), \n",
    "        col('company'), \n",
    "        col('job_location'),\n",
    "        col('skills'), \n",
    "        col('seniority'), \n",
    "        col('job_type')\n",
    "    )\n",
    ")\n",
    "\n",
    "final_count = jobs_with_embeddings.count()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f'\\nGenerated embeddings for {final_count:,} jobs in {elapsed:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:25.687177Z",
     "iopub.status.busy": "2026-01-26T15:57:25.687062Z",
     "iopub.status.idle": "2026-01-26T15:57:29.146480Z",
     "shell.execute_reply": "2026-01-26T15:57:29.146098Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample embedding strings:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                job_title|                                                                                      embedding_text|\n",
      "+-----------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|            Community Nurse (RMN -Band 6)|Role of Community Nurse (RMN -Band 6) at Hays in Telford, England, United Kingdom. Required skill...|\n",
      "|Software Lead Engineer in Cudahy, WI, USA|Role of Software Lead Engineer in Cudahy, WI, USA at Energy Jobline in Cudahy, WI. Required skill...|\n",
      "|                 Nursing Clinical Adjunct|Role of Nursing Clinical Adjunct at State of South Carolina in Beaufort, SC. Required skills: Nur...|\n",
      "|                  Virtualization Engineer|Role of Virtualization Engineer at Epic in Lawrence, KS. Required skills: VMware Engineer, VMware...|\n",
      "|                  LEAD SALES ASSOCIATE-FT|Role of LEAD SALES ASSOCIATE-FT at Dollar General in Lake Butler, FL. Required skills: Customer s...|\n",
      "+-----------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# show sample embedding strings\n",
    "print('\\nSample embedding strings:')\n",
    "jobs_with_embeddings.filter(\n",
    "    col('skills').isNotNull()\n",
    ").select(\n",
    "    'job_title', 'embedding_text'\n",
    ").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:29.147500Z",
     "iopub.status.busy": "2026-01-26T15:57:29.147371Z",
     "iopub.status.idle": "2026-01-26T15:57:29.149913Z",
     "shell.execute_reply": "2026-01-26T15:57:29.149591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output paths:\n",
      " linkedin: /home/developer/project/output/processed/linkedin\n",
      " skills:  /home/developer/project/output/skill_dictionary\n"
     ]
    }
   ],
   "source": [
    "# define output paths\n",
    "# LinkedIn-specific output (processed with skills JOIN)\n",
    "linkedin_path = os.path.join(OUTPUT_DIR, 'processed', 'linkedin')\n",
    "skill_dict_path = os.path.join(OUTPUT_DIR, 'skill_dictionary')\n",
    "\n",
    "os.makedirs(linkedin_path, exist_ok=True)\n",
    "os.makedirs(skill_dict_path, exist_ok=True)\n",
    "\n",
    "print('Output paths:')\n",
    "print(f' linkedin: {linkedin_path}')\n",
    "print(f' skills:  {skill_dict_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:29.150609Z",
     "iopub.status.busy": "2026-01-26T15:57:29.150538Z",
     "iopub.status.idle": "2026-01-26T15:57:38.192186Z",
     "shell.execute_reply": "2026-01-26T15:57:38.191832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving LinkedIn jobs to /home/developer/project/output/processed/linkedin/linkedin_jobs_with_skills\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n",
      "\n",
      "Saving skill dictionary to /home/developer/project/output/skill_dictionary/all_skills\n",
      "Saved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save LinkedIn jobs with skills and seniority\n",
    "output_file = os.path.join(linkedin_path, 'linkedin_jobs_with_skills')\n",
    "print(f'Saving LinkedIn jobs to {output_file}')\n",
    "jobs_with_embeddings.write.mode('overwrite').parquet(output_file)\n",
    "print('Saved')\n",
    "\n",
    "# save skill dictionary\n",
    "skill_output = os.path.join(skill_dict_path, 'all_skills')\n",
    "print(f'\\nSaving skill dictionary to {skill_output}')\n",
    "skill_counts.write.mode('overwrite').parquet(skill_output)\n",
    "print('Saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:38.193453Z",
     "iopub.status.busy": "2026-01-26T15:57:38.193365Z",
     "iopub.status.idle": "2026-01-26T15:57:40.479851Z",
     "shell.execute_reply": "2026-01-26T15:57:40.479396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seniority Levels (extracted from job titles):\n",
      "+--------------+---------+----------+\n",
      "|     seniority|job_count|percentage|\n",
      "+--------------+---------+----------+\n",
      "|           mid|  1081113|      80.2|\n",
      "|        senior|   150007|      11.1|\n",
      "|lead/principal|    84749|       6.3|\n",
      "|        junior|    31258|       2.3|\n",
      "|        intern|     1584|       0.1|\n",
      "+--------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show seniority stats\n",
    "print('Seniority Levels (extracted from job titles):')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT seniority, COUNT(*) as job_count,\n",
    "           ROUND(COUNT(*) * 100.0 / (SELECT COUNT(*) FROM jobs), 1) as percentage\n",
    "    FROM jobs\n",
    "    GROUP BY seniority ORDER BY job_count DESC\n",
    "\"\"\").show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:40.480772Z",
     "iopub.status.busy": "2026-01-26T15:57:40.480689Z",
     "iopub.status.idle": "2026-01-26T15:57:42.071553Z",
     "shell.execute_reply": "2026-01-26T15:57:42.071022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Levels:\n",
      "+----------+---------+\n",
      "| job_level|job_count|\n",
      "+----------+---------+\n",
      "|Mid senior|  1204445|\n",
      "| Associate|   144009|\n",
      "+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show job level stats\n",
    "print('Job Levels:')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT job_level, COUNT(*) as job_count\n",
    "    FROM jobs WHERE job_level IS NOT NULL AND job_level != '' AND job_level != 'nan'\n",
    "    GROUP BY job_level ORDER BY job_count DESC\n",
    "\"\"\").show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:42.072637Z",
     "iopub.status.busy": "2026-01-26T15:57:42.072528Z",
     "iopub.status.idle": "2026-01-26T15:57:42.076281Z",
     "shell.execute_reply": "2026-01-26T15:57:42.075637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK PROCESSING COMPLETE\n",
      "\n",
      "Total jobs: 1,348,711\n",
      "Jobs with skills: 1,294,374\n",
      "Unique skills: 3,298,453\n",
      "\n",
      "Seniority extracted from job titles (5 levels):\n",
      " - lead/principal, senior, mid, junior, intern\n",
      "\n",
      "Outputs:\n",
      " - /home/developer/project/output/processed/linkedin/linkedin_jobs_with_skills/\n",
      " - /home/developer/project/output/skill_dictionary/all_skills/\n"
     ]
    }
   ],
   "source": [
    "# summary\n",
    "print('SPARK PROCESSING COMPLETE')\n",
    "print(f'\\nTotal jobs: {total_count:,}')\n",
    "print(f'Jobs with skills: {with_skills:,}')\n",
    "print(f'Unique skills: {unique_skills:,}')\n",
    "print(f'\\nSeniority extracted from job titles (5 levels):')\n",
    "print(f' - lead/principal, senior, mid, junior, intern')\n",
    "print(f'\\nOutputs:')\n",
    "print(f' - {linkedin_path}/linkedin_jobs_with_skills/')\n",
    "print(f' - {skill_dict_path}/all_skills/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:42.077405Z",
     "iopub.status.busy": "2026-01-26T15:57:42.077265Z",
     "iopub.status.idle": "2026-01-26T15:57:42.253943Z",
     "shell.execute_reply": "2026-01-26T15:57:42.253447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying output\n",
      "Verified: 1,348,711 records\n",
      "\n",
      "Sample with skills and seniority:\n",
      "+--------------------------------------------------+---------+--------------------------------------------------+\n",
      "|                                         job_title|seniority|                                            skills|\n",
      "+--------------------------------------------------+---------+--------------------------------------------------+\n",
      "|          Customer Service Operations Team Manager|      mid|Leadership, Supervision, Team building, Communi...|\n",
      "|Regional Planner IV - Senior Trails and Greenwa...|   senior|Data Analysis, Budgeting, Policy Guidance, Best...|\n",
      "|                   Traffic Control/Foreman Flagger|      mid|Traffic Control, Foreman Flaggers, Google Appli...|\n",
      "|        Licensed Nursing Home Administrator - LNHA|      mid|Nursing Home Administrator, Financial reporting...|\n",
      "|                               Recruitment Advisor|      mid|Recruitment, Candidate Attraction, Job Descript...|\n",
      "+--------------------------------------------------+---------+--------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# verify output\n",
    "print('\\nVerifying output')\n",
    "verify_df = spark.read.parquet(os.path.join(linkedin_path, 'linkedin_jobs_with_skills'))\n",
    "print(f'Verified: {verify_df.count():,} records')\n",
    "print('\\nSample with skills and seniority:')\n",
    "verify_df.filter(\n",
    "    col('skills').isNotNull()\n",
    ").select(\n",
    "    'job_title', 'seniority', 'skills'\n",
    ").show(5, truncate=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-26T15:57:42.254922Z",
     "iopub.status.busy": "2026-01-26T15:57:42.254830Z",
     "iopub.status.idle": "2026-01-26T15:57:42.348039Z",
     "shell.execute_reply": "2026-01-26T15:57:42.347549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "# stop spark\n",
    "spark.stop()\n",
    "print('\\nSpark session stopped')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports loaded\n",
      "project root: /home/developer/project\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand\n",
    "\n",
    "# setup paths and detect project root\n",
    "cwd = os.getcwd()\n",
    "if 'notebooks' in cwd:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print('imports loaded')\n",
    "print(f'project root: {PROJECT_ROOT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/27 01:33:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark started: 4.1.1\n"
     ]
    }
   ],
   "source": [
    "# spark needs more memory for embeddings with 768 floats each\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('TrainValTestSplits') \\\n",
    "    .config('spark.driver.memory', '16g') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .config('spark.driver.maxResultSize', '4g') \\\n",
    "    .getOrCreate()\n",
    "print(f'spark started: {spark.version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading CV splits\n",
      "\n",
      "CV split sizes:\n",
      " training: 5,839 CVs\n",
      " validation: 730 CVs\n",
      " test: 730 CVs\n",
      "\n",
      "split percentages:\n",
      " training: 80.0%\n",
      " validation: 10.0%\n",
      " test: 10.0%\n"
     ]
    }
   ],
   "source": [
    "cv_data_dir = os.path.join(PROJECT_ROOT, 'ingest_cv', 'output')\n",
    "\n",
    "print('loading CV splits')\n",
    "train_cv_ids = pd.read_parquet(cv_data_dir / 'training_set_cv_ids.parquet')\n",
    "val_cv_ids = pd.read_parquet(cv_data_dir / 'validation_set_cv_ids.parquet')\n",
    "test_cv_ids = pd.read_parquet(cv_data_dir / 'test_set_cv_ids.parquet')\n",
    "\n",
    "print(f'\\nCV split sizes:')\n",
    "print(f' training: {len(train_cv_ids):,} CVs')\n",
    "print(f' validation: {len(val_cv_ids):,} CVs')\n",
    "print(f' test: {len(test_cv_ids):,} CVs')\n",
    "total_cvs = len(train_cv_ids) + len(val_cv_ids) + len(test_cv_ids)\n",
    "print(f'\\nsplit percentages:')\n",
    "print(f' training: {len(train_cv_ids)/total_cvs*100:.1f}%')\n",
    "print(f' validation: {len(val_cv_ids)/total_cvs*100:.1f}%')\n",
    "print(f' test: {len(test_cv_ids)/total_cvs*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train n val: 0\n",
      "train n test: 0\n",
      "val n test: 0\n",
      "\n",
      "CV splits validation: PASSED (no overlap)\n"
     ]
    }
   ],
   "source": [
    "# check for overlaps\n",
    "train_cv_set = set(train_cv_ids.iloc[:, 0].values)\n",
    "val_cv_set = set(val_cv_ids.iloc[:, 0].values)\n",
    "test_cv_set = set(test_cv_ids.iloc[:, 0].values)\n",
    "print(f'\\ntrain n val: {len(train_cv_set & val_cv_set)}')\n",
    "print(f'train n test: {len(train_cv_set & test_cv_set)}')\n",
    "print(f'val n test: {len(val_cv_set & test_cv_set)}')\n",
    "no_overlap = (\n",
    "    len(train_cv_set & val_cv_set) == 0 and\n",
    "    len(train_cv_set & test_cv_set) == 0 and\n",
    "    len(val_cv_set & test_cv_set) == 0\n",
    ")\n",
    "if no_overlap:\n",
    "    print('\\nCV splits validation: PASSED (no overlap)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading job embeddings with Spark\n",
      "loaded 165,193 jobs\n",
      "columns: ['job_id', 'embedding_text', 'embedding', 'isco_code']\n",
      "\n",
      "sample:\n",
      "+------+------------------------------------------------------------+------------------------------------------------------------+---------+\n",
      "|job_id|                                              embedding_text|                                                   embedding|isco_code|\n",
      "+------+------------------------------------------------------------+------------------------------------------------------------+---------+\n",
      "|    B7|passage: Role of $18.00 Assistant Manager at McDonald's i...|[-0.030975342, -0.018707275, -0.029586792, -0.03857422, 0...|        1|\n",
      "|   B16|passage: Role of $4,500 Sign on Bonus - MDS Coordinator a...|[-0.013710022, -0.016983032, 0.0032138824, -0.0390625, 0....|        1|\n",
      "|   B18|passage: Role of $45/hr - School RN at Maxim Healthcare S...|[-0.014793396, -0.046905518, -0.0357666, -0.013916016, 0....|        2|\n",
      "+------+------------------------------------------------------------+------------------------------------------------------------+---------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "# load job embeddings with Spark\n",
    "jobs_path = str(os.path.join(PROJECT_ROOT, 'training', 'output', 'embeddings', 'jobs_embedded.parquet'))\n",
    "\n",
    "print('loading job embeddings with Spark')\n",
    "jobs_df = spark.read.parquet(jobs_path)\n",
    "total_jobs = jobs_df.count()\n",
    "print(f'loaded {total_jobs:,} jobs')\n",
    "print(f'columns: {jobs_df.columns}')\n",
    "print('\\nsample:')\n",
    "jobs_df.show(3, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "has isco_code column: True\n",
      "\n",
      "ISCO distribution in embeddings:\n",
      "  0: 639\n",
      "  1: 59,897\n",
      "  2: 84,320\n",
      "  3: 12,146\n",
      "  4: 1,220\n",
      "  5: 3,051\n",
      "  6: 157\n",
      "  7: 1,255\n",
      "  8: 1,418\n",
      "  9: 1,090\n"
     ]
    }
   ],
   "source": [
    "# check if isco_code column exists from 05 stratified sampling\n",
    "has_isco = 'isco_code' in jobs_df.columns\n",
    "print(f'has isco_code column: {has_isco}')\n",
    "\n",
    "if has_isco:\n",
    "    print('\\nISCO distribution in embeddings:')\n",
    "    isco_counts = jobs_df.groupBy('isco_code').count().orderBy('isco_code').collect()\n",
    "    for row in isco_counts:\n",
    "        print(f'  {row[\"isco_code\"]}: {row[\"count\"]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating stratified job splits by ISCO domain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/27 01:34:16 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stratified sampling complete\n"
     ]
    }
   ],
   "source": [
    "# ISCO names for display\n",
    "ISCO_NAMES = {\n",
    "    0: 'Military',\n",
    "    1: 'Managers',\n",
    "    2: 'Professionals', \n",
    "    3: 'Technicians',\n",
    "    4: 'Clerical',\n",
    "    5: 'Service/Sales',\n",
    "    6: 'Agriculture',\n",
    "    7: 'Craft/Trade',\n",
    "    8: 'Operators',\n",
    "    9: 'Elementary'\n",
    "}\n",
    "\n",
    "if has_isco:\n",
    "    # stratified split using sampleBy with fractions per ISCO code\n",
    "    print('creating stratified job splits by ISCO domain...')\n",
    "    \n",
    "    # get counts per domain for fraction calculation\n",
    "    domain_counts = {row['isco_code']: row['count'] for row in isco_counts}\n",
    "    \n",
    "    # calculate fractions for 80/10/10 split per domain\n",
    "    # train: 0.80, val: 0.10, test: 0.10\n",
    "    train_fractions = {code: 0.80 for code in domain_counts.keys()}\n",
    "    val_fractions = {code: 0.10 for code in domain_counts.keys()}\n",
    "    test_fractions = {code: 0.10 for code in domain_counts.keys()}\n",
    "    \n",
    "    # shuffle first for randomness\n",
    "    jobs_shuffled = jobs_df.orderBy(rand(seed=42))\n",
    "    \n",
    "    # sample train set\n",
    "    train_jobs_df = jobs_shuffled.sampleBy('isco_code', train_fractions, seed=42)\n",
    "    \n",
    "    # get remaining jobs (not in train)\n",
    "    train_ids = set(train_jobs_df.select('job_id').rdd.flatMap(lambda x: x).collect())\n",
    "    remaining_df = jobs_shuffled.filter(~col('job_id').isin(list(train_ids)))\n",
    "    \n",
    "    # split remaining 50/50 for val/test\n",
    "    val_test_fractions = {code: 0.50 for code in domain_counts.keys()}\n",
    "    val_jobs_df = remaining_df.sampleBy('isco_code', val_test_fractions, seed=43)\n",
    "    \n",
    "    # test is everything not in train or val\n",
    "    val_ids = set(val_jobs_df.select('job_id').rdd.flatMap(lambda x: x).collect())\n",
    "    test_jobs_df = remaining_df.filter(~col('job_id').isin(list(val_ids)))\n",
    "    \n",
    "    print('stratified sampling complete')\n",
    "    \n",
    "else:\n",
    "    # fallback to random split if no isco_code\n",
    "    print('WARNING: no isco_code column, falling back to random split')\n",
    "    jobs_shuffled = jobs_df.orderBy(rand(seed=42))\n",
    "    train_jobs_df, val_jobs_df, test_jobs_df = jobs_shuffled.randomSplit([0.8, 0.1, 0.1], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/27 01:34:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:27 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:28 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:30 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "26/01/27 01:34:31 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "26/01/27 01:34:31 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "26/01/27 01:34:32 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "26/01/27 01:34:33 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "job split sizes:\n",
      "  training: 132,170 jobs (80.1%)\n",
      "  validation: 16,378 jobs (9.9%)\n",
      "  test: 16,428 jobs (10.0%)\n",
      "  total: 164,976\n"
     ]
    }
   ],
   "source": [
    "# cache and count\n",
    "train_jobs_df.cache()\n",
    "val_jobs_df.cache()\n",
    "test_jobs_df.cache()\n",
    "\n",
    "train_count = train_jobs_df.count()\n",
    "val_count = val_jobs_df.count()\n",
    "test_count = test_jobs_df.count()\n",
    "total_split = train_count + val_count + test_count\n",
    "\n",
    "print(f'\\njob split sizes:')\n",
    "print(f'  training: {train_count:,} jobs ({train_count/total_split*100:.1f}%)')\n",
    "print(f'  validation: {val_count:,} jobs ({val_count/total_split*100:.1f}%)')\n",
    "print(f'  test: {test_count:,} jobs ({test_count/total_split*100:.1f}%)')\n",
    "print(f'  total: {total_split:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/27 01:34:36 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "26/01/27 01:34:39 WARN DAGScheduler: Broadcasting large task binary with size 3.5 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ISCO                 Train        Val       Test\n",
      "--------------------------------------------------\n",
      "0 Military            517         50         62\n",
      "1 Managers         47,874      5,937      5,977\n",
      "2 Professionals     67,617      8,421      8,388\n",
      "3 Technicians       9,677      1,174      1,222\n",
      "4 Clerical            980        109        113\n",
      "5 Service/Sales      2,423        324        303\n",
      "6 Agriculture         128         16         15\n",
      "7 Craft/Trade         967        114        125\n",
      "8 Operators         1,121        144        118\n",
      "9 Elementary          866         89        105\n",
      "\n",
      "stratification: VERIFIED\n"
     ]
    }
   ],
   "source": [
    "# verify stratification maintained in each split\n",
    "if has_isco:\n",
    "    train_isco = train_jobs_df.groupBy('isco_code').count().orderBy('isco_code').collect()\n",
    "    val_isco = val_jobs_df.groupBy('isco_code').count().orderBy('isco_code').collect()\n",
    "    test_isco = test_jobs_df.groupBy('isco_code').count().orderBy('isco_code').collect()\n",
    "    \n",
    "    print(f'\\n{\"ISCO\":15} {\"Train\":>10} {\"Val\":>10} {\"Test\":>10}')\n",
    "    print('-' * 50)\n",
    "    \n",
    "    train_dict = {r['isco_code']: r['count'] for r in train_isco}\n",
    "    val_dict = {r['isco_code']: r['count'] for r in val_isco}\n",
    "    test_dict = {r['isco_code']: r['count'] for r in test_isco}\n",
    "    \n",
    "    all_codes = sorted(set(train_dict.keys()) | set(val_dict.keys()) | set(test_dict.keys()))\n",
    "    for code in all_codes:\n",
    "        name = ISCO_NAMES.get(code, 'Unknown')\n",
    "        t = train_dict.get(code, 0)\n",
    "        v = val_dict.get(code, 0)\n",
    "        ts = test_dict.get(code, 0)\n",
    "        print(f'{code} {name:12} {t:>10,} {v:>10,} {ts:>10,}')\n",
    "    \n",
    "    print('\\nstratification: VERIFIED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/27 01:34:45 WARN DAGScheduler: Broadcasting large task binary with size 3.4 MiB\n",
      "26/01/27 01:34:48 WARN DAGScheduler: Broadcasting large task binary with size 3.7 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved job splits:\n",
      "  /home/developer/project/output/splits/train_jobs.parquet: 132,170 jobs\n",
      "  /home/developer/project/output/splits/val_jobs.parquet: 16,378 jobs\n",
      "  /home/developer/project/output/splits/test_jobs.parquet: 16,428 jobs\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(PROJECT_ROOT, 'training', 'output', 'splits')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_jobs_path = output_dir / 'train_jobs.parquet'\n",
    "val_jobs_path = output_dir / 'val_jobs.parquet'\n",
    "test_jobs_path = output_dir / 'test_jobs.parquet'\n",
    "\n",
    "train_jobs_df.write.mode('overwrite').parquet(str(train_jobs_path))\n",
    "val_jobs_df.write.mode('overwrite').parquet(str(val_jobs_path))\n",
    "test_jobs_df.write.mode('overwrite').parquet(str(test_jobs_path))\n",
    "\n",
    "print(f'saved job splits:')\n",
    "print(f'  {train_jobs_path}: {train_count:,} jobs')\n",
    "print(f'  {val_jobs_path}: {val_count:,} jobs')\n",
    "print(f'  {test_jobs_path}: {test_count:,} jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading CV embeddings with Spark\n",
      "loaded 7,299 CVs\n",
      "\n",
      "columns: ['cv_id', 'embedding_text', 'embedding']\n",
      "\n",
      "sample:\n",
      "+-----+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|cv_id|                                              embedding_text|                                                   embedding|\n",
      "+-----+------------------------------------------------------------+------------------------------------------------------------+\n",
      "|   A1|Query: I am a Python Developer with 0 years of experience...|[0.016677856, -0.032989502, -0.033813477, 0.004070282, 0....|\n",
      "|   A2|Query: I am a Operations Manager with 11 years of experie...|[0.005622864, -0.026641846, -0.05456543, 0.0016298294, 0....|\n",
      "|   A3|Query: I am a DevOps Engineer with 0 years of experience,...|[0.008331299, -0.030975342, -0.05633545, 0.013511658, 0.0...|\n",
      "+-----+------------------------------------------------------------+------------------------------------------------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "cvs_path = str(os.path.join(PROJECT_ROOT, 'training', 'output', 'embeddings', 'cvs_embedded.parquet'))\n",
    "\n",
    "print('loading CV embeddings with Spark')\n",
    "cvs_df = spark.read.parquet(cvs_path)\n",
    "\n",
    "print(f'loaded {cvs_df.count():,} CVs')\n",
    "print(f'\\ncolumns: {cvs_df.columns}')\n",
    "print('\\nsample:')\n",
    "cvs_df.show(3, truncate=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV ID column: anchor\n",
      "\n",
      "filtering CVs based on existing splits\n"
     ]
    }
   ],
   "source": [
    "# get CV ID column name\n",
    "cv_id_col = train_cv_ids.columns[0]\n",
    "print(f'CV ID column: {cv_id_col}')\n",
    "# convert to lists for filtering\n",
    "train_cv_list = train_cv_ids[cv_id_col].tolist()\n",
    "val_cv_list = val_cv_ids[cv_id_col].tolist()\n",
    "test_cv_list = test_cv_ids[cv_id_col].tolist()\n",
    "\n",
    "print(f'\\nfiltering CVs based on existing splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV split sizes:\n",
      " training: 5,839 CVs\n",
      " validation: 730 CVs\n",
      " test: 730 CVs\n",
      "\n",
      "all CV IDs found in embeddings\n"
     ]
    }
   ],
   "source": [
    "# filter with Spark\n",
    "train_cvs_df = cvs_df.filter(col('cv_id').isin(train_cv_list))\n",
    "val_cvs_df = cvs_df.filter(col('cv_id').isin(val_cv_list))\n",
    "test_cvs_df = cvs_df.filter(col('cv_id').isin(test_cv_list))\n",
    "\n",
    "# cache and count\n",
    "train_cvs_df.cache()\n",
    "val_cvs_df.cache()\n",
    "test_cvs_df.cache()\n",
    "\n",
    "train_cv_count = train_cvs_df.count()\n",
    "val_cv_count = val_cvs_df.count()\n",
    "test_cv_count = test_cvs_df.count()\n",
    "\n",
    "print(f'\\nCV split sizes:')\n",
    "print(f' training: {train_cv_count:,} CVs')\n",
    "print(f' validation: {val_cv_count:,} CVs')\n",
    "print(f' test: {test_cv_count:,} CVs')\n",
    "\n",
    "if train_cv_count == len(train_cv_ids):\n",
    "    print('\\nall CV IDs found in embeddings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved CV splits:\n",
      "  /home/developer/project/output/splits/train_cvs.parquet: 5,839 CVs\n",
      "  /home/developer/project/output/splits/val_cvs.parquet: 730 CVs\n",
      "  /home/developer/project/output/splits/test_cvs.parquet: 730 CVs\n"
     ]
    }
   ],
   "source": [
    "train_cvs_path = output_dir / 'train_cvs.parquet'\n",
    "val_cvs_path = output_dir / 'val_cvs.parquet'\n",
    "test_cvs_path = output_dir / 'test_cvs.parquet'\n",
    "\n",
    "train_cvs_df.write.mode('overwrite').parquet(str(train_cvs_path))\n",
    "val_cvs_df.write.mode('overwrite').parquet(str(val_cvs_path))\n",
    "test_cvs_df.write.mode('overwrite').parquet(str(test_cvs_path))\n",
    "\n",
    "print(f'saved CV splits:')\n",
    "print(f'  {train_cvs_path}: {train_cv_count:,} CVs')\n",
    "print(f'  {val_cvs_path}: {val_cv_count:,} CVs')\n",
    "print(f'  {test_cvs_path}: {test_cv_count:,} CVs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "jobs (stratified by ISCO):\n",
      " training: 132,170 (80.1%)\n",
      " validation: 16,378 (9.9%)\n",
      " test: 16,428 (10.0%)\n",
      " total: 164,976\n",
      "\n",
      "CVs (from colleague splits):\n",
      " training: 5,839 (80.0%)\n",
      " validation: 730 (10.0%)\n",
      " test: 730 (10.0%)\n",
      " total: 7,299\n",
      "\n",
      "validation checks:\n",
      " CV splits no overlap: True\n",
      " job splits stratified: True\n",
      "\n",
      "ratios (jobs:CVs):\n",
      " training: 132170:5839 = 1:265\n",
      " validation: 16378:730\n",
      " test: 16428:730\n",
      "\n",
      "all splits ready for training\n"
     ]
    }
   ],
   "source": [
    "print(f'\\njobs (stratified by ISCO):')\n",
    "print(f' training: {train_count:,} ({train_count/total_split*100:.1f}%)')\n",
    "print(f' validation: {val_count:,} ({val_count/total_split*100:.1f}%)')\n",
    "print(f' test: {test_count:,} ({test_count/total_split*100:.1f}%)')\n",
    "print(f' total: {total_split:,}')\n",
    "\n",
    "print(f'\\nCVs (from colleague splits):')\n",
    "print(f' training: {train_cv_count:,} ({train_cv_count/total_cvs*100:.1f}%)')\n",
    "print(f' validation: {val_cv_count:,} ({val_cv_count/total_cvs*100:.1f}%)')\n",
    "print(f' test: {test_cv_count:,} ({test_cv_count/total_cvs*100:.1f}%)')\n",
    "print(f' total: {total_cvs:,}')\n",
    "\n",
    "print(f'\\nvalidation checks:')\n",
    "print(f' CV splits no overlap: {no_overlap}')\n",
    "if has_isco:\n",
    "    print(f' job splits stratified: True')\n",
    "\n",
    "print(f'\\nratios (jobs:CVs):')\n",
    "print(f' training: {train_count}:{train_cv_count} = 1:{train_cv_count//max(1,train_count//train_cv_count if train_cv_count else 1)}')\n",
    "print(f' validation: {val_count}:{val_cv_count}')\n",
    "print(f' test: {test_count}:{test_cv_count}')\n",
    "\n",
    "print(f'\\nall splits ready for training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output directory: /home/developer/project/output/splits/\n",
      "\n",
      "job splits (stratified by ISCO):\n",
      " train_jobs.parquet: 132,170 jobs (80%)\n",
      " val_jobs.parquet: 16,378 jobs (10%)\n",
      " test_jobs.parquet: 16,428 jobs (10%)\n",
      "\n",
      "CV splits (from colleague):\n",
      " train_cvs.parquet: 5,839 CVs (80%)\n",
      " val_cvs.parquet: 730 CVs (10%)\n",
      " test_cvs.parquet: 730 CVs (10%)\n",
      "\n",
      "all splits created with Spark\n"
     ]
    }
   ],
   "source": [
    "print(f'output directory: {output_dir}/')\n",
    "print(f'\\njob splits (stratified by ISCO):')\n",
    "print(f' train_jobs.parquet: {train_count:,} jobs (80%)')\n",
    "print(f' val_jobs.parquet: {val_count:,} jobs (10%)')\n",
    "print(f' test_jobs.parquet: {test_count:,} jobs (10%)')\n",
    "print(f'\\nCV splits (from colleague):')\n",
    "print(f' train_cvs.parquet: {train_cv_count:,} CVs (80%)')\n",
    "print(f' val_cvs.parquet: {val_cv_count:,} CVs (10%)')\n",
    "print(f' test_cvs.parquet: {test_cv_count:,} CVs (10%)')\n",
    "print(f'\\nall splits created with Spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark stopped\n"
     ]
    }
   ],
   "source": [
    "spark.stop()\n",
    "print('spark stopped')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

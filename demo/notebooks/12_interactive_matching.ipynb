{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /home/developer/project\n"
     ]
    }
   ],
   "source": [
    "# Interactive CV-Job Matching\n",
    "# You can upload a CV (or paste text) and find matching jobs from 1.3M+ job postings\n",
    "\n",
    "# - Upload CV as PDF/DOCX/TXT or paste directly\n",
    "# - Search 1.3M+ jobs using bi-encoder + cross-encoder\n",
    "# - Take actions (apply, save, skip) to provide feedback\n",
    "# - Feedback is logged to SQLite for future model retraining\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Find project root (cv-job-matcher-project/)\n",
    "# Notebooks are at */notebooks/ so we need to go up TWO levels\n",
    "cwd = os.getcwd()\n",
    "if 'notebooks' in cwd:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(cwd))  # TWO levels up\n",
    "else:\n",
    "    PROJECT_ROOT = cwd\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /home/developer/project\n"
     ]
    }
   ],
   "source": [
    "# set working directory first\n",
    "import os\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded successfully\n",
      "PyTorch version: 2.6.0+cu124\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import sys\n",
    "import uuid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# IPython display stuff\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# widgets for interactive UI\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# add project root to path so we can import our modules\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# our ML models\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "\n",
    "# our custom modules from demo scripts\n",
    "from demo.scripts.feedback_storage import init_db, log_action, get_action_count, get_action_summary\n",
    "from demo.scripts.document_parser import parse_document, detect_document_type\n",
    "\n",
    "print(\"Imports loaded successfully\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Bi-encoder model: /home/developer/project/training/output/models/cv-job-matcher-e5 (directory)\n",
      "  [OK] Faiss index: /home/developer/project/training/output/indexes/jobs_full_index.faiss (4149.4 MB)\n",
      "  [OK] Job IDs: /home/developer/project/training/output/indexes/jobs_full_ids.npy (193.8 MB)\n",
      "  [OK] Jobs parquet: /home/developer/project/ingest_job_postings/output/unified_job_postings/unified_jobs.parquet (directory)\n",
      "\n",
      "All files found, ready to load.\n"
     ]
    }
   ],
   "source": [
    "# make sure all required files exist before loading\n",
    "# this prevents confusing errors later\n",
    "\n",
    "\n",
    "MODEL_PATH = os.path.join(PROJECT_ROOT, \"training\", \"output\", \"models\", \"cv-job-matcher-e5\")\n",
    "INDEX_PATH = os.path.join(PROJECT_ROOT, \"training\", \"output\", \"indexes\", \"jobs_full_index.faiss\")\n",
    "IDS_PATH = os.path.join(PROJECT_ROOT, \"training\", \"output\", \"indexes\", \"jobs_full_ids.npy\")\n",
    "JOBS_PATH = os.path.join(PROJECT_ROOT, \"ingest_job_postings\", \"output\", \"unified_job_postings\", \"unified_jobs.parquet\")\n",
    "\n",
    "# check each file\n",
    "checks = [\n",
    "    (MODEL_PATH, \"Bi-encoder model\"),\n",
    "    (INDEX_PATH, \"Faiss index\"),\n",
    "    (IDS_PATH, \"Job IDs\"),\n",
    "    (JOBS_PATH, \"Jobs parquet\")\n",
    "]\n",
    "\n",
    "all_ok = True\n",
    "for path, name in checks:\n",
    "    if Path(path).exists():\n",
    "        # show file size too\n",
    "        if Path(path).is_file():\n",
    "            size_mb = Path(path).stat().st_size / 1e6\n",
    "            print(f\"  [OK] {name}: {path} ({size_mb:.1f} MB)\")\n",
    "        else:\n",
    "            print(f\"  [OK] {name}: {path} (directory)\")\n",
    "    else:\n",
    "        print(f\"  [MISSING] {name}: {path}\")\n",
    "        all_ok = False\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\nAll files found, ready to load.\")\n",
    "else:\n",
    "    print(\"\\nERROR: Some files missing. Run previous notebooks first.\")\n",
    "    raise FileNotFoundError(\"Required files missing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 18:05:00,542 - Load pretrained SentenceTransformer: /home/developer/project/training/output/models/cv-job-matcher-e5\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bi-encoder loaded\n",
      " Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "# load bi-encoder (our fine-tuned model)\n",
    "# load with fp16 for faster inference\n",
    "bi_encoder = SentenceTransformer(\n",
    "    MODEL_PATH,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}  # fp16 precision\n",
    ")\n",
    "emb_dim = bi_encoder.get_sentence_embedding_dimension()\n",
    "print(f\" Bi-encoder loaded\")\n",
    "print(f\" Embedding dimension: {emb_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Cross-encoder loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# load cross-encoder for reranking\n",
    "# this is a pre-trained model, we use it as-is\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L12-v2\", device=device)\n",
    "print(f\" Cross-encoder loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Index loaded in 2.9s\n",
      "  Total jobs: 1,345,711\n",
      "  nprobe set to: 20\n"
     ]
    }
   ],
   "source": [
    "# load Faiss index\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "jobs_index = faiss.read_index(INDEX_PATH)\n",
    "jobs_index.nprobe = 20  # number of clusters to search (balance speed/accuracy)\n",
    "\n",
    "load_time = time.time() - start\n",
    "print(f\"  Index loaded in {load_time:.1f}s\")\n",
    "print(f\"  Total jobs: {jobs_index.ntotal:,}\")\n",
    "print(f\"  nprobe set to: {jobs_index.nprobe}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Job IDs loaded: 1,345,711\n",
      "  Jobs dataframe loaded: 1,345,711 rows\n",
      "  ID lookup built\n",
      "\n",
      "Job columns: ['id', 'job_title', 'company', 'job_location', 'skills', 'seniority', 'embedding_text']\n"
     ]
    }
   ],
   "source": [
    "# load job IDs and metadata\n",
    "\n",
    "# these are the IDs that correspond to index positions\n",
    "job_ids = np.load(IDS_PATH, allow_pickle=True)\n",
    "print(f\"  Job IDs loaded: {len(job_ids):,}\")\n",
    "\n",
    "# full job data\n",
    "jobs_df = pd.read_parquet(JOBS_PATH)\n",
    "print(f\"  Jobs dataframe loaded: {len(jobs_df):,} rows\")\n",
    "\n",
    "# build a lookup dict: job_id -> dataframe index\n",
    "# because job_ids array might not be in same order as dataframe\n",
    "job_id_to_row = {jid: idx for idx, jid in enumerate(jobs_df['id'])}\n",
    "print(f\"  ID lookup built\")\n",
    "\n",
    "print(f\"\\nJob columns: {list(jobs_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database initialized at /home/developer/project/demo/data/feedback/feedback.db\n",
      "\n",
      "Feedback database ready\n",
      "Current action count: 36\n"
     ]
    }
   ],
   "source": [
    "# initialize the feedback database\n",
    "# this creates the SQLite file if it doesn't exist\n",
    "init_db()\n",
    "\n",
    "# show current stats\n",
    "current_count = get_action_count()\n",
    "print(f\"\\nFeedback database ready\")\n",
    "print(f\"Current action count: {current_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Jobs searchable: 1,345,711\n",
      "  Bi-encoder: 768D embeddings\n",
      "  Cross-encoder: on cuda\n",
      "  Feedback actions logged: 36\n"
     ]
    }
   ],
   "source": [
    "# final status\n",
    "print(f\"  Jobs searchable: {jobs_index.ntotal:,}\")\n",
    "print(f\"  Bi-encoder: {emb_dim}D embeddings\")\n",
    "print(f\"  Cross-encoder: on {device}\")\n",
    "print(f\"  Feedback actions logged: {current_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session ID: c69d705d\n",
      "Mode: Job Seeker (upload CV -> find matching jobs)\n",
      "Results to show: 10\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION: Number of results to show\n",
    "# Change this value to see more or fewer results\n",
    "NUM_RESULTS = 10\n",
    "\n",
    "# session state, keeps track of current user, CV, and matches\n",
    "class DemoState:\n",
    "    '''holds state for the demo session'''\n",
    "\n",
    "    def __init__(self):\n",
    "        # unique session ID for this run\n",
    "        self.session_id = str(uuid.uuid4())[:8]\n",
    "\n",
    "        # role is fixed to job_seeker in this demo\n",
    "        self.role = 'job_seeker'\n",
    "\n",
    "        # CV text that user uploaded/pasted\n",
    "        self.uploaded_text = None\n",
    "        self.uploaded_id = None\n",
    "\n",
    "        # matching results\n",
    "        self.matches = []\n",
    "        self.current_page = 0\n",
    "\n",
    "        # run counter to prevent stale button clicks\n",
    "        # increments each time we do a new search\n",
    "        self.run_id = 0\n",
    "        \n",
    "        # track which actions already logged (prevent duplicates)\n",
    "        self.logged_actions = set()\n",
    "\n",
    "    def reset(self):\n",
    "        '''reset for a new CV search'''\n",
    "        self.uploaded_text = None\n",
    "        self.uploaded_id = None\n",
    "        self.matches = []\n",
    "        self.current_page = 0\n",
    "        self.run_id += 1\n",
    "        self.logged_actions = set()\n",
    "\n",
    "# create the global state object\n",
    "state = DemoState()\n",
    "\n",
    "print(f\"Session ID: {state.session_id}\")\n",
    "print(f\"Mode: {state.role.replace('_', ' ').title()} (upload CV -> find matching jobs)\")\n",
    "print(f\"Results to show: {NUM_RESULTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cell-11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching functions defined\n"
     ]
    }
   ],
   "source": [
    "def find_matches(query_text, top_k=50):\n",
    "    # Find top-k job matches for a CV using bi-encoder\n",
    "    # Args:\n",
    "    #     query_text: CV text (raw)\n",
    "    #     top_k: Number of candidates to retrieve (default 50)\n",
    "    # Returns:\n",
    "    #     List of match dicts with: job_id, bi_score, title, company...\n",
    "    # e5 models expect a prefix for queries vs passages\n",
    "    # our CV is the query, jobs are passages\n",
    "    prefix = \"query: \"\n",
    "    \n",
    "    # clean up the text in case user accidentally pasted with prefix\n",
    "    clean_text = query_text.replace(\"query: \", \"\").replace(\"Query: \", \"\").replace(\"passage: \", \"\")\n",
    "    prefixed_text = prefix + clean_text\n",
    "    \n",
    "    # encode the query\n",
    "    # normalize_embeddings=True for cosine similarity\n",
    "    query_emb = bi_encoder.encode(\n",
    "        [prefixed_text], \n",
    "        convert_to_numpy=True, \n",
    "        normalize_embeddings=True\n",
    "    )\n",
    "    \n",
    "    # search the Faiss index\n",
    "    # returns: similarities (inner product = cosine for normalized vectors)\n",
    "    #          indices (positions in the index)\n",
    "    similarities, indices = jobs_index.search(query_emb, top_k)\n",
    "    \n",
    "    # build result list\n",
    "    matches = []\n",
    "    for rank, (sim, idx) in enumerate(zip(similarities[0], indices[0]), 1):\n",
    "        # get job ID from the IDs array\n",
    "        job_id = job_ids[idx]\n",
    "        \n",
    "        # get job metadata from dataframe\n",
    "        # need to find the row with this job_id\n",
    "        if job_id in job_id_to_row:\n",
    "            row_idx = job_id_to_row[job_id]\n",
    "            job_row = jobs_df.iloc[row_idx]\n",
    "        else:\n",
    "            # fallback: try direct index access\n",
    "            job_row = jobs_df.iloc[idx]\n",
    "        \n",
    "        matches.append({\n",
    "            'rank': rank,\n",
    "            'job_id': str(job_id),\n",
    "            'bi_score': float(sim),\n",
    "            'title': job_row.get('job_title', 'Unknown Title'),\n",
    "            'company': job_row.get('company', 'Unknown Company'),\n",
    "            'location': job_row.get('job_location', 'Unknown'),\n",
    "            'skills': job_row.get('skills', ''),\n",
    "            'seniority': job_row.get('seniority', ''),\n",
    "            'text': job_row.get('embedding_text', '')\n",
    "        })\n",
    "    \n",
    "    return matches\n",
    "\n",
    "\n",
    "def rerank_matches(query_text, matches, top_k=10):\n",
    "    # Rerank matches using cross-encoder for better accuracy.\n",
    "    # Cross-encoder looks at query+doc together (not separately),\n",
    "    # so it's more accurate but slower, that's why we only use it\n",
    "    # on the top-50 candidates from bi-encoder\n",
    "    \n",
    "    # Args:\n",
    "    #     query_text: Original CV text\n",
    "    #     matches: List of match dicts from find_matches()\n",
    "    #     top_k: Number to return after reranking (default 10)\n",
    "    \n",
    "    # Returns:\n",
    "    #     Reranked list with cross_score added\n",
    "    # clean the query (remove any prefixes)\n",
    "    clean_query = query_text.replace(\"query: \", \"\").replace(\"Query: \", \"\").replace(\"passage: \", \"\")\n",
    "    \n",
    "    # create pairs for cross-encoder: (query, document)\n",
    "    pairs = []\n",
    "    for m in matches:\n",
    "        doc_text = m['text'].replace(\"passage: \", \"\")\n",
    "        pairs.append((clean_query, doc_text))\n",
    "    \n",
    "    # score all pairs\n",
    "    # batch_size=128 is good for GPU\n",
    "    cross_scores = cross_encoder.predict(pairs, batch_size=128)\n",
    "    \n",
    "    # add cross-encoder scores to matches\n",
    "    for m, score in zip(matches, cross_scores):\n",
    "        m['cross_score'] = float(score)\n",
    "    \n",
    "    # sort by cross-encoder score (higher is better)\n",
    "    reranked = sorted(matches, key=lambda x: x['cross_score'], reverse=True)\n",
    "    \n",
    "    return reranked[:top_k]\n",
    "\n",
    "\n",
    "print(\"Matching functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: 'Python developer with 5 years experience in Django, PostgreSQL, and AWS.'\n",
      "\n",
      "Step 1: Bi-encoder search\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215bbba3c2764a20821ca3f5e9cc1ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 5 matches in 224.7ms\n",
      "Step 2: Cross-encoder rerank\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a64d1d440a4408a7d2c7e125cd124e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Reranked to top-3 in 50.1ms\n",
      "\n",
      "Top 3 matches:\n",
      "  1. Senior Engineer / Python Developer with Django at Allnessjobs\n",
      "     bi=0.626, cross=8.30\n",
      "  2. Python/Django Application Developer at Pinnacle Group, Inc.\n",
      "     bi=0.635, cross=8.15\n",
      "  3. Python Developer at SThree\n",
      "     bi=0.586, cross=7.28\n"
     ]
    }
   ],
   "source": [
    "# quick test of matching functions\n",
    "# just to make sure they work before we build the UI\n",
    "\n",
    "test_cv = \"Python developer with 5 years experience in Django, PostgreSQL, and AWS.\"\n",
    "print(f\"Testing with: '{test_cv}'\")\n",
    "print(\"\")\n",
    "\n",
    "# step 1: bi-encoder retrieval\n",
    "print(\"Step 1: Bi-encoder search\")\n",
    "import time\n",
    "start = time.time()\n",
    "test_matches = find_matches(test_cv, top_k=5)\n",
    "bi_time = time.time() - start\n",
    "print(f\"  Found {len(test_matches)} matches in {bi_time*1000:.1f}ms\")\n",
    "\n",
    "# step 2: cross-encoder rerank\n",
    "print(\"Step 2: Cross-encoder rerank\")\n",
    "start = time.time()\n",
    "test_reranked = rerank_matches(test_cv, test_matches, top_k=3)\n",
    "cross_time = time.time() - start\n",
    "print(f\" Reranked to top-3 in {cross_time*1000:.1f}ms\")\n",
    "\n",
    "# show results\n",
    "print(\"\\nTop 3 matches:\")\n",
    "for i, m in enumerate(test_reranked, 1):\n",
    "    print(f\"  {i}. {m['title']} at {m['company']}\")\n",
    "    print(f\"     bi={m['bi_score']:.3f}, cross={m['cross_score']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display functions defined\n"
     ]
    }
   ],
   "source": [
    "def display_match(match, idx, current_run_id):\n",
    "    # display a single match with formatted HTML and action buttons.\n",
    "    # current_run_id is used to prevent stale button clicks\n",
    "    \n",
    "    score = match['cross_score']\n",
    "    if score > 5:\n",
    "        color = '#90EE90'  # light green\n",
    "    elif score > 0:\n",
    "        color = '#FFD700'  # gold\n",
    "    else:\n",
    "        color = '#FFB6C1'  # light pink\n",
    "\n",
    "    html = f'''\n",
    "    <div style=\"border: 1px solid #ddd; padding: 15px; margin: 10px 0; \n",
    "                border-radius: 8px; background: #fafafa;\">\n",
    "        <h3 style=\"margin-top: 0;\">#{idx+1}: {match['title']}</h3>\n",
    "        <p style=\"margin: 5px 0;\">\n",
    "            <strong>Company:</strong> {match['company']} &nbsp;|&nbsp; \n",
    "            <strong>Location:</strong> {match['location']} &nbsp;|&nbsp;\n",
    "            <strong>Level:</strong> {match['seniority']}\n",
    "        </p>\n",
    "        <p style=\"margin: 5px 0;\">\n",
    "            <strong>Match Score:</strong> \n",
    "            <span style=\"background: {color}; padding: 2px 8px; border-radius: 4px; \n",
    "                         font-weight: bold;\">{score:.2f}</span>\n",
    "            <span style=\"color: #888;\">(bi-encoder: {match['bi_score']:.4f})</span>\n",
    "        </p>\n",
    "    '''\n",
    "\n",
    "    skills = match.get('skills', '')\n",
    "    if skills:\n",
    "        if isinstance(skills, list):\n",
    "            skills_str = ', '.join(skills[:8])\n",
    "            if len(skills) > 8:\n",
    "                skills_str += f\" (+{len(skills)-8} more)\"\n",
    "        else:\n",
    "            skills_list = str(skills).split(',')[:8]\n",
    "            skills_str = ', '.join(s.strip() for s in skills_list)\n",
    "        html += f'<p style=\"margin: 5px 0;\"><strong>Skills:</strong> {skills_str}</p>'\n",
    "\n",
    "    preview = match['text'][:400].replace('passage: ', '')\n",
    "    html += f'''\n",
    "        <p style=\"color: #555; font-style: italic; margin-top: 10px; \n",
    "                  padding: 10px; background: #f0f0f0; border-radius: 4px;\">\n",
    "            {preview}...\n",
    "        </p>\n",
    "    </div>\n",
    "    '''\n",
    "\n",
    "    display(HTML(html))\n",
    "\n",
    "    button_configs = [\n",
    "        ('View Full', 'view_full', 'info'),\n",
    "        ('Apply', 'apply', 'success'),\n",
    "        ('Save', 'save', ''),\n",
    "        ('Skip', 'skip', 'warning'),\n",
    "        ('Not Interested', 'not_interested', 'danger')\n",
    "    ]\n",
    "\n",
    "    buttons = []\n",
    "    for label, action, style in button_configs:\n",
    "        btn = widgets.Button(\n",
    "            description=label,\n",
    "            button_style=style,\n",
    "            layout=widgets.Layout(width='120px', margin='2px')\n",
    "        )\n",
    "        \n",
    "        # capture match, action, and run_id in closure\n",
    "        def make_callback(m=match, a=action, rid=current_run_id):\n",
    "            def callback(b):\n",
    "                # only process if this run is still current\n",
    "                if rid != state.run_id:\n",
    "                    return  # stale button, ignore\n",
    "                handle_action(m, a, b)\n",
    "            return callback\n",
    "\n",
    "        btn.on_click(make_callback())\n",
    "        buttons.append(btn)\n",
    "\n",
    "    display(widgets.HBox(buttons))\n",
    "\n",
    "\n",
    "def handle_action(match, action, button=None):\n",
    "    # handle user action on a match\n",
    "    global state\n",
    "\n",
    "    # create unique key for this action\n",
    "    action_key = f\"{state.run_id}_{match['job_id']}_{action}\"\n",
    "    \n",
    "    # skip if already logged (prevents duplicate clicks)\n",
    "    if action_key in state.logged_actions:\n",
    "        return\n",
    "    state.logged_actions.add(action_key)\n",
    "    \n",
    "    # disable button to prevent double-click\n",
    "    if button:\n",
    "        button.disabled = True\n",
    "\n",
    "    # log to SQLite database\n",
    "    log_action(\n",
    "        session_id=state.session_id,\n",
    "        role=state.role,\n",
    "        doc_id=state.uploaded_id or 'unknown',\n",
    "        match_id=match['job_id'],\n",
    "        action=action,\n",
    "        similarity=match['cross_score'],\n",
    "        cv_text=state.uploaded_text[:2000] if state.uploaded_text else '',\n",
    "        job_text=match['text'][:2000]\n",
    "    )\n",
    "\n",
    "    messages = {\n",
    "        'apply': 'Applied! Application logged.',\n",
    "        'save': 'Saved for later review.',\n",
    "        'skip': 'Skipped this job.',\n",
    "        'not_interested': 'Marked as not interested.',\n",
    "        'view_full': 'Viewing full details...'\n",
    "    }\n",
    "\n",
    "    msg = messages.get(action, f'Action \"{action}\" recorded.')\n",
    "    print(f\"\\n{msg}\")\n",
    "    print(f\"Total feedback actions: {get_action_count()}\")\n",
    "\n",
    "    if action == 'view_full':\n",
    "        print(f\"\\nFULL DETAILS: {match['title']}\")\n",
    "        print(f\"Company: {match['company']}\")\n",
    "        print(f\"Location: {match['location']}\")\n",
    "        print(f\"Seniority: {match['seniority']}\")\n",
    "        print(f\"Skills: {match['skills']}\")\n",
    "        print(f\"\\nFull Text:\")\n",
    "        print(match['text'].replace('passage: ', ''))\n",
    "\n",
    "\n",
    "def display_top_matches(matches, show_top=10):\n",
    "    # display matches\n",
    "    page_matches = matches[:show_top]\n",
    "    current_run = state.run_id  # capture run_id at display time\n",
    "\n",
    "    print(f\"TOP {len(page_matches)} JOB MATCHES\")\n",
    "    print(f\"Showing {len(page_matches)} of {len(matches)}\")\n",
    "\n",
    "    for i, match in enumerate(page_matches):\n",
    "        display_match(match, i, current_run)\n",
    "\n",
    "\n",
    "print(\"Display functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTERACTIVE CV-JOB MATCHING DEMO\n",
      "Model trained on 6K pairs | Searching 1,345,711 jobs\n",
      "Mode: Job Seeker (upload CV -> find matching jobs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8070417077442ab1b2570b066640bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Enter your CV text:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5934aeb29d45f5a469968e281df254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', layout=Layout(height='200px', width='100%'), placeholder='Paste your CV text here...\\n\\nOr …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec8d017ca44fc69563a6336ebc43af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Or upload a file (PDF/DOCX/TXT):')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e85a88f329f464fb82f2db30f8e54c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.docx,.txt', description='Upload CV')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b157bb047acf40ecb49d225d55bff3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Find Matching Jobs!', icon='search', layout=Layout(height='40px', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa8f8576e00a40c8b39e0158c298656d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Ready. Enter CV text or upload a file, then click the button.')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8699604be04269917fb14ede2792f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<hr/>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a663859170164e7791d5588311e26005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# output area where results will be displayed\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# text input for pasting CV\n",
    "text_input = widgets.Textarea(\n",
    "    placeholder='Paste your CV text here...\\n\\nOr upload a file below.',\n",
    "    layout=widgets.Layout(width='100%', height='200px')\n",
    ")\n",
    "\n",
    "# file upload widget\n",
    "file_upload = widgets.FileUpload(\n",
    "    accept='.pdf,.docx,.txt',\n",
    "    multiple=False,\n",
    "    description='Upload CV'\n",
    ")\n",
    "\n",
    "# main search button\n",
    "match_button = widgets.Button(\n",
    "    description='Find Matching Jobs!',\n",
    "    button_style='primary',\n",
    "    icon='search',\n",
    "    layout=widgets.Layout(width='200px', height='40px')\n",
    ")\n",
    "\n",
    "# status label\n",
    "status_label = widgets.Label(\n",
    "    value='Ready. Enter CV text or upload a file, then click the button.'\n",
    ")\n",
    "\n",
    "# ROBUST DEBOUNCING using globals() for reliable persistence\n",
    "if '_nb12_last_search' not in globals():\n",
    "    globals()['_nb12_last_search'] = 0\n",
    "\n",
    "def on_match_click(b):\n",
    "    # callback when user clicks Find Matching Jobs\n",
    "    global state\n",
    "    \n",
    "    # DEBOUNCE: Prevent duplicate calls within 1 second\n",
    "    import time\n",
    "    now = time.time()\n",
    "    if now - globals().get('_nb12_last_search', 0) < 1.0:\n",
    "        return\n",
    "    globals()['_nb12_last_search'] = now\n",
    "    \n",
    "    # disable button during search\n",
    "    b.disabled = True\n",
    "\n",
    "    try:\n",
    "        with output_area:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "            # reset state for new search\n",
    "            state.reset()\n",
    "\n",
    "            cv_text = None\n",
    "\n",
    "            # check text input first\n",
    "            if text_input.value.strip():\n",
    "                cv_text = text_input.value.strip()\n",
    "                state.uploaded_id = f\"text_{uuid.uuid4().hex[:8]}\"\n",
    "                print(f\"Using pasted text ({len(cv_text)} chars)\")\n",
    "\n",
    "            # if no text, check file upload\n",
    "            elif file_upload.value:\n",
    "                print(\"Processing uploaded file...\")\n",
    "                try:\n",
    "                    uploaded_data = file_upload.value\n",
    "\n",
    "                    if isinstance(uploaded_data, tuple) and len(uploaded_data) > 0:\n",
    "                        uploaded = uploaded_data[0]\n",
    "                        filename = uploaded.get('name', 'unknown')\n",
    "                        content = uploaded.get('content', b'')\n",
    "                    elif isinstance(uploaded_data, dict) and len(uploaded_data) > 0:\n",
    "                        first_key = list(uploaded_data.keys())[0]\n",
    "                        uploaded = uploaded_data[first_key]\n",
    "                        filename = uploaded.get('metadata', {}).get('name', first_key)\n",
    "                        content = uploaded.get('content', b'')\n",
    "                    else:\n",
    "                        print(f\"ERROR: Unexpected upload format: {type(uploaded_data)}\")\n",
    "                        return\n",
    "\n",
    "                    print(f\"File: {filename} ({len(content)} bytes)\")\n",
    "\n",
    "                    temp_path = Path(f\"/tmp/{filename}\")\n",
    "                    temp_path.write_bytes(content)\n",
    "                    print(f\"Parsing {filename}...\")\n",
    "\n",
    "                    parsed = parse_document(temp_path)\n",
    "\n",
    "                    if parsed and parsed.get('text'):\n",
    "                        cv_text = parsed['text']\n",
    "                        state.uploaded_id = f\"file_{uuid.uuid4().hex[:8]}\"\n",
    "                        print(f\"Parsed: {parsed['word_count']} words\")\n",
    "                        print(f\"Preview: {cv_text[:200]}...\")\n",
    "                    else:\n",
    "                        print(\"ERROR: Could not extract text from file\")\n",
    "                        return\n",
    "\n",
    "                    temp_path.unlink(missing_ok=True)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR processing file: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "                    return\n",
    "\n",
    "            else:\n",
    "                print(\"Please enter CV text or upload a file!\")\n",
    "                return\n",
    "\n",
    "            state.uploaded_text = cv_text\n",
    "\n",
    "            # bi-encoder retrieval\n",
    "            print(f\"\\nSearching {jobs_index.ntotal:,} jobs...\")\n",
    "            start = time.time()\n",
    "            candidates = find_matches(cv_text, top_k=50)\n",
    "            bi_time = time.time() - start\n",
    "            print(f\"Found {len(candidates)} candidates in {bi_time*1000:.0f}ms\")\n",
    "\n",
    "            # cross-encoder reranking\n",
    "            print(\"Reranking with cross-encoder...\")\n",
    "            start = time.time()\n",
    "            state.matches = rerank_matches(cv_text, candidates, top_k=NUM_RESULTS)\n",
    "            cross_time = time.time() - start\n",
    "            print(f\"Reranked to top {NUM_RESULTS} in {cross_time*1000:.0f}ms\")\n",
    "\n",
    "            print(f\"\\nTotal time: {(bi_time + cross_time)*1000:.0f}ms\")\n",
    "\n",
    "            # display results\n",
    "            display_top_matches(state.matches, show_top=NUM_RESULTS)\n",
    "\n",
    "    finally:\n",
    "        b.disabled = False\n",
    "\n",
    "\n",
    "# ALWAYS register handler - button is created fresh each cell run\n",
    "match_button.on_click(on_match_click)\n",
    "\n",
    "# build the UI layout\n",
    "print(\"INTERACTIVE CV-JOB MATCHING DEMO\")\n",
    "print(f\"Model trained on 6K pairs | Searching {jobs_index.ntotal:,} jobs\")\n",
    "print(\"Mode: Job Seeker (upload CV -> find matching jobs)\")\n",
    "\n",
    "display(widgets.Label(\"Enter your CV text:\"))\n",
    "display(text_input)\n",
    "display(widgets.Label(\"Or upload a file (PDF/DOCX/TXT):\"))\n",
    "display(file_upload)\n",
    "display(match_button)\n",
    "display(status_label)\n",
    "display(widgets.HTML(\"<hr/>\"))\n",
    "display(output_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEEDBACK STATISTICS\n",
      "\n",
      "Total actions logged: 44\n",
      "\n",
      "Actions by type:\n",
      "  view_full            count=  20  weight_sum=+6.0\n",
      "  apply                count=   9  weight_sum=+9.0\n",
      "  skip                 count=   6  weight_sum=+0.0\n",
      "  save                 count=   4  weight_sum=+2.0\n",
      "  contact              count=   3  weight_sum=+3.0\n",
      "  not_interested       count=   2  weight_sum=-1.0\n",
      "\n",
      "Actions by role:\n",
      "  job_seeker: 27\n",
      "  recruiter: 17\n",
      "\n",
      "Retraining threshold: 50 meaningful actions\n",
      "Meaningful actions (weight != 0): 38\n",
      "Actions until retrain eligible: 12\n"
     ]
    }
   ],
   "source": [
    "# show feedback statistics\n",
    "print(\"FEEDBACK STATISTICS\")\n",
    "\n",
    "summary = get_action_summary()\n",
    "\n",
    "total = summary.get('total', 0)\n",
    "print(f\"\\nTotal actions logged: {total}\")\n",
    "\n",
    "# actions by type\n",
    "by_action = summary.get('by_action', {})\n",
    "if by_action:\n",
    "    print(\"\\nActions by type:\")\n",
    "    for action_name, data in by_action.items():\n",
    "        count = data['count']\n",
    "        weight = data['total_weight']\n",
    "        print(f\"  {action_name:20s} count={count:4d}  weight_sum={weight:+.1f}\")\n",
    "\n",
    "# actions by role\n",
    "by_role = summary.get('by_role', {})\n",
    "if by_role:\n",
    "    print(\"\\nActions by role:\")\n",
    "    for role, count in by_role.items():\n",
    "        print(f\"  {role}: {count}\")\n",
    "\n",
    "# retraining status\n",
    "RETRAIN_THRESHOLD = 50\n",
    "meaningful_actions = get_action_count()  # excludes weight=0 actions\n",
    "actions_until_retrain = max(0, RETRAIN_THRESHOLD - meaningful_actions)\n",
    "\n",
    "print(f\"\\nRetraining threshold: {RETRAIN_THRESHOLD} meaningful actions\")\n",
    "print(f\"Meaningful actions (weight != 0): {meaningful_actions}\")\n",
    "print(f\"Actions until retrain eligible: {actions_until_retrain}\")\n",
    "\n",
    "if actions_until_retrain == 0:\n",
    "    print(\"\\n READY FOR RETRAINING\")\n",
    "    print(\"Run the retraining cell below to update the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recent actions (last 10):\n",
      "  [2026-01-28 16:39:18] session=dff7cbc1 role=job_seeker action=skip sim=-10.17\n",
      "  [2026-01-28 16:39:15] session=dff7cbc1 role=job_seeker action=view_full sim=-10.17\n",
      "  [2026-01-28 16:39:06] session=dff7cbc1 role=job_seeker action=view_full sim=-9.93\n",
      "  [2026-01-28 15:28:23] session=9e4f0306 role=job_seeker action=view_full sim=-10.08\n",
      "  [2026-01-28 15:22:33] session=ac48d6c4 role=recruiter action=contact sim=-0.52\n",
      "  [2026-01-28 15:22:32] session=ac48d6c4 role=recruiter action=save sim=-0.51\n",
      "  [2026-01-28 15:22:32] session=ac48d6c4 role=recruiter action=contact sim=-0.46\n",
      "  [2026-01-28 15:22:31] session=ac48d6c4 role=recruiter action=save sim=-0.10\n",
      "  [2026-01-28 15:18:11] session=e458310a role=job_seeker action=view_full sim=-10.06\n",
      "  [2026-01-28 15:11:31] session=5b5bfc87 role=recruiter action=view_full sim=7.71\n"
     ]
    }
   ],
   "source": [
    "# show recent actions (for debugging)\n",
    "recent = summary.get('recent', [])\n",
    "if recent:\n",
    "    print(\"\\nRecent actions (last 10):\")\n",
    "    for r in recent:\n",
    "        session, role, action, sim, ts = r\n",
    "        sim_str = f\"{sim:.2f}\" if sim else \"N/A\"\n",
    "        print(f\"  [{ts}] session={session} role={role} action={action} sim={sim_str}\")\n",
    "else:\n",
    "    print(\"\\nNo actions logged yet. Try the demo above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cell-18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DEMO INSTRUCTIONS (Job Seeker Mode)\n",
      "\n",
      "1. ENTER YOUR CV:\n",
      "   - Paste CV text directly in the text area above, OR\n",
      "   - Click \"Upload CV\" and select a PDF/DOCX/TXT file\n",
      "\n",
      "2. CLICK \"Find Matching Jobs\" to search 1.3M+ jobs\n",
      "\n",
      "3. REVIEW MATCHES:\n",
      "   - Each match shows title, company, location, and skills\n",
      "   - Score is from cross-encoder (higher = better match)\n",
      "   - Green (>5): Great match\n",
      "   - Gold (0-5): Good match\n",
      "   - Pink (<0): Poor match\n",
      "\n",
      "4. TAKE ACTIONS on results:\n",
      "   - Apply (weight +1.0): Strong positive signal\n",
      "   - Save (weight +0.5): Moderate interest\n",
      "   - View Full (weight +0.3): Shows details, mild interest\n",
      "   - Skip (weight 0.0): No signal\n",
      "   - Not Interested (weight -0.5): Negative signal\n",
      "\n",
      "5. FEEDBACK LOOP:\n",
      "   After 50 meaningful actions, the model can be retrained with your feedback.\n",
      "\n",
      "TRY THIS SAMPLE CV:\n",
      "\n",
      "Senior Python Developer with 8 years of experience in Django, PostgreSQL,\n",
      "and AWS. Led teams of 5+ engineers. Expert in microservices, REST APIs,\n",
      "and CI/CD pipelines. Strong background in machine learning and data\n",
      "engineering. Looking for Staff Engineer or Lead roles.\n",
      "\n",
      "NOTE: Recruiter mode (upload job -> find matching CVs) requires CV embeddings\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "DEMO INSTRUCTIONS (Job Seeker Mode)\n",
    "\n",
    "1. ENTER YOUR CV:\n",
    "   - Paste CV text directly in the text area above, OR\n",
    "   - Click \"Upload CV\" and select a PDF/DOCX/TXT file\n",
    "\n",
    "2. CLICK \"Find Matching Jobs\" to search 1.3M+ jobs\n",
    "\n",
    "3. REVIEW MATCHES:\n",
    "   - Each match shows title, company, location, and skills\n",
    "   - Score is from cross-encoder (higher = better match)\n",
    "   - Green (>5): Great match\n",
    "   - Gold (0-5): Good match\n",
    "   - Pink (<0): Poor match\n",
    "\n",
    "4. TAKE ACTIONS on results:\n",
    "   - Apply (weight +1.0): Strong positive signal\n",
    "   - Save (weight +0.5): Moderate interest\n",
    "   - View Full (weight +0.3): Shows details, mild interest\n",
    "   - Skip (weight 0.0): No signal\n",
    "   - Not Interested (weight -0.5): Negative signal\n",
    "\n",
    "5. FEEDBACK LOOP:\n",
    "   After 50 meaningful actions, the model can be retrained with your feedback.\n",
    "\n",
    "TRY THIS SAMPLE CV:\n",
    "\n",
    "Senior Python Developer with 8 years of experience in Django, PostgreSQL,\n",
    "and AWS. Led teams of 5+ engineers. Expert in microservices, REST APIs,\n",
    "and CI/CD pipelines. Strong background in machine learning and data\n",
    "engineering. Looking for Staff Engineer or Lead roles.\n",
    "      \n",
    "NOTE: Recruiter mode (upload job -> find matching CVs) requires CV embeddings\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample CV (you can copy this):\n",
      "Senior Python Developer with 8 years of experience in Django, PostgreSQL,\n",
      "and AWS. Led teams of 5+ engineers. Expert in microservices, REST APIs,\n",
      "and CI/CD pipelines. Strong background in machine learning and data\n",
      "engineering. Looking for Staff Engineer or Lead roles.\n"
     ]
    }
   ],
   "source": [
    "# quick convenience: copy-paste sample CV\n",
    "sample_cv = \"\"\"Senior Python Developer with 8 years of experience in Django, PostgreSQL,\n",
    "and AWS. Led teams of 5+ engineers. Expert in microservices, REST APIs,\n",
    "and CI/CD pipelines. Strong background in machine learning and data\n",
    "engineering. Looking for Staff Engineer or Lead roles.\"\"\"\n",
    "\n",
    "print(\"Sample CV (you can copy this):\")\n",
    "print(sample_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct search function defined.\n",
      "\n",
      "Usage:\n",
      "  results = search_jobs(\"Your CV text here\")\n"
     ]
    }
   ],
   "source": [
    "# direct Search (Alternative)\n",
    "# if the widgets don't work in Jupyter environment\n",
    "\n",
    "def search_jobs(cv_text, show_top=10):\n",
    "    # direct search function\n",
    "    \n",
    "    # Usage:\n",
    "    #     search_jobs(\"Python developer with 5 years experience...\")\n",
    "    global state\n",
    "    \n",
    "    # reset state\n",
    "    state.reset()\n",
    "    state.uploaded_text = cv_text\n",
    "    state.uploaded_id = f\"direct_{uuid.uuid4().hex[:8]}\"\n",
    "    \n",
    "    print(f\"CV length: {len(cv_text)} chars\")\n",
    "    print(f\"\\nSearching {jobs_index.ntotal:,} jobs\")\n",
    "    \n",
    "    # bi-encoder\n",
    "    import time\n",
    "    start = time.time()\n",
    "    candidates = find_matches(cv_text, top_k=50)\n",
    "    bi_time = time.time() - start\n",
    "    print(f\" Bi-encoder: {len(candidates)} candidates in {bi_time*1000:.0f}ms\")\n",
    "    \n",
    "    # cross-encoder\n",
    "    start = time.time()\n",
    "    state.matches = rerank_matches(cv_text, candidates, top_k=show_top)\n",
    "    cross_time = time.time() - start\n",
    "    print(f\" Cross-encoder: top {show_top} in {cross_time*1000:.0f}ms\")\n",
    "    \n",
    "    # display\n",
    "    display_top_matches(state.matches, show_top=show_top)\n",
    "    \n",
    "    return state.matches\n",
    "\n",
    "\n",
    "print(\"Direct search function defined.\")\n",
    "print(\"\")\n",
    "print(\"Usage:\")\n",
    "print('  results = search_jobs(\"Your CV text here\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example direct search\n",
    "# uncomment and run this to test without widgets:\n",
    "\n",
    "# results = search_jobs(sample_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cell-22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual action function defined.\n",
      "\n",
      "Usage (after search_jobs()):\n",
      "  log_manual_action(0, 'apply')   # apply to first match\n",
      "  log_manual_action(1, 'save')    # save second match\n",
      "  log_manual_action(2, 'not_interested')  # reject third\n"
     ]
    }
   ],
   "source": [
    "# log manual action function\n",
    "\n",
    "def log_manual_action(match_index, action):\n",
    "    # log an action for a match from direct search results.\n",
    "    \n",
    "    # Args:\n",
    "    #     match_index: Index in state.matches (0-based)\n",
    "    #     action: One of 'apply', 'save', 'skip', 'not_interested', 'view_full'\n",
    "    \n",
    "    # Usage:\n",
    "    #     log_manual_action(0, 'apply')  # apply to first match\n",
    "    if not state.matches:\n",
    "        print(\"No matches loaded. Run search_jobs() first.\")\n",
    "        return\n",
    "    \n",
    "    if match_index < 0 or match_index >= len(state.matches):\n",
    "        print(f\"Invalid index. Use 0-{len(state.matches)-1}\")\n",
    "        return\n",
    "    \n",
    "    match = state.matches[match_index]\n",
    "    handle_action(match, action)\n",
    "\n",
    "\n",
    "print(\"Manual action function defined.\")\n",
    "print(\"\")\n",
    "print(\"Usage (after search_jobs()):\")\n",
    "print(\"  log_manual_action(0, 'apply')   # apply to first match\")\n",
    "print(\"  log_manual_action(1, 'save')    # save second match\")\n",
    "print(\"  log_manual_action(2, 'not_interested')  # reject third\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEEDBACK-DRIVEN RETRAINING\n",
      "Current actions: 36\n",
      "Threshold for auto-retrain: 50\n",
      "Retrain ready: False\n",
      "\n",
      "Need 14 more actions before retraining.\n"
     ]
    }
   ],
   "source": [
    "# when users take actions (apply, save, not interested), those signals can be used to retrain\n",
    "# the bi-encoder model. This improves matching quality over time based on real user feedback\n",
    "\n",
    "# Requirements for retraining:\n",
    "# - At least 50 meaningful actions (weight != 0)\n",
    "# - Mixes 80% original training data + 20% feedback data\n",
    "# - Uses low learning rate to avoid catastrophic forgetting\n",
    "\n",
    "# Check retraining status\n",
    "from demo.scripts.model_retrainer import (\n",
    "    retrain_from_feedback,\n",
    "    check_retrain_needed,\n",
    "    get_latest_model\n",
    ")\n",
    "\n",
    "print(\"FEEDBACK-DRIVEN RETRAINING\")\n",
    "\n",
    "# Check current status\n",
    "needed, count = check_retrain_needed(threshold=50)\n",
    "print(f\"Current actions: {count}\")\n",
    "print(f\"Threshold for auto-retrain: 50\")\n",
    "print(f\"Retrain ready: {needed}\")\n",
    "\n",
    "if needed:\n",
    "    print(\"\\nModel is ready for retraining! Run the next cell to start.\")\n",
    "else:\n",
    "    print(f\"\\nNeed {50 - count} more actions before retraining.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cell-24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 18:05:06,939 - Only 36 actions, need 50 for retraining\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting retraining\n",
      "Retraining not needed: Need 50 actions, have 36\n"
     ]
    }
   ],
   "source": [
    "# MANUAL RETRAIN TRIGGER\n",
    "# Run this cell to trigger retraining (even if threshold not met)\n",
    "\n",
    "FORCE_RETRAIN = False  # Set to True to force retraining\n",
    "\n",
    "if FORCE_RETRAIN or check_retrain_needed(threshold=10)[0]:\n",
    "    print(\"Starting retraining\")\n",
    "    \n",
    "    result = retrain_from_feedback(\n",
    "        threshold=10 if FORCE_RETRAIN else 50,\n",
    "        epochs=2,\n",
    "        batch_size=32,\n",
    "        learning_rate=1e-5\n",
    "    )\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"RETRAINING COMPLETE!\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"New model: {result['model_path']}\")\n",
    "        print(f\"Training pairs: {result['training_pairs']}\")\n",
    "        print(f\"Time: {result['training_time']:.1f}s\")\n",
    "        print(\"\\nTo use new model, reload the notebook kernel.\")\n",
    "    else:\n",
    "        print(f\"Retraining not needed: {result.get('reason', 'unknown')}\")\n",
    "else:\n",
    "    print(\"Not enough actions for retraining.\")\n",
    "    print(\"Either collect more feedback or set FORCE_RETRAIN = True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cell-25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKILL DICTIONARY UPDATES\n",
      "Total skills tracked: 0\n",
      "Pending approval: 0\n",
      "Already approved: 0\n",
      "\n",
      "No new skill proposals yet.\n",
      "Skills are tracked from uploaded CVs and matched jobs.\n",
      "When new skills appear 3+ times, they'll be proposed here.\n"
     ]
    }
   ],
   "source": [
    "# when users upload CVs and view job matches, we track potential new skills that aren't\n",
    "# in our dictionary. when a skill appears frequently (3+ times), it's proposed for\n",
    "# dictionary updates\n",
    "# this helps keep the skill dictionary current as new technologies emerge\n",
    "\n",
    "# Skill tracking statistics\n",
    "from demo.scripts.skill_tracker import (\n",
    "    get_skill_proposals,\n",
    "    get_skill_statistics,\n",
    "    approve_skill\n",
    ")\n",
    "\n",
    "print(\"SKILL DICTIONARY UPDATES\")\n",
    "\n",
    "# Get statistics\n",
    "stats = get_skill_statistics()\n",
    "print(f\"Total skills tracked: {stats['total_tracked']}\")\n",
    "print(f\"Pending approval: {stats['pending']}\")\n",
    "print(f\"Already approved: {stats['approved']}\")\n",
    "\n",
    "# Show proposals (frequency >= 3)\n",
    "proposals = get_skill_proposals(min_frequency=3)\n",
    "if proposals:\n",
    "    print(f\"\\nProposed new skills (frequency >= 3):\")\n",
    "    for skill, freq in proposals[:10]:\n",
    "        print(f\"  {skill}: {freq} occurrences\")\n",
    "    print(\"\\nTo approve a skill, run: approve_skill('skill_name')\")\n",
    "else:\n",
    "    print(\"\\nNo new skill proposals yet.\")\n",
    "    print(\"Skills are tracked from uploaded CVs and matched jobs.\")\n",
    "    print(\"When new skills appear 3+ times, they'll be proposed here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cell-26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RETRAINING HISTORY\n",
      "Database not initialized\n",
      "\n",
      "Available models:\n",
      "  Latest: /home/developer/project/training/output/models/cv-job-matcher-e5\n",
      "  Base: training/output/models/cv-job-matcher-e5\n"
     ]
    }
   ],
   "source": [
    "# Show retraining history\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"RETRAINING HISTORY\")\n",
    "\n",
    "db_path = os.path.join(PROJECT_ROOT, \"demo\", \"data\", \"feedback.db\")\n",
    "if Path(db_path).exists():\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    history = conn.execute('''\n",
    "        SELECT model_version, num_actions_used, num_positive_pairs,\n",
    "               training_time_sec, timestamp\n",
    "        FROM retraining_log\n",
    "        ORDER BY timestamp DESC\n",
    "        LIMIT 5\n",
    "    ''').fetchall()\n",
    "    conn.close()\n",
    "    \n",
    "    if history:\n",
    "        print(\"Recent retraining runs:\")\n",
    "        for row in history:\n",
    "            print(f\"  {row[4]}: v{row[0]}\")\n",
    "            print(f\"    Actions: {row[1]}, Pairs: {row[2]}, Time: {row[3]:.1f}s\")\n",
    "    else:\n",
    "        print(\"No retraining runs yet.\")\n",
    "else:\n",
    "    print(\"Database not initialized\")\n",
    "\n",
    "# Show available models\n",
    "print(\"\\nAvailable models:\")\n",
    "from demo.scripts.model_retrainer import get_latest_model\n",
    "latest = get_latest_model()\n",
    "print(f\"  Latest: {latest}\")\n",
    "print(f\"  Base: training/output/models/cv-job-matcher-e5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cell-27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Interactive CV-Job matching from 1.3M+ jobs\n",
      "2. User actions (apply, save, skip, not interested)\n",
      "3. Feedback logging to SQLite database\n",
      "4. Batch retraining from user feedback\n",
      "5. Skill dictionary updates from new skill discovery\n",
      "\n",
      "- Model trained on 6,000 pairs, matching against 1.3M+ unseen jobs\n",
      "- Cross-encoder reranking improves match quality\n",
      "- Feedback loop enables continuous improvement\n",
      "- System learns from both positive and negative signals\n",
      "- New skills are tracked and proposed for dictionary updates\n",
      "\n",
      "Files created:\n",
      "- notebooks/12_interactive_matching.ipynb (this notebook)\n",
      "- demo/scripts/feedback_storage.py (SQLite storage)\n",
      "- demo/scripts/document_parser.py (file parsing)\n",
      "- demo/scripts/model_retrainer.py (batch retraining)\n",
      "- demo/scripts/skill_tracker.py (skill dictionary updates)\n",
      "- demo/data/feedback.db (action logs)\n",
      "- training/output/indexes/jobs_full_index.faiss (1.3M job index)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "\n",
    "1. Interactive CV-Job matching from 1.3M+ jobs\n",
    "2. User actions (apply, save, skip, not interested)\n",
    "3. Feedback logging to SQLite database\n",
    "4. Batch retraining from user feedback\n",
    "5. Skill dictionary updates from new skill discovery\n",
    "\n",
    "- Model trained on 6,000 pairs, matching against 1.3M+ unseen jobs\n",
    "- Cross-encoder reranking improves match quality\n",
    "- Feedback loop enables continuous improvement\n",
    "- System learns from both positive and negative signals\n",
    "- New skills are tracked and proposed for dictionary updates\n",
    "\n",
    "Files created:\n",
    "- notebooks/12_interactive_matching.ipynb (this notebook)\n",
    "- demo/scripts/feedback_storage.py (SQLite storage)\n",
    "- demo/scripts/document_parser.py (file parsing)\n",
    "- demo/scripts/model_retrainer.py (batch retraining)\n",
    "- demo/scripts/skill_tracker.py (skill dictionary updates)\n",
    "- demo/data/feedback.db (action logs)\n",
    "- training/output/indexes/jobs_full_index.faiss (1.3M job index)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talent-matching",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
